{'activation_fn': 'relu',
 'batch_size': 4,
 'channel': -1,
 'cond_label_size': None,
 'conditional': False,
 'data_dir': './data/',
 'dataset': '',
 'device': device(type='cuda', index=0),
 'flip_toy_var_order': False,
 'generate': False,
 'hidden_size': 64,
 'input_dims': torch.Size([3, 128, 128]),
 'input_order': 'random',
 'input_size': 49152,
 'log_interval': 1000,
 'lr': 0.0001,
 'model': 'realnvp',
 'n_blocks': 32,
 'n_components': 1,
 'n_epochs': 50,
 'n_hidden': 1,
 'no_batch_norm': False,
 'no_cuda': False,
 'output_dir': './results/mafencoderFalse_channel-1_nblocks32/',
 'restore_file': None,
 'results_file': 'results.txt',
 'seed': 1,
 'start_epoch': 0,
 'train': True,
 'use_encoder': False}
RealNVP(
  (net): FlowSequential(
    (0): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=49152, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=49152, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=49152, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=49152, bias=True)
      )
    )
    (1): BatchNorm()
    (2): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=49152, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=49152, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=49152, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=49152, bias=True)
      )
    )
    (3): BatchNorm()
    (4): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=49152, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=49152, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=49152, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=49152, bias=True)
      )
    )
    (5): BatchNorm()
    (6): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=49152, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=49152, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=49152, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=49152, bias=True)
      )
    )
    (7): BatchNorm()
    (8): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=49152, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=49152, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=49152, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=49152, bias=True)
      )
    )
    (9): BatchNorm()
    (10): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=49152, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=49152, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=49152, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=49152, bias=True)
      )
    )
    (11): BatchNorm()
    (12): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=49152, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=49152, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=49152, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=49152, bias=True)
      )
    )
    (13): BatchNorm()
    (14): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=49152, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=49152, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=49152, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=49152, bias=True)
      )
    )
    (15): BatchNorm()
    (16): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=49152, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=49152, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=49152, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=49152, bias=True)
      )
    )
    (17): BatchNorm()
    (18): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=49152, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=49152, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=49152, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=49152, bias=True)
      )
    )
    (19): BatchNorm()
    (20): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=49152, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=49152, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=49152, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=49152, bias=True)
      )
    )
    (21): BatchNorm()
    (22): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=49152, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=49152, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=49152, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=49152, bias=True)
      )
    )
    (23): BatchNorm()
    (24): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=49152, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=49152, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=49152, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=49152, bias=True)
      )
    )
    (25): BatchNorm()
    (26): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=49152, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=49152, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=49152, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=49152, bias=True)
      )
    )
    (27): BatchNorm()
    (28): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=49152, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=49152, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=49152, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=49152, bias=True)
      )
    )
    (29): BatchNorm()
    (30): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=49152, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=49152, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=49152, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=49152, bias=True)
      )
    )
    (31): BatchNorm()
    (32): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=49152, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=49152, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=49152, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=49152, bias=True)
      )
    )
    (33): BatchNorm()
    (34): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=49152, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=49152, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=49152, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=49152, bias=True)
      )
    )
    (35): BatchNorm()
    (36): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=49152, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=49152, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=49152, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=49152, bias=True)
      )
    )
    (37): BatchNorm()
    (38): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=49152, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=49152, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=49152, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=49152, bias=True)
      )
    )
    (39): BatchNorm()
    (40): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=49152, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=49152, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=49152, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=49152, bias=True)
      )
    )
    (41): BatchNorm()
    (42): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=49152, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=49152, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=49152, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=49152, bias=True)
      )
    )
    (43): BatchNorm()
    (44): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=49152, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=49152, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=49152, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=49152, bias=True)
      )
    )
    (45): BatchNorm()
    (46): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=49152, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=49152, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=49152, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=49152, bias=True)
      )
    )
    (47): BatchNorm()
    (48): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=49152, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=49152, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=49152, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=49152, bias=True)
      )
    )
    (49): BatchNorm()
    (50): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=49152, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=49152, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=49152, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=49152, bias=True)
      )
    )
    (51): BatchNorm()
    (52): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=49152, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=49152, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=49152, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=49152, bias=True)
      )
    )
    (53): BatchNorm()
    (54): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=49152, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=49152, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=49152, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=49152, bias=True)
      )
    )
    (55): BatchNorm()
    (56): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=49152, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=49152, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=49152, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=49152, bias=True)
      )
    )
    (57): BatchNorm()
    (58): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=49152, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=49152, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=49152, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=49152, bias=True)
      )
    )
    (59): BatchNorm()
    (60): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=49152, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=49152, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=49152, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=49152, bias=True)
      )
    )
    (61): BatchNorm()
    (62): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=49152, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=49152, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=49152, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=49152, bias=True)
      )
    )
    (63): BatchNorm()
  )
)
{'activation_fn': 'relu',
 'batch_size': 4,
 'channel': -1,
 'cond_label_size': None,
 'conditional': False,
 'data_dir': './data/',
 'dataset': '',
 'device': device(type='cuda', index=0),
 'flip_toy_var_order': False,
 'generate': False,
 'hidden_size': 64,
 'input_dims': torch.Size([3, 128, 128]),
 'input_order': 'random',
 'input_size': 65536,
 'log_interval': 1000,
 'lr': 0.0001,
 'model': 'realnvp',
 'n_blocks': 32,
 'n_components': 1,
 'n_epochs': 50,
 'n_hidden': 1,
 'no_batch_norm': False,
 'no_cuda': False,
 'output_dir': './results/mafencoderTrue_channel-1_nblocks32/',
 'restore_file': None,
 'results_file': 'results.txt',
 'seed': 1,
 'start_epoch': 0,
 'train': True,
 'use_encoder': True}
RealNVP(
  (net): FlowSequential(
    (0): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=65536, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=65536, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=65536, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=65536, bias=True)
      )
    )
    (1): BatchNorm()
    (2): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=65536, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=65536, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=65536, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=65536, bias=True)
      )
    )
    (3): BatchNorm()
    (4): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=65536, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=65536, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=65536, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=65536, bias=True)
      )
    )
    (5): BatchNorm()
    (6): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=65536, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=65536, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=65536, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=65536, bias=True)
      )
    )
    (7): BatchNorm()
    (8): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=65536, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=65536, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=65536, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=65536, bias=True)
      )
    )
    (9): BatchNorm()
    (10): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=65536, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=65536, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=65536, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=65536, bias=True)
      )
    )
    (11): BatchNorm()
    (12): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=65536, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=65536, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=65536, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=65536, bias=True)
      )
    )
    (13): BatchNorm()
    (14): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=65536, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=65536, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=65536, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=65536, bias=True)
      )
    )
    (15): BatchNorm()
    (16): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=65536, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=65536, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=65536, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=65536, bias=True)
      )
    )
    (17): BatchNorm()
    (18): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=65536, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=65536, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=65536, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=65536, bias=True)
      )
    )
    (19): BatchNorm()
    (20): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=65536, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=65536, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=65536, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=65536, bias=True)
      )
    )
    (21): BatchNorm()
    (22): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=65536, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=65536, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=65536, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=65536, bias=True)
      )
    )
    (23): BatchNorm()
    (24): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=65536, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=65536, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=65536, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=65536, bias=True)
      )
    )
    (25): BatchNorm()
    (26): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=65536, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=65536, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=65536, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=65536, bias=True)
      )
    )
    (27): BatchNorm()
    (28): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=65536, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=65536, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=65536, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=65536, bias=True)
      )
    )
    (29): BatchNorm()
    (30): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=65536, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=65536, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=65536, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=65536, bias=True)
      )
    )
    (31): BatchNorm()
    (32): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=65536, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=65536, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=65536, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=65536, bias=True)
      )
    )
    (33): BatchNorm()
    (34): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=65536, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=65536, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=65536, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=65536, bias=True)
      )
    )
    (35): BatchNorm()
    (36): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=65536, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=65536, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=65536, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=65536, bias=True)
      )
    )
    (37): BatchNorm()
    (38): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=65536, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=65536, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=65536, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=65536, bias=True)
      )
    )
    (39): BatchNorm()
    (40): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=65536, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=65536, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=65536, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=65536, bias=True)
      )
    )
    (41): BatchNorm()
    (42): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=65536, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=65536, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=65536, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=65536, bias=True)
      )
    )
    (43): BatchNorm()
    (44): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=65536, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=65536, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=65536, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=65536, bias=True)
      )
    )
    (45): BatchNorm()
    (46): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=65536, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=65536, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=65536, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=65536, bias=True)
      )
    )
    (47): BatchNorm()
    (48): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=65536, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=65536, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=65536, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=65536, bias=True)
      )
    )
    (49): BatchNorm()
    (50): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=65536, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=65536, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=65536, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=65536, bias=True)
      )
    )
    (51): BatchNorm()
    (52): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=65536, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=65536, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=65536, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=65536, bias=True)
      )
    )
    (53): BatchNorm()
    (54): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=65536, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=65536, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=65536, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=65536, bias=True)
      )
    )
    (55): BatchNorm()
    (56): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=65536, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=65536, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=65536, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=65536, bias=True)
      )
    )
    (57): BatchNorm()
    (58): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=65536, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=65536, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=65536, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=65536, bias=True)
      )
    )
    (59): BatchNorm()
    (60): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=65536, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=65536, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=65536, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=65536, bias=True)
      )
    )
    (61): BatchNorm()
    (62): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=65536, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=65536, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=65536, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=65536, bias=True)
      )
    )
    (63): BatchNorm()
  )
)
Evaluate (epoch 0) -- logp(x) = -82207.102 +/- 459.265
Evaluate (epoch 0) -- logp(x) = -131263192.000 +/- 2208194.500
Evaluate (epoch 0) -- logp(x) = -3023682.750 +/- 397858.000
Evaluate (epoch 0) -- logp(x) = -294254208.000 +/- 4687390.000
Evaluate (epoch 0) -- logp(x) = -79395.578 +/- 42.422
Evaluate (epoch 0) -- logp(x) = 83021.586 +/- 656.738
Evaluate (epoch 0) -- logp(x) = 71503.078 +/- 248.118
Evaluate (epoch 0) -- logp(x) = 74072.016 +/- 425.665
Evaluate (epoch 0) -- logp(x) = 25837.363 +/- 651.272
Evaluate (epoch 0) -- logp(x) = 95974.594 +/- 50.153
Evaluate (epoch 1) -- logp(x) = -89397.938 +/- 80.106
Evaluate (epoch 1) -- logp(x) = -7406126.000 +/- 235931.719
Evaluate (epoch 1) -- logp(x) = -620746.688 +/- 50056.090
Evaluate (epoch 1) -- logp(x) = -16257731.000 +/- 190415.109
Evaluate (epoch 1) -- logp(x) = -505366.125 +/- 3096.836
Evaluate (epoch 1) -- logp(x) = 85899.883 +/- 1428.647
Evaluate (epoch 1) -- logp(x) = 72979.844 +/- 471.298
Evaluate (epoch 1) -- logp(x) = 80368.750 +/- 365.750
Evaluate (epoch 1) -- logp(x) = -20188.281 +/- 1521.552
Evaluate (epoch 1) -- logp(x) = 97909.859 +/- 250.353
Evaluate (epoch 2) -- logp(x) = -12219396.000 +/- 696553.250
Evaluate (epoch 2) -- logp(x) = -172379840.000 +/- 7220226.500
Evaluate (epoch 2) -- logp(x) = -51908768.000 +/- 5040876.000
Evaluate (epoch 2) -- logp(x) = -374297088.000 +/- 4367574.500
Evaluate (epoch 2) -- logp(x) = -148571904.000 +/- 324932.344
Evaluate (epoch 2) -- logp(x) = 89454.156 +/- 382.405
Evaluate (epoch 2) -- logp(x) = 77271.781 +/- 302.994
Evaluate (epoch 2) -- logp(x) = 77592.469 +/- 295.558
Evaluate (epoch 2) -- logp(x) = -25656.855 +/- 3529.824
Evaluate (epoch 2) -- logp(x) = 92066.898 +/- 606.670
Evaluate (epoch 3) -- logp(x) = -76666432.000 +/- 15206084.000
Evaluate (epoch 3) -- logp(x) = -452998.125 +/- 52496.145
Evaluate (epoch 3) -- logp(x) = -169643.250 +/- 136.894
Evaluate (epoch 3) -- logp(x) = -7856779264.000 +/- 491251648.000
Evaluate (epoch 3) -- logp(x) = -935420032.000 +/- 6904864.000
Evaluate (epoch 3) -- logp(x) = 83827.391 +/- 198.117
Evaluate (epoch 3) -- logp(x) = 74157.641 +/- 100.768
Evaluate (epoch 3) -- logp(x) = 76571.812 +/- 139.487
Evaluate (epoch 3) -- logp(x) = -3601539.500 +/- 102227.125
Evaluate (epoch 3) -- logp(x) = 84030.906 +/- 136.800
Evaluate (epoch 4) -- logp(x) = -239688.500 +/- 79.498
Evaluate (epoch 4) -- logp(x) = -245726.406 +/- 73.599
Evaluate (epoch 4) -- logp(x) = -241776.266 +/- 37.833
Evaluate (epoch 4) -- logp(x) = -251232.000 +/- 28.767
Evaluate (epoch 4) -- logp(x) = -241909.250 +/- 1.304
Evaluate (epoch 4) -- logp(x) = 64271.555 +/- 374.232
Evaluate (epoch 4) -- logp(x) = 55297.492 +/- 47.771
Evaluate (epoch 4) -- logp(x) = 57616.180 +/- 191.570
Evaluate (epoch 4) -- logp(x) = -4051482.000 +/- 96626.234
Evaluate (epoch 4) -- logp(x) = 68156.430 +/- 45.529
Evaluate (epoch 5) -- logp(x) = -259886.812 +/- 172.911
Evaluate (epoch 5) -- logp(x) = -269640.500 +/- 37.312
Evaluate (epoch 5) -- logp(x) = -263842.969 +/- 226.087
Evaluate (epoch 5) -- logp(x) = -269955.375 +/- 41.946
Evaluate (epoch 5) -- logp(x) = -261607.641 +/- 31.590
Evaluate (epoch 5) -- logp(x) = 38550.516 +/- 348.107
Evaluate (epoch 5) -- logp(x) = 33501.078 +/- 123.984
Evaluate (epoch 5) -- logp(x) = 34812.109 +/- 145.945
Evaluate (epoch 5) -- logp(x) = -4186460.500 +/- 266395.188
Evaluate (epoch 6) -- logp(x) = -278823.000 +/- 86.952
Evaluate (epoch 5) -- logp(x) = 39016.359 +/- 26.126
Evaluate (epoch 6) -- logp(x) = -19251422.000 +/- 1843446.000
Evaluate (epoch 6) -- logp(x) = -282751.188 +/- 390.521
Evaluate (epoch 6) -- logp(x) = -2297417.500 +/- 191128.672
Evaluate (epoch 6) -- logp(x) = -284459.656 +/- 6.281
Evaluate (epoch 7) -- logp(x) = -219811.656 +/- 143.368
Evaluate (epoch 7) -- logp(x) = -1004067584.000 +/- 73284632.000
Evaluate (epoch 7) -- logp(x) = -232470.172 +/- 240.605
Evaluate (epoch 7) -- logp(x) = -6835155968.000 +/- 604138752.000
Evaluate (epoch 7) -- logp(x) = -228696.906 +/- 44.691
Evaluate (epoch 6) -- logp(x) = 17865.867 +/- 877.226
Evaluate (epoch 6) -- logp(x) = 3446.576 +/- 819.408
Evaluate (epoch 6) -- logp(x) = 20513.270 +/- 52.533
Evaluate (epoch 6) -- logp(x) = -679826624.000 +/- 14396714.000
Evaluate (epoch 6) -- logp(x) = -2211.589 +/- 1480.506
Evaluate (epoch 8) -- logp(x) = -238056.719 +/- 206.233
Evaluate (epoch 8) -- logp(x) = -241033.703 +/- 346.711
Evaluate (epoch 8) -- logp(x) = -243738.938 +/- 642.612
Evaluate (epoch 8) -- logp(x) = -271508.719 +/- 1765.865
Evaluate (epoch 8) -- logp(x) = -254169.469 +/- 315.473
Evaluate (epoch 7) -- logp(x) = -3471.742 +/- 1135.995
Evaluate (epoch 7) -- logp(x) = -37714.262 +/- 4212.431
Evaluate (epoch 7) -- logp(x) = -48298.789 +/- 4643.188
Evaluate (epoch 7) -- logp(x) = -21081.246 +/- 233.261
Evaluate (epoch 7) -- logp(x) = 31553.531 +/- 176.108
Evaluate (epoch 9) -- logp(x) = -667975.812 +/- 77571.758
Evaluate (epoch 9) -- logp(x) = -266286.562 +/- 442.282
Evaluate (epoch 9) -- logp(x) = -2374226.750 +/- 170703.797
Evaluate (epoch 9) -- logp(x) = -279844.281 +/- 1062.715
Evaluate (epoch 9) -- logp(x) = -288527.625 +/- 3867.873
Evaluate (epoch 8) -- logp(x) = -4076133.500 +/- 727327.750
Evaluate (epoch 8) -- logp(x) = -1303462.375 +/- 175927.406
Evaluate (epoch 8) -- logp(x) = -939545.688 +/- 44748.527
Evaluate (epoch 8) -- logp(x) = -78175328.000 +/- 11015264.000
Evaluate (epoch 8) -- logp(x) = -1582428.125 +/- 41587.590
Evaluate (epoch 10) -- logp(x) = -3215252.750 +/- 585838.688
Evaluate (epoch 10) -- logp(x) = -282211.688 +/- 737.136
Evaluate (epoch 10) -- logp(x) = -20414356.000 +/- 3988099.000
Evaluate (epoch 10) -- logp(x) = -281304.594 +/- 132.423
Evaluate (epoch 10) -- logp(x) = -272048.688 +/- 105.698
Evaluate (epoch 9) -- logp(x) = -558074944.000 +/- 32386876.000
Evaluate (epoch 9) -- logp(x) = -366517120.000 +/- 29207100.000
Evaluate (epoch 9) -- logp(x) = -1405885184.000 +/- 25684820.000
Evaluate (epoch 9) -- logp(x) = -63108.633 +/- 267.383
Evaluate (epoch 9) -- logp(x) = -52477208.000 +/- 2016013.750
Evaluate (epoch 11) -- logp(x) = -3946160640.000 +/- 784080832.000
Evaluate (epoch 11) -- logp(x) = -256472.812 +/- 57.928
Evaluate (epoch 11) -- logp(x) = -603067121664.000 +/- 48637579264.000
Evaluate (epoch 11) -- logp(x) = -260697.781 +/- 98.104
Evaluate (epoch 11) -- logp(x) = -258413.906 +/- 1886.032
Evaluate (epoch 10) -- logp(x) = -913299840.000 +/- 68493952.000
Evaluate (epoch 10) -- logp(x) = -1615678464.000 +/- 82695336.000
Evaluate (epoch 10) -- logp(x) = -938846016.000 +/- 39006916.000
Evaluate (epoch 10) -- logp(x) = -194145.250 +/- 147.750
Evaluate (epoch 10) -- logp(x) = -7109529.000 +/- 1340153.375
Evaluate (epoch 12) -- logp(x) = -267436.094 +/- 104.132
Evaluate (epoch 12) -- logp(x) = -262482.406 +/- 137.698
Evaluate (epoch 12) -- logp(x) = -293386.625 +/- 3132.841
Evaluate (epoch 12) -- logp(x) = -269167.562 +/- 178.511
Evaluate (epoch 12) -- logp(x) = -1212312.625 +/- 20067.346
Evaluate (epoch 13) -- logp(x) = -319082.375 +/- 88.973
Evaluate (epoch 13) -- logp(x) = -64222080.000 +/- 4686970.000
Evaluate (epoch 13) -- logp(x) = -339603.500 +/- 4087.884
Evaluate (epoch 11) -- logp(x) = -891105600.000 +/- 66695244.000
Evaluate (epoch 13) -- logp(x) = -4861486.000 +/- 658195.000
Evaluate (epoch 11) -- logp(x) = -2314432000.000 +/- 197732464.000
Evaluate (epoch 13) -- logp(x) = -320132.000 +/- 11.236
Evaluate (epoch 11) -- logp(x) = -2420691456.000 +/- 162898720.000
Evaluate (epoch 11) -- logp(x) = -2288019.250 +/- 416915.344
Evaluate (epoch 11) -- logp(x) = -80393664.000 +/- 3959424.250
Evaluate (epoch 14) -- logp(x) = -325495.344 +/- 43.969
Evaluate (epoch 14) -- logp(x) = -1228048.500 +/- 181243.016
Evaluate (epoch 14) -- logp(x) = -327098.125 +/- 305.350
Evaluate (epoch 14) -- logp(x) = -54171009024.000 +/- 3537461504.000
Evaluate (epoch 14) -- logp(x) = -326906.500 +/- 6.434
Evaluate (epoch 12) -- logp(x) = -6866253824.000 +/- 880814464.000
Evaluate (epoch 12) -- logp(x) = -23665537024.000 +/- 1878559872.000
Evaluate (epoch 12) -- logp(x) = -15296518144.000 +/- 1094195200.000
Evaluate (epoch 12) -- logp(x) = -896777.812 +/- 105075.117
Evaluate (epoch 12) -- logp(x) = -2429976320.000 +/- 482916704.000
Evaluate (epoch 15) -- logp(x) = -289327.250 +/- 588.039
Evaluate (epoch 15) -- logp(x) = -2163602688.000 +/- 401910560.000
Evaluate (epoch 15) -- logp(x) = -15212570.000 +/- 2955314.250
Evaluate (epoch 15) -- logp(x) = -21775407104.000 +/- 455918496.000
Evaluate (epoch 15) -- logp(x) = -290058.312 +/- 10.821
Evaluate (epoch 13) -- logp(x) = -177050.188 +/- 162.690
Evaluate (epoch 13) -- logp(x) = -55457888.000 +/- 5921884.500
Evaluate (epoch 13) -- logp(x) = -178056.844 +/- 62.802
Evaluate (epoch 13) -- logp(x) = -272652.625 +/- 19155.100
Evaluate (epoch 13) -- logp(x) = -45501980.000 +/- 366745.188
Evaluate (epoch 16) -- logp(x) = -314781.000 +/- 115.088
Evaluate (epoch 16) -- logp(x) = -496661.094 +/- 32317.529
Evaluate (epoch 16) -- logp(x) = -1152057.250 +/- 165351.609
Evaluate (epoch 16) -- logp(x) = -1988189696.000 +/- 45173396.000
Evaluate (epoch 16) -- logp(x) = -318386.438 +/- 9.172
Evaluate (epoch 14) -- logp(x) = -668384.938 +/- 106469.094
Evaluate (epoch 14) -- logp(x) = -23137689600.000 +/- 1155204352.000
Evaluate (epoch 14) -- logp(x) = -16023906304.000 +/- 416431168.000
Evaluate (epoch 14) -- logp(x) = -70721056.000 +/- 8740098.000
Evaluate (epoch 14) -- logp(x) = -2230677248.000 +/- 76721280.000
Evaluate (epoch 17) -- logp(x) = -256824.609 +/- 58.776
Evaluate (epoch 17) -- logp(x) = -582823968768.000 +/- 63958040576.000
Evaluate (epoch 17) -- logp(x) = -273884.938 +/- 2637.008
Evaluate (epoch 17) -- logp(x) = -17920830734336.000 +/- 147518685184.000
Evaluate (epoch 17) -- logp(x) = -216152604672.000 +/- 1683794304.000
Evaluate (epoch 15) -- logp(x) = -916406.125 +/- 158547.797
Evaluate (epoch 15) -- logp(x) = -2798824960.000 +/- 140960016.000
Evaluate (epoch 15) -- logp(x) = -2358429696.000 +/- 69193248.000
Evaluate (epoch 15) -- logp(x) = -17213958.000 +/- 2244243.500
Evaluate (epoch 15) -- logp(x) = -539052544.000 +/- 26101646.000
Evaluate (epoch 18) -- logp(x) = -370901.125 +/- 107.820
Evaluate (epoch 18) -- logp(x) = -474995.312 +/- 21814.982
Evaluate (epoch 18) -- logp(x) = -369833.938 +/- 363.974
Evaluate (epoch 18) -- logp(x) = -3413017.750 +/- 137044.656
Evaluate (epoch 18) -- logp(x) = -371208.188 +/- 8.171
Evaluate (epoch 16) -- logp(x) = -58403602432.000 +/- 11426875392.000
Evaluate (epoch 16) -- logp(x) = -100776712.000 +/- 9310182.000
Evaluate (epoch 16) -- logp(x) = -23891760.000 +/- 1902909.250
Evaluate (epoch 16) -- logp(x) = -6568978.500 +/- 780128.375
Evaluate (epoch 16) -- logp(x) = -114901.539 +/- 187.703
Evaluate (epoch 19) -- logp(x) = -237331.781 +/- 46.793
Evaluate (epoch 19) -- logp(x) = -864536576.000 +/- 173469136.000
Evaluate (epoch 19) -- logp(x) = -661070086144.000 +/- 75592343552.000
Evaluate (epoch 19) -- logp(x) = -38916206592.000 +/- 6293573632.000
Evaluate (epoch 19) -- logp(x) = -2374128107520.000 +/- 14589892608.000
Evaluate (epoch 20) -- logp(x) = -294827.812 +/- 12.607
Evaluate (epoch 20) -- logp(x) = -57837236.000 +/- 11503531.000
Evaluate (epoch 20) -- logp(x) = -295532.094 +/- 24.217
Evaluate (epoch 20) -- logp(x) = -197657853952.000 +/- 26434951168.000
Evaluate (epoch 20) -- logp(x) = -292751.906 +/- 17.967
Evaluate (epoch 17) -- logp(x) = -137567.531 +/- 230.856
Evaluate (epoch 17) -- logp(x) = -137274.000 +/- 183.839
Evaluate (epoch 17) -- logp(x) = -141187.328 +/- 190.964
Evaluate (epoch 17) -- logp(x) = -16738590.000 +/- 773595.375
Evaluate (epoch 17) -- logp(x) = -134785.328 +/- 24.156
Evaluate (epoch 21) -- logp(x) = -248997.438 +/- 25.980
Evaluate (epoch 21) -- logp(x) = -6081250.000 +/- 1115969.375
Evaluate (epoch 21) -- logp(x) = -250502.250 +/- 124.074
Evaluate (epoch 21) -- logp(x) = -49722424.000 +/- 2929125.500
Evaluate (epoch 21) -- logp(x) = -250648.219 +/- 8.826
Evaluate (epoch 18) -- logp(x) = -6645587.500 +/- 456339.656
Evaluate (epoch 18) -- logp(x) = -5964367.000 +/- 676854.688
Evaluate (epoch 18) -- logp(x) = -32184200.000 +/- 1698606.750
Evaluate (epoch 18) -- logp(x) = -19852178.000 +/- 3668363.750
Evaluate (epoch 18) -- logp(x) = -315825.844 +/- 2700.782
Evaluate (epoch 22) -- logp(x) = -266313.062 +/- 59.329
Evaluate (epoch 22) -- logp(x) = -3139101.250 +/- 555591.438
Evaluate (epoch 22) -- logp(x) = -337733.812 +/- 13865.771
Evaluate (epoch 22) -- logp(x) = -409827328.000 +/- 35091596.000
Evaluate (epoch 22) -- logp(x) = -268147.781 +/- 7.267
Evaluate (epoch 19) -- logp(x) = -350632.625 +/- 1323.898
Evaluate (epoch 19) -- logp(x) = -349774.125 +/- 1273.720
Evaluate (epoch 19) -- logp(x) = -366660.438 +/- 338.667
Evaluate (epoch 19) -- logp(x) = -334941.938 +/- 82.743
Evaluate (epoch 19) -- logp(x) = -330969.875 +/- 13.852
Evaluate (epoch 23) -- logp(x) = -231712.594 +/- 10.711
Evaluate (epoch 23) -- logp(x) = -7306716160.000 +/- 1389085056.000
Evaluate (epoch 23) -- logp(x) = -235469.547 +/- 532.847
Evaluate (epoch 23) -- logp(x) = -55215947776.000 +/- 797293952.000
Evaluate (epoch 23) -- logp(x) = -232951.375 +/- 18.793
Evaluate (epoch 20) -- logp(x) = -636805.250 +/- 45396.766
Evaluate (epoch 20) -- logp(x) = -135933104.000 +/- 27235780.000
Evaluate (epoch 20) -- logp(x) = -849682.875 +/- 39093.281
Evaluate (epoch 20) -- logp(x) = -296648.844 +/- 242.145
Evaluate (epoch 20) -- logp(x) = -352452672.000 +/- 8760071.000
Evaluate (epoch 24) -- logp(x) = -8760567267328.000 +/- 673870577664.000
Evaluate (epoch 24) -- logp(x) = -364623.000 +/- 122.655
Evaluate (epoch 24) -- logp(x) = -574024581120.000 +/- 96312803328.000
Evaluate (epoch 24) -- logp(x) = -369229.625 +/- 30.375
Evaluate (epoch 24) -- logp(x) = -10492416000.000 +/- 906306112.000
Evaluate (epoch 21) -- logp(x) = -7323467776.000 +/- 647585792.000
Evaluate (epoch 21) -- logp(x) = -72209848.000 +/- 14436772.000
Evaluate (epoch 21) -- logp(x) = -9081870336.000 +/- 167219856.000
Evaluate (epoch 21) -- logp(x) = -205264.188 +/- 348.678
Evaluate (epoch 21) -- logp(x) = -390744.344 +/- 25049.867
Evaluate (epoch 25) -- logp(x) = -330822.562 +/- 121.965
Evaluate (epoch 25) -- logp(x) = -343753.438 +/- 225.779
Evaluate (epoch 25) -- logp(x) = -348974.125 +/- 563.777
Evaluate (epoch 25) -- logp(x) = -358396.219 +/- 158.511
Evaluate (epoch 25) -- logp(x) = -340143.312 +/- 17.478
Evaluate (epoch 22) -- logp(x) = -8186288.500 +/- 1535427.625
Evaluate (epoch 22) -- logp(x) = -1764776.500 +/- 201848.031
Evaluate (epoch 22) -- logp(x) = -12302390.000 +/- 676623.938
Evaluate (epoch 26) -- logp(x) = -276281.406 +/- 72.388
Evaluate (epoch 22) -- logp(x) = -244447.531 +/- 4424.554
Evaluate (epoch 26) -- logp(x) = -137340387328.000 +/- 27571599360.000
Evaluate (epoch 22) -- logp(x) = -433983.938 +/- 19036.045
Evaluate (epoch 26) -- logp(x) = -24859590656.000 +/- 3378701568.000
Evaluate (epoch 26) -- logp(x) = -2894718.500 +/- 70521.977
Evaluate (epoch 26) -- logp(x) = -1414147866624.000 +/- 249806323712.000
Evaluate (epoch 27) -- logp(x) = -248082.312 +/- 80.995
Evaluate (epoch 27) -- logp(x) = -16855824596992.000 +/- 1013014134784.000
Evaluate (epoch 27) -- logp(x) = -788581711872.000 +/- 86048841728.000
Evaluate (epoch 27) -- logp(x) = -32020243677184.000 +/- 262540378112.000
Evaluate (epoch 27) -- logp(x) = -1790339579904.000 +/- 130690424832.000
Evaluate (epoch 23) -- logp(x) = -4181522.750 +/- 783985.438
Evaluate (epoch 23) -- logp(x) = -249807.703 +/- 10161.165
Evaluate (epoch 23) -- logp(x) = -856626.375 +/- 125578.461
Evaluate (epoch 23) -- logp(x) = -11860566.000 +/- 1826750.250
Evaluate (epoch 23) -- logp(x) = -358993.375 +/- 5383.517
Evaluate (epoch 28) -- logp(x) = -262245.406 +/- 29.385
Evaluate (epoch 28) -- logp(x) = -1102793932800.000 +/- 56868667392.000
Evaluate (epoch 28) -- logp(x) = -169663496192.000 +/- 32625485824.000
Evaluate (epoch 28) -- logp(x) = -3746376450048.000 +/- 69319499776.000
Evaluate (epoch 28) -- logp(x) = -264629.156 +/- 54.342
Evaluate (epoch 24) -- logp(x) = -185761.906 +/- 132.672
Evaluate (epoch 24) -- logp(x) = -182486.125 +/- 289.855
Evaluate (epoch 24) -- logp(x) = -20757176.000 +/- 2163428.250
Evaluate (epoch 24) -- logp(x) = -3959990.250 +/- 333325.250
Evaluate (epoch 24) -- logp(x) = -1698051200.000 +/- 323110912.000
Evaluate (epoch 29) -- logp(x) = -296939.938 +/- 10.502
Evaluate (epoch 29) -- logp(x) = -1111893.250 +/- 17468.500
Evaluate (epoch 29) -- logp(x) = -697478.688 +/- 28288.975
Evaluate (epoch 29) -- logp(x) = -220946656.000 +/- 15136094.000
Evaluate (epoch 29) -- logp(x) = -295900.750 +/- 31.133
Evaluate (epoch 25) -- logp(x) = -7203885.000 +/- 1302283.500
Evaluate (epoch 25) -- logp(x) = -1973275.750 +/- 272997.281
Evaluate (epoch 25) -- logp(x) = -25568876.000 +/- 1743272.125
Evaluate (epoch 25) -- logp(x) = -2113130624.000 +/- 417765824.000
Evaluate (epoch 25) -- logp(x) = -273932.125 +/- 279.932
Evaluate (epoch 30) -- logp(x) = -228699.234 +/- 37.267
Evaluate (epoch 30) -- logp(x) = -305251.188 +/- 7392.897
Evaluate (epoch 30) -- logp(x) = -3522162.000 +/- 583204.750
Evaluate (epoch 30) -- logp(x) = -30668935168.000 +/- 1298461440.000
Evaluate (epoch 30) -- logp(x) = -230053.641 +/- 15.703
Evaluate (epoch 26) -- logp(x) = -904918.875 +/- 70179.414
Evaluate (epoch 26) -- logp(x) = -699488.375 +/- 45552.762
Evaluate (epoch 26) -- logp(x) = -28776754.000 +/- 1799312.625
Evaluate (epoch 26) -- logp(x) = -5508868.000 +/- 632336.062
Evaluate (epoch 26) -- logp(x) = -291900.500 +/- 506.243
Evaluate (epoch 31) -- logp(x) = -243114.875 +/- 6.655
Evaluate (epoch 31) -- logp(x) = -46062900.000 +/- 9183205.000
Evaluate (epoch 31) -- logp(x) = -454556.562 +/- 22420.098
Evaluate (epoch 31) -- logp(x) = -10115157590016.000 +/- 814485340160.000
Evaluate (epoch 31) -- logp(x) = -242788.562 +/- 11.525
Evaluate (epoch 27) -- logp(x) = -301423.875 +/- 1834.217
Evaluate (epoch 27) -- logp(x) = -6915598.000 +/- 1330966.750
Evaluate (epoch 27) -- logp(x) = -310856.719 +/- 1073.816
Evaluate (epoch 27) -- logp(x) = -503166432.000 +/- 49925840.000
Evaluate (epoch 27) -- logp(x) = -24555988.000 +/- 1661506.000
Evaluate (epoch 32) -- logp(x) = -305712.250 +/- 36.679
Evaluate (epoch 32) -- logp(x) = -311730.875 +/- 244.798
Evaluate (epoch 32) -- logp(x) = -309743.062 +/- 893.310
Evaluate (epoch 32) -- logp(x) = -324945.250 +/- 298.129
Evaluate (epoch 32) -- logp(x) = -302919.656 +/- 25.575
Evaluate (epoch 33) -- logp(x) = -255478.781 +/- 26.369
Evaluate (epoch 33) -- logp(x) = -8396612829184.000 +/- 280810717184.000
Evaluate (epoch 33) -- logp(x) = -1756052.500 +/- 297178.031
Evaluate (epoch 33) -- logp(x) = -14049043546112.000 +/- 316724346880.000
Evaluate (epoch 28) -- logp(x) = -1433336.250 +/- 210152.516
Evaluate (epoch 33) -- logp(x) = -254242.828 +/- 7.131
Evaluate (epoch 28) -- logp(x) = -776302.812 +/- 75887.000
Evaluate (epoch 28) -- logp(x) = -4597333504.000 +/- 890712384.000
Evaluate (epoch 28) -- logp(x) = -133580256.000 +/- 26486222.000
Evaluate (epoch 28) -- logp(x) = -375020.375 +/- 552.987
Evaluate (epoch 34) -- logp(x) = -274835.750 +/- 19.013
Evaluate (epoch 34) -- logp(x) = -282438.438 +/- 197.728
Evaluate (epoch 34) -- logp(x) = -13163438276608.000 +/- 2606749450240.000
Evaluate (epoch 34) -- logp(x) = -366998.500 +/- 4703.390
Evaluate (epoch 34) -- logp(x) = -274340.906 +/- 5.458
Evaluate (epoch 29) -- logp(x) = -367103.781 +/- 128.567
Evaluate (epoch 29) -- logp(x) = -367003.656 +/- 164.338
Evaluate (epoch 29) -- logp(x) = -3134644.000 +/- 546607.688
Evaluate (epoch 29) -- logp(x) = -3554293.000 +/- 633780.250
Evaluate (epoch 29) -- logp(x) = -365917.938 +/- 35.004
Evaluate (epoch 35) -- logp(x) = -259273.500 +/- 34.854
Evaluate (epoch 35) -- logp(x) = -248276800.000 +/- 49777472.000
Evaluate (epoch 35) -- logp(x) = -19860625408.000 +/- 3932938496.000
Evaluate (epoch 35) -- logp(x) = -74800672.000 +/- 14123776.000
Evaluate (epoch 35) -- logp(x) = -259055.125 +/- 2.439
Evaluate (epoch 30) -- logp(x) = -383158.938 +/- 94.904
Evaluate (epoch 30) -- logp(x) = -434767.969 +/- 9284.402
Evaluate (epoch 30) -- logp(x) = -382322.969 +/- 134.418
Evaluate (epoch 30) -- logp(x) = -409646.062 +/- 90.895
Evaluate (epoch 30) -- logp(x) = -366430.750 +/- 448.711
Evaluate (epoch 36) -- logp(x) = -257819.484 +/- 69.358
Evaluate (epoch 36) -- logp(x) = -499902.625 +/- 12561.882
Evaluate (epoch 36) -- logp(x) = -618411130880.000 +/- 122463608832.000
Evaluate (epoch 36) -- logp(x) = -80676093952.000 +/- 3695801344.000
Evaluate (epoch 36) -- logp(x) = -257984.297 +/- 1.873
Evaluate (epoch 31) -- logp(x) = -1876523.000 +/- 139860.641
Evaluate (epoch 31) -- logp(x) = -1414764.125 +/- 102483.266
Evaluate (epoch 31) -- logp(x) = -350906.844 +/- 4385.161
Evaluate (epoch 31) -- logp(x) = -315913.031 +/- 477.195
Evaluate (epoch 31) -- logp(x) = -4490692.000 +/- 8200.507
Evaluate (epoch 37) -- logp(x) = -317594.281 +/- 39.833
Evaluate (epoch 37) -- logp(x) = -351046.406 +/- 1322.584
Evaluate (epoch 37) -- logp(x) = -723906304.000 +/- 87394352.000
Evaluate (epoch 37) -- logp(x) = -1260211.250 +/- 57247.742
Evaluate (epoch 37) -- logp(x) = -316366.938 +/- 6.215
Evaluate (epoch 32) -- logp(x) = -306902.219 +/- 2461.654
Evaluate (epoch 32) -- logp(x) = -381944.125 +/- 19070.057
Evaluate (epoch 32) -- logp(x) = -2363852324864.000 +/- 328410660864.000
Evaluate (epoch 32) -- logp(x) = -289353.375 +/- 535.483
Evaluate (epoch 32) -- logp(x) = -335188.969 +/- 1205.410
Evaluate (epoch 38) -- logp(x) = -422071.500 +/- 117.297
Evaluate (epoch 38) -- logp(x) = -4647164.000 +/- 846709.188
Evaluate (epoch 38) -- logp(x) = -620741120.000 +/- 122790800.000
Evaluate (epoch 38) -- logp(x) = -279213184.000 +/- 7946092.500
Evaluate (epoch 38) -- logp(x) = -421216.688 +/- 33.798
Evaluate (epoch 33) -- logp(x) = -284669.438 +/- 153.392
Evaluate (epoch 33) -- logp(x) = -289067.562 +/- 365.226
Evaluate (epoch 33) -- logp(x) = -570035.375 +/- 18343.234
Evaluate (epoch 33) -- logp(x) = -281397.625 +/- 85.419
Evaluate (epoch 33) -- logp(x) = -286342.688 +/- 45.286
Evaluate (epoch 39) -- logp(x) = -334217.688 +/- 26.151
Evaluate (epoch 39) -- logp(x) = -1906312478720.000 +/- 382700158976.000
Evaluate (epoch 39) -- logp(x) = -59639730176.000 +/- 11810362368.000
Evaluate (epoch 39) -- logp(x) = -1241820800.000 +/- 176514800.000
Evaluate (epoch 39) -- logp(x) = -334341.438 +/- 5.415
Evaluate (epoch 40) -- logp(x) = -262924.250 +/- 7.461
Evaluate (epoch 40) -- logp(x) = -212794800.000 +/- 31364490.000
Evaluate (epoch 40) -- logp(x) = -265438.219 +/- 291.111
Evaluate (epoch 40) -- logp(x) = -14587740028928.000 +/- 1123997188096.000
Evaluate (epoch 40) -- logp(x) = -263488.625 +/- 26.665
Evaluate (epoch 34) -- logp(x) = -5009250.500 +/- 279437.094
Evaluate (epoch 34) -- logp(x) = -6148863.000 +/- 525446.188
Evaluate (epoch 34) -- logp(x) = -2845759.000 +/- 366395.844
Evaluate (epoch 34) -- logp(x) = -308241.938 +/- 1032.520
Evaluate (epoch 34) -- logp(x) = -1563456.250 +/- 2651.583
Evaluate (epoch 41) -- logp(x) = -240414.406 +/- 8.898
Evaluate (epoch 41) -- logp(x) = -6978298445824.000 +/- 696902287360.000
Evaluate (epoch 41) -- logp(x) = -260561.156 +/- 3989.089
Evaluate (epoch 41) -- logp(x) = -21219650830336.000 +/- 1628927426560.000
Evaluate (epoch 41) -- logp(x) = -240460.500 +/- 2.291
Evaluate (epoch 35) -- logp(x) = -8049367.500 +/- 1550001.750
Evaluate (epoch 35) -- logp(x) = -241898.812 +/- 401.285
Evaluate (epoch 35) -- logp(x) = -256632.938 +/- 275.792
Evaluate (epoch 35) -- logp(x) = -258335.469 +/- 2471.812
Evaluate (epoch 35) -- logp(x) = -23563234.000 +/- 117250.094
Evaluate (epoch 42) -- logp(x) = -221259.734 +/- 21.224
Evaluate (epoch 42) -- logp(x) = -516792342544384.000 +/- 55754157457408.000
Evaluate (epoch 42) -- logp(x) = -226495.375 +/- 969.270
Evaluate (epoch 42) -- logp(x) = -844457679257600.000 +/- 21059602481152.000
Evaluate (epoch 42) -- logp(x) = -221515.875 +/- 2.775
Evaluate (epoch 36) -- logp(x) = -754995.438 +/- 16283.163
Evaluate (epoch 36) -- logp(x) = -2189355.750 +/- 219842.578
Evaluate (epoch 36) -- logp(x) = -295047.312 +/- 7734.071
Evaluate (epoch 36) -- logp(x) = -35502224.000 +/- 7008694.000
Evaluate (epoch 36) -- logp(x) = -737677.188 +/- 3068.991
Evaluate (epoch 43) -- logp(x) = -233042.516 +/- 49.657
Evaluate (epoch 43) -- logp(x) = -222234208.000 +/- 22437486.000
Evaluate (epoch 43) -- logp(x) = -253780.141 +/- 3994.376
Evaluate (epoch 43) -- logp(x) = -322538209280.000 +/- 45200474112.000
Evaluate (epoch 43) -- logp(x) = -233171.094 +/- 9.406
Evaluate (epoch 37) -- logp(x) = -289554.781 +/- 9943.908
Evaluate (epoch 37) -- logp(x) = -345615744.000 +/- 69334416.000
Evaluate (epoch 37) -- logp(x) = -810665705472.000 +/- 160509263872.000
Evaluate (epoch 37) -- logp(x) = -205671.312 +/- 1117.870
Evaluate (epoch 37) -- logp(x) = -8313512.500 +/- 589918.750
Evaluate (epoch 44) -- logp(x) = -232177.047 +/- 31.233
Evaluate (epoch 44) -- logp(x) = -24012595920896.000 +/- 2716126674944.000
Evaluate (epoch 44) -- logp(x) = -1106397626368.000 +/- 219099365376.000
Evaluate (epoch 44) -- logp(x) = -205045714911232.000 +/- 11410493931520.000
Evaluate (epoch 44) -- logp(x) = -232287.062 +/- 6.874
Evaluate (epoch 38) -- logp(x) = -502250766336.000 +/- 99140935680.000
Evaluate (epoch 38) -- logp(x) = -55126646784.000 +/- 11065701376.000
Evaluate (epoch 38) -- logp(x) = -2193787977728.000 +/- 272488644608.000
Evaluate (epoch 38) -- logp(x) = -5140169.500 +/- 591473.250
Evaluate (epoch 38) -- logp(x) = -3161918.000 +/- 138545.562
Evaluate (epoch 45) -- logp(x) = -315150.250 +/- 19.590
Evaluate (epoch 45) -- logp(x) = -1292989497344.000 +/- 155959820288.000
Evaluate (epoch 45) -- logp(x) = -316601.219 +/- 235.972
Evaluate (epoch 45) -- logp(x) = -8831067226112.000 +/- 332183699456.000
Evaluate (epoch 45) -- logp(x) = -315058.156 +/- 3.550
Evaluate (epoch 39) -- logp(x) = -86829842432.000 +/- 11246158848.000
Evaluate (epoch 39) -- logp(x) = -159426019328.000 +/- 13600055296.000
Evaluate (epoch 46) -- logp(x) = -267296.750 +/- 11.494
Evaluate (epoch 39) -- logp(x) = -129403797504.000 +/- 7723883520.000
Evaluate (epoch 46) -- logp(x) = -207354512.000 +/- 41559464.000
Evaluate (epoch 46) -- logp(x) = -268999.250 +/- 120.339
Evaluate (epoch 39) -- logp(x) = -471615392.000 +/- 93672112.000
Evaluate (epoch 39) -- logp(x) = -343939.312 +/- 318.361
Evaluate (epoch 46) -- logp(x) = -6641534631936.000 +/- 399850340352.000
Evaluate (epoch 46) -- logp(x) = -268458.344 +/- 1.514
Evaluate (epoch 47) -- logp(x) = -316336.375 +/- 25.846
Evaluate (epoch 47) -- logp(x) = -339732.938 +/- 806.918
Evaluate (epoch 47) -- logp(x) = -319020.281 +/- 244.544
Evaluate (epoch 47) -- logp(x) = -27546516.000 +/- 2184242.000
Evaluate (epoch 47) -- logp(x) = -318273.875 +/- 5.121
Evaluate (epoch 40) -- logp(x) = -2578308096.000 +/- 136076288.000
Evaluate (epoch 40) -- logp(x) = -2743687680.000 +/- 37395228.000
Evaluate (epoch 40) -- logp(x) = -3466348544.000 +/- 68273584.000
Evaluate (epoch 40) -- logp(x) = -386591.875 +/- 24.651
Evaluate (epoch 40) -- logp(x) = -2306933248.000 +/- 79432416.000
Evaluate (epoch 48) -- logp(x) = -216395.188 +/- 28.738
Evaluate (epoch 48) -- logp(x) = -299089.875 +/- 4840.616
Evaluate (epoch 48) -- logp(x) = -52240964.000 +/- 10302189.000
Evaluate (epoch 48) -- logp(x) = -14388116.000 +/- 958327.500
Evaluate (epoch 48) -- logp(x) = -217206.156 +/- 5.956
Evaluate (epoch 41) -- logp(x) = -218556976.000 +/- 42603060.000
Evaluate (epoch 41) -- logp(x) = -8773110.000 +/- 954458.125
Evaluate (epoch 41) -- logp(x) = -10241014.000 +/- 1917274.625
Evaluate (epoch 41) -- logp(x) = -49620036.000 +/- 3432198.500
Evaluate (epoch 41) -- logp(x) = -563812.500 +/- 1239.295
Evaluate (epoch 49) -- logp(x) = -244732.969 +/- 27.032
Evaluate (epoch 49) -- logp(x) = -640339.812 +/- 76860.836
Evaluate (epoch 49) -- logp(x) = -264231712.000 +/- 30427334.000
Evaluate (epoch 49) -- logp(x) = -344933.438 +/- 6516.396
Evaluate (epoch 49) -- logp(x) = -246162.250 +/- 31.070
Evaluate (epoch 42) -- logp(x) = -3328747.250 +/- 398749.156
Evaluate (epoch 42) -- logp(x) = -15265851.000 +/- 1241639.375
Evaluate (epoch 42) -- logp(x) = -19396148.000 +/- 596637.188
Evaluate (epoch 42) -- logp(x) = -4496483.500 +/- 755711.000
Evaluate (epoch 42) -- logp(x) = -26193338.000 +/- 92473.750
Evaluate (epoch 43) -- logp(x) = -17072775168.000 +/- 2191383808.000
Evaluate (epoch 43) -- logp(x) = -9928886272.000 +/- 714850176.000
Evaluate (epoch 43) -- logp(x) = -13243619328.000 +/- 1230445440.000
Evaluate (epoch 43) -- logp(x) = -7108042240.000 +/- 798489152.000
Evaluate (epoch 43) -- logp(x) = -14850035712.000 +/- 173132384.000
Evaluate (epoch 44) -- logp(x) = -2021112576.000 +/- 401594432.000
Evaluate (epoch 44) -- logp(x) = -620930.625 +/- 1159.383
Evaluate (epoch 44) -- logp(x) = -615376.188 +/- 136.629
Evaluate (epoch 44) -- logp(x) = -2223449.750 +/- 281985.250
Evaluate (epoch 44) -- logp(x) = -614951.625 +/- 27.226
Evaluate (epoch 45) -- logp(x) = -29753683968.000 +/- 3898454784.000
Evaluate (epoch 45) -- logp(x) = -17751652352.000 +/- 3044872704.000
Evaluate (epoch 45) -- logp(x) = -68384690176.000 +/- 11724672000.000
Evaluate (epoch 45) -- logp(x) = -3395252992.000 +/- 464064704.000
Evaluate (epoch 45) -- logp(x) = -823124928.000 +/- 38711532.000
Evaluate (epoch 46) -- logp(x) = -189256450048.000 +/- 35766923264.000
Evaluate (epoch 46) -- logp(x) = -1369288960.000 +/- 163954096.000
Evaluate (epoch 46) -- logp(x) = -151333191680.000 +/- 10093661184.000
Evaluate (epoch 46) -- logp(x) = -29165632.000 +/- 5182685.000
Evaluate (epoch 46) -- logp(x) = -96145489920.000 +/- 18924513280.000
Evaluate (epoch 47) -- logp(x) = -367274819584.000 +/- 34810417152.000
Evaluate (epoch 47) -- logp(x) = -1303573561344.000 +/- 240608919552.000
Evaluate (epoch 47) -- logp(x) = -275503251456.000 +/- 53913968640.000
Evaluate (epoch 47) -- logp(x) = -13526662144.000 +/- 1552948992.000
Evaluate (epoch 47) -- logp(x) = -3727120384.000 +/- 412265568.000
Evaluate (epoch 48) -- logp(x) = -6276362.000 +/- 311148.250
Evaluate (epoch 48) -- logp(x) = -3292420.250 +/- 185095.562
Evaluate (epoch 48) -- logp(x) = -2911987.750 +/- 152430.172
Evaluate (epoch 48) -- logp(x) = -159326633984.000 +/- 4681672192.000
Evaluate (epoch 48) -- logp(x) = -3774464.500 +/- 29065.885
Evaluate (epoch 49) -- logp(x) = -100940688.000 +/- 13748427.000
Evaluate (epoch 49) -- logp(x) = -346677641216.000 +/- 69593554944.000
Evaluate (epoch 49) -- logp(x) = -321499365376.000 +/- 63662698496.000
Evaluate (epoch 49) -- logp(x) = -2199543611392.000 +/- 133146066944.000
Evaluate (epoch 49) -- logp(x) = -21704788.000 +/- 2067793.750
{'activation_fn': 'relu',
 'batch_size': 4,
 'channel': -1,
 'cond_label_size': None,
 'conditional': False,
 'data_dir': './data/',
 'dataset': '',
 'device': device(type='cuda', index=0),
 'flip_toy_var_order': False,
 'generate': False,
 'hidden_size': 64,
 'input_dims': torch.Size([3, 128, 128]),
 'input_order': 'random',
 'input_size': 65536,
 'log_interval': 1000,
 'lr': 0.0001,
 'model': 'realnvp',
 'n_blocks': 32,
 'n_components': 1,
 'n_epochs': 50,
 'n_hidden': 1,
 'no_batch_norm': False,
 'no_cuda': False,
 'output_dir': './results/mafencoderTrue_channel-1_nblocks32/',
 'restore_file': None,
 'results_file': 'results.txt',
 'seed': 1,
 'start_epoch': 0,
 'train': False,
 'use_encoder': True}
RealNVP(
  (net): FlowSequential(
    (0): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=65536, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=65536, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=65536, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=65536, bias=True)
      )
    )
    (1): BatchNorm()
    (2): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=65536, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=65536, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=65536, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=65536, bias=True)
      )
    )
    (3): BatchNorm()
    (4): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=65536, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=65536, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=65536, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=65536, bias=True)
      )
    )
    (5): BatchNorm()
    (6): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=65536, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=65536, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=65536, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=65536, bias=True)
      )
    )
    (7): BatchNorm()
    (8): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=65536, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=65536, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=65536, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=65536, bias=True)
      )
    )
    (9): BatchNorm()
    (10): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=65536, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=65536, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=65536, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=65536, bias=True)
      )
    )
    (11): BatchNorm()
    (12): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=65536, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=65536, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=65536, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=65536, bias=True)
      )
    )
    (13): BatchNorm()
    (14): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=65536, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=65536, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=65536, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=65536, bias=True)
      )
    )
    (15): BatchNorm()
    (16): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=65536, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=65536, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=65536, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=65536, bias=True)
      )
    )
    (17): BatchNorm()
    (18): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=65536, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=65536, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=65536, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=65536, bias=True)
      )
    )
    (19): BatchNorm()
    (20): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=65536, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=65536, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=65536, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=65536, bias=True)
      )
    )
    (21): BatchNorm()
    (22): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=65536, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=65536, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=65536, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=65536, bias=True)
      )
    )
    (23): BatchNorm()
    (24): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=65536, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=65536, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=65536, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=65536, bias=True)
      )
    )
    (25): BatchNorm()
    (26): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=65536, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=65536, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=65536, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=65536, bias=True)
      )
    )
    (27): BatchNorm()
    (28): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=65536, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=65536, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=65536, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=65536, bias=True)
      )
    )
    (29): BatchNorm()
    (30): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=65536, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=65536, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=65536, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=65536, bias=True)
      )
    )
    (31): BatchNorm()
    (32): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=65536, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=65536, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=65536, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=65536, bias=True)
      )
    )
    (33): BatchNorm()
    (34): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=65536, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=65536, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=65536, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=65536, bias=True)
      )
    )
    (35): BatchNorm()
    (36): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=65536, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=65536, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=65536, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=65536, bias=True)
      )
    )
    (37): BatchNorm()
    (38): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=65536, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=65536, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=65536, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=65536, bias=True)
      )
    )
    (39): BatchNorm()
    (40): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=65536, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=65536, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=65536, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=65536, bias=True)
      )
    )
    (41): BatchNorm()
    (42): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=65536, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=65536, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=65536, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=65536, bias=True)
      )
    )
    (43): BatchNorm()
    (44): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=65536, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=65536, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=65536, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=65536, bias=True)
      )
    )
    (45): BatchNorm()
    (46): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=65536, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=65536, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=65536, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=65536, bias=True)
      )
    )
    (47): BatchNorm()
    (48): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=65536, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=65536, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=65536, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=65536, bias=True)
      )
    )
    (49): BatchNorm()
    (50): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=65536, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=65536, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=65536, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=65536, bias=True)
      )
    )
    (51): BatchNorm()
    (52): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=65536, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=65536, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=65536, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=65536, bias=True)
      )
    )
    (53): BatchNorm()
    (54): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=65536, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=65536, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=65536, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=65536, bias=True)
      )
    )
    (55): BatchNorm()
    (56): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=65536, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=65536, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=65536, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=65536, bias=True)
      )
    )
    (57): BatchNorm()
    (58): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=65536, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=65536, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=65536, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=65536, bias=True)
      )
    )
    (59): BatchNorm()
    (60): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=65536, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=65536, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=65536, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=65536, bias=True)
      )
    )
    (61): BatchNorm()
    (62): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=65536, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=65536, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=65536, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=65536, bias=True)
      )
    )
    (63): BatchNorm()
  )
)
Evaluate logp(x) = -69271.578 +/- 3.452
Evaluate logp(x) = -69520.422 +/- 2.637
Evaluate logp(x) = -69563.969 +/- 1.664
Evaluate logp(x) = -68753.469 +/- 2.417
Evaluate logp(x) = -69212.508 +/- 1.497
{'activation_fn': 'relu',
 'batch_size': 4,
 'channel': -1,
 'cond_label_size': None,
 'conditional': False,
 'data_dir': './data/',
 'dataset': '',
 'device': device(type='cuda', index=0),
 'flip_toy_var_order': False,
 'generate': False,
 'hidden_size': 64,
 'input_dims': torch.Size([3, 128, 128]),
 'input_order': 'random',
 'input_size': 49152,
 'log_interval': 1000,
 'lr': 0.0001,
 'model': 'realnvp',
 'n_blocks': 32,
 'n_components': 1,
 'n_epochs': 50,
 'n_hidden': 1,
 'no_batch_norm': False,
 'no_cuda': False,
 'output_dir': './results/mafencoderFalse_channel-1_nblocks32/',
 'restore_file': None,
 'results_file': 'results.txt',
 'seed': 1,
 'start_epoch': 0,
 'train': False,
 'use_encoder': False}
RealNVP(
  (net): FlowSequential(
    (0): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=49152, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=49152, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=49152, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=49152, bias=True)
      )
    )
    (1): BatchNorm()
    (2): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=49152, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=49152, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=49152, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=49152, bias=True)
      )
    )
    (3): BatchNorm()
    (4): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=49152, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=49152, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=49152, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=49152, bias=True)
      )
    )
    (5): BatchNorm()
    (6): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=49152, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=49152, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=49152, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=49152, bias=True)
      )
    )
    (7): BatchNorm()
    (8): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=49152, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=49152, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=49152, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=49152, bias=True)
      )
    )
    (9): BatchNorm()
    (10): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=49152, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=49152, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=49152, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=49152, bias=True)
      )
    )
    (11): BatchNorm()
    (12): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=49152, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=49152, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=49152, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=49152, bias=True)
      )
    )
    (13): BatchNorm()
    (14): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=49152, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=49152, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=49152, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=49152, bias=True)
      )
    )
    (15): BatchNorm()
    (16): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=49152, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=49152, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=49152, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=49152, bias=True)
      )
    )
    (17): BatchNorm()
    (18): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=49152, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=49152, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=49152, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=49152, bias=True)
      )
    )
    (19): BatchNorm()
    (20): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=49152, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=49152, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=49152, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=49152, bias=True)
      )
    )
    (21): BatchNorm()
    (22): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=49152, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=49152, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=49152, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=49152, bias=True)
      )
    )
    (23): BatchNorm()
    (24): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=49152, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=49152, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=49152, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=49152, bias=True)
      )
    )
    (25): BatchNorm()
    (26): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=49152, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=49152, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=49152, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=49152, bias=True)
      )
    )
    (27): BatchNorm()
    (28): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=49152, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=49152, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=49152, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=49152, bias=True)
      )
    )
    (29): BatchNorm()
    (30): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=49152, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=49152, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=49152, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=49152, bias=True)
      )
    )
    (31): BatchNorm()
    (32): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=49152, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=49152, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=49152, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=49152, bias=True)
      )
    )
    (33): BatchNorm()
    (34): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=49152, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=49152, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=49152, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=49152, bias=True)
      )
    )
    (35): BatchNorm()
    (36): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=49152, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=49152, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=49152, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=49152, bias=True)
      )
    )
    (37): BatchNorm()
    (38): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=49152, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=49152, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=49152, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=49152, bias=True)
      )
    )
    (39): BatchNorm()
    (40): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=49152, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=49152, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=49152, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=49152, bias=True)
      )
    )
    (41): BatchNorm()
    (42): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=49152, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=49152, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=49152, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=49152, bias=True)
      )
    )
    (43): BatchNorm()
    (44): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=49152, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=49152, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=49152, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=49152, bias=True)
      )
    )
    (45): BatchNorm()
    (46): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=49152, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=49152, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=49152, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=49152, bias=True)
      )
    )
    (47): BatchNorm()
    (48): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=49152, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=49152, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=49152, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=49152, bias=True)
      )
    )
    (49): BatchNorm()
    (50): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=49152, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=49152, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=49152, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=49152, bias=True)
      )
    )
    (51): BatchNorm()
    (52): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=49152, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=49152, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=49152, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=49152, bias=True)
      )
    )
    (53): BatchNorm()
    (54): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=49152, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=49152, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=49152, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=49152, bias=True)
      )
    )
    (55): BatchNorm()
    (56): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=49152, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=49152, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=49152, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=49152, bias=True)
      )
    )
    (57): BatchNorm()
    (58): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=49152, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=49152, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=49152, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=49152, bias=True)
      )
    )
    (59): BatchNorm()
    (60): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=49152, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=49152, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=49152, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=49152, bias=True)
      )
    )
    (61): BatchNorm()
    (62): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=49152, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=49152, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=49152, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=49152, bias=True)
      )
    )
    (63): BatchNorm()
  )
)
Evaluate logp(x) = -77789.133 +/- 475.535
Evaluate logp(x) = -101605.250 +/- 1670.716
Evaluate logp(x) = -56034.320 +/- 156.966
Evaluate logp(x) = -343258.781 +/- 10397.344
Evaluate logp(x) = -55498.508 +/- 2.147
{'activation_fn': 'relu',
 'batch_size': 4,
 'channel': -1,
 'cond_label_size': None,
 'conditional': False,
 'data_dir': './data/',
 'dataset': '',
 'device': device(type='cuda', index=0),
 'flip_toy_var_order': False,
 'generate': False,
 'hidden_size': 64,
 'input_dims': torch.Size([3, 128, 128]),
 'input_order': 'random',
 'input_size': 49152,
 'log_interval': 1000,
 'lr': 0.0001,
 'model': 'realnvp',
 'n_blocks': 32,
 'n_components': 1,
 'n_epochs': 50,
 'n_hidden': 1,
 'no_batch_norm': False,
 'no_cuda': False,
 'output_dir': './results/mafencoderFalse_channel-1_nblocks32/',
 'restore_file': None,
 'results_file': 'results.txt',
 'seed': 1,
 'start_epoch': 0,
 'train': True,
 'use_encoder': False}
RealNVP(
  (net): FlowSequential(
    (0): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=49152, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=49152, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=49152, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=49152, bias=True)
      )
    )
    (1): BatchNorm()
    (2): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=49152, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=49152, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=49152, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=49152, bias=True)
      )
    )
    (3): BatchNorm()
    (4): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=49152, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=49152, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=49152, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=49152, bias=True)
      )
    )
    (5): BatchNorm()
    (6): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=49152, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=49152, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=49152, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=49152, bias=True)
      )
    )
    (7): BatchNorm()
    (8): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=49152, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=49152, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=49152, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=49152, bias=True)
      )
    )
    (9): BatchNorm()
    (10): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=49152, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=49152, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=49152, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=49152, bias=True)
      )
    )
    (11): BatchNorm()
    (12): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=49152, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=49152, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=49152, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=49152, bias=True)
      )
    )
    (13): BatchNorm()
    (14): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=49152, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=49152, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=49152, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=49152, bias=True)
      )
    )
    (15): BatchNorm()
    (16): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=49152, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=49152, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=49152, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=49152, bias=True)
      )
    )
    (17): BatchNorm()
    (18): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=49152, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=49152, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=49152, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=49152, bias=True)
      )
    )
    (19): BatchNorm()
    (20): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=49152, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=49152, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=49152, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=49152, bias=True)
      )
    )
    (21): BatchNorm()
    (22): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=49152, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=49152, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=49152, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=49152, bias=True)
      )
    )
    (23): BatchNorm()
    (24): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=49152, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=49152, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=49152, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=49152, bias=True)
      )
    )
    (25): BatchNorm()
    (26): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=49152, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=49152, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=49152, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=49152, bias=True)
      )
    )
    (27): BatchNorm()
    (28): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=49152, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=49152, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=49152, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=49152, bias=True)
      )
    )
    (29): BatchNorm()
    (30): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=49152, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=49152, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=49152, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=49152, bias=True)
      )
    )
    (31): BatchNorm()
    (32): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=49152, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=49152, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=49152, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=49152, bias=True)
      )
    )
    (33): BatchNorm()
    (34): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=49152, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=49152, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=49152, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=49152, bias=True)
      )
    )
    (35): BatchNorm()
    (36): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=49152, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=49152, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=49152, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=49152, bias=True)
      )
    )
    (37): BatchNorm()
    (38): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=49152, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=49152, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=49152, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=49152, bias=True)
      )
    )
    (39): BatchNorm()
    (40): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=49152, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=49152, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=49152, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=49152, bias=True)
      )
    )
    (41): BatchNorm()
    (42): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=49152, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=49152, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=49152, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=49152, bias=True)
      )
    )
    (43): BatchNorm()
    (44): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=49152, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=49152, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=49152, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=49152, bias=True)
      )
    )
    (45): BatchNorm()
    (46): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=49152, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=49152, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=49152, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=49152, bias=True)
      )
    )
    (47): BatchNorm()
    (48): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=49152, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=49152, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=49152, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=49152, bias=True)
      )
    )
    (49): BatchNorm()
    (50): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=49152, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=49152, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=49152, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=49152, bias=True)
      )
    )
    (51): BatchNorm()
    (52): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=49152, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=49152, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=49152, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=49152, bias=True)
      )
    )
    (53): BatchNorm()
    (54): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=49152, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=49152, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=49152, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=49152, bias=True)
      )
    )
    (55): BatchNorm()
    (56): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=49152, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=49152, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=49152, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=49152, bias=True)
      )
    )
    (57): BatchNorm()
    (58): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=49152, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=49152, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=49152, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=49152, bias=True)
      )
    )
    (59): BatchNorm()
    (60): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=49152, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=49152, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=49152, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=49152, bias=True)
      )
    )
    (61): BatchNorm()
    (62): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=49152, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=49152, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=49152, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=49152, bias=True)
      )
    )
    (63): BatchNorm()
  )
)
Evaluate (epoch 0) -- logp(x) = -78697.422 +/- 324.949
Evaluate (epoch 0) -- logp(x) = -385456.625 +/- 12164.676
Evaluate (epoch 0) -- logp(x) = -76357.750 +/- 262.108
Evaluate (epoch 0) -- logp(x) = -1663057.750 +/- 31735.559
Evaluate (epoch 0) -- logp(x) = -78056.359 +/- 4.630
Evaluate (epoch 1) -- logp(x) = -84769.047 +/- 143.588
Evaluate (epoch 1) -- logp(x) = -21231890.000 +/- 810694.250
Evaluate (epoch 1) -- logp(x) = -863599.750 +/- 102521.781
Evaluate (epoch 1) -- logp(x) = -67316848.000 +/- 368526.562
Evaluate (epoch 1) -- logp(x) = -726930.875 +/- 5364.166
Evaluate (epoch 2) -- logp(x) = -576205.438 +/- 82916.648
Evaluate (epoch 2) -- logp(x) = -5882933.000 +/- 311540.125
Evaluate (epoch 2) -- logp(x) = -2361687.500 +/- 209052.234
Evaluate (epoch 2) -- logp(x) = -42373408.000 +/- 147582.328
Evaluate (epoch 2) -- logp(x) = -165183.344 +/- 85.538
Evaluate (epoch 3) -- logp(x) = -56775696.000 +/- 2734708.750
Evaluate (epoch 3) -- logp(x) = -28936906.000 +/- 1335888.000
Evaluate (epoch 3) -- logp(x) = -171723.891 +/- 196.914
Evaluate (epoch 3) -- logp(x) = -744189440.000 +/- 15489556.000
Evaluate (epoch 3) -- logp(x) = -170687.625 +/- 7.280
Evaluate (epoch 4) -- logp(x) = -175067.984 +/- 110.478
Evaluate (epoch 4) -- logp(x) = -6170872832.000 +/- 454479200.000
Evaluate (epoch 4) -- logp(x) = -180429.125 +/- 55.046
Evaluate (epoch 4) -- logp(x) = -187030781952.000 +/- 1736325504.000
Evaluate (epoch 4) -- logp(x) = -180090.062 +/- 8.686
Evaluate (epoch 5) -- logp(x) = -212156640.000 +/- 24481276.000
Evaluate (epoch 5) -- logp(x) = -592224.625 +/- 23814.861
Evaluate (epoch 5) -- logp(x) = -238916.984 +/- 123.904
Evaluate (epoch 5) -- logp(x) = -64111556.000 +/- 2590729.500
Evaluate (epoch 5) -- logp(x) = -235065.469 +/- 39.811
Evaluate (epoch 6) -- logp(x) = -928051328.000 +/- 179868416.000
Evaluate (epoch 6) -- logp(x) = -291780.062 +/- 416.054
Evaluate (epoch 6) -- logp(x) = -281328.125 +/- 146.017
Evaluate (epoch 6) -- logp(x) = -329162.625 +/- 294.357
Evaluate (epoch 6) -- logp(x) = -279918.062 +/- 3.496
Evaluate (epoch 7) -- logp(x) = -843375872.000 +/- 167585936.000
Evaluate (epoch 7) -- logp(x) = -3158027272192.000 +/- 65259982848.000
Evaluate (epoch 7) -- logp(x) = -230198.688 +/- 461.662
Evaluate (epoch 7) -- logp(x) = -5793101381632.000 +/- 50681487360.000
Evaluate (epoch 7) -- logp(x) = -261512.750 +/- 190.008
Evaluate (epoch 8) -- logp(x) = -11614445568.000 +/- 2308461056.000
Evaluate (epoch 8) -- logp(x) = -2290148442112.000 +/- 53540864000.000
Evaluate (epoch 8) -- logp(x) = -233947.469 +/- 945.957
Evaluate (epoch 8) -- logp(x) = -5877872459776.000 +/- 56198696960.000
Evaluate (epoch 8) -- logp(x) = -3486477.000 +/- 316314.562
Evaluate (epoch 9) -- logp(x) = -205475808.000 +/- 40788600.000
Evaluate (epoch 9) -- logp(x) = -4450878488576.000 +/- 234252042240.000
Evaluate (epoch 9) -- logp(x) = -267707.312 +/- 27.654
Evaluate (epoch 9) -- logp(x) = -53495948378112.000 +/- 1156025810944.000
Evaluate (epoch 9) -- logp(x) = -267858.375 +/- 6.714
Evaluate (epoch 10) -- logp(x) = -345508741120.000 +/- 68673826816.000
Evaluate (epoch 10) -- logp(x) = -48120243159040.000 +/- 2443391795200.000
Evaluate (epoch 10) -- logp(x) = -227181.250 +/- 1554.991
Evaluate (epoch 10) -- logp(x) = -522575079800832.000 +/- 19141769560064.000
Evaluate (epoch 10) -- logp(x) = -49579032576.000 +/- 2547543296.000
Evaluate (epoch 11) -- logp(x) = -2875693989888.000 +/- 571577597952.000
Evaluate (epoch 11) -- logp(x) = -453558468608.000 +/- 63014461440.000
Evaluate (epoch 11) -- logp(x) = -205987.844 +/- 393.399
Evaluate (epoch 11) -- logp(x) = -6156185501696.000 +/- 659614859264.000
Evaluate (epoch 11) -- logp(x) = -212276.672 +/- 119.205
Evaluate (epoch 12) -- logp(x) = -3270939.250 +/- 584323.375
Evaluate (epoch 12) -- logp(x) = -53561966592.000 +/- 3310471936.000
Evaluate (epoch 12) -- logp(x) = -325547.250 +/- 1452.789
Evaluate (epoch 12) -- logp(x) = -82576302080.000 +/- 1832373760.000
Evaluate (epoch 12) -- logp(x) = -692647040.000 +/- 8739515.000
Evaluate (epoch 13) -- logp(x) = -298616.781 +/- 1556.647
Evaluate (epoch 13) -- logp(x) = -219175108608.000 +/- 39419871232.000
Evaluate (epoch 13) -- logp(x) = -290029.156 +/- 152.710
Evaluate (epoch 13) -- logp(x) = -201625042944.000 +/- 15230386176.000
Evaluate (epoch 13) -- logp(x) = -293081.625 +/- 4.401
Evaluate (epoch 14) -- logp(x) = -400657.531 +/- 1909.730
Evaluate (epoch 14) -- logp(x) = -355129.906 +/- 408.307
Evaluate (epoch 14) -- logp(x) = -343363.031 +/- 1364.651
Evaluate (epoch 14) -- logp(x) = -1425671.250 +/- 30564.762
Evaluate (epoch 14) -- logp(x) = -351113.188 +/- 16.469
Evaluate (epoch 15) -- logp(x) = -13409652.000 +/- 2567253.500
Evaluate (epoch 15) -- logp(x) = -10433166336.000 +/- 2051751936.000
Evaluate (epoch 15) -- logp(x) = -1510425.750 +/- 202428.031
Evaluate (epoch 15) -- logp(x) = -14363365376.000 +/- 598987008.000
Evaluate (epoch 15) -- logp(x) = -44418547712.000 +/- 116919568.000
Evaluate (epoch 16) -- logp(x) = -1290525466427392.000 +/- 53494031581184.000
Evaluate (epoch 16) -- logp(x) = -3004282437632.000 +/- 160682672128.000
Evaluate (epoch 16) -- logp(x) = -2088700846538752.000 +/- 145400715542528.000
Evaluate (epoch 16) -- logp(x) = -27329564770304.000 +/- 603488387072.000
Evaluate (epoch 16) -- logp(x) = -531895.875 +/- 9216.616
Evaluate (epoch 17) -- logp(x) = -484535.500 +/- 180.228
Evaluate (epoch 17) -- logp(x) = -15275419648.000 +/- 752433856.000
Evaluate (epoch 17) -- logp(x) = -173131936.000 +/- 34188948.000
Evaluate (epoch 17) -- logp(x) = -140484395008.000 +/- 2086031488.000
Evaluate (epoch 17) -- logp(x) = -504316.344 +/- 18.654
Evaluate (epoch 18) -- logp(x) = -408777.312 +/- 169.662
Evaluate (epoch 18) -- logp(x) = -35690423255040.000 +/- 1191571226624.000
Evaluate (epoch 18) -- logp(x) = -5284504272896.000 +/- 1046487826432.000
Evaluate (epoch 18) -- logp(x) = -299242115563520.000 +/- 4327215464448.000
Evaluate (epoch 18) -- logp(x) = -157669524504576.000 +/- 15047557382144.000
Evaluate (epoch 19) -- logp(x) = -440198.625 +/- 212.981
Evaluate (epoch 19) -- logp(x) = -4544364806144.000 +/- 239486943232.000
Evaluate (epoch 19) -- logp(x) = -482892.750 +/- 6815.025
Evaluate (epoch 19) -- logp(x) = -59921269784576.000 +/- 1199085060096.000
Evaluate (epoch 19) -- logp(x) = -20319309824.000 +/- 625434944.000
Evaluate (epoch 20) -- logp(x) = -382708.812 +/- 1113.301
Evaluate (epoch 20) -- logp(x) = -288189755424768.000 +/- 13678275461120.000
Evaluate (epoch 20) -- logp(x) = -87172072.000 +/- 17186796.000
Evaluate (epoch 20) -- logp(x) = -3753521706958848.000 +/- 63634554224640.000
Evaluate (epoch 20) -- logp(x) = -25834198220668928.000 +/- 1742411994759168.000
Evaluate (epoch 21) -- logp(x) = -5430788489216.000 +/- 619337416704.000
Evaluate (epoch 21) -- logp(x) = -196827127218176.000 +/- 8794402193408.000
Evaluate (epoch 21) -- logp(x) = -15472350199808.000 +/- 1388976537600.000
Evaluate (epoch 21) -- logp(x) = -2695158550233088.000 +/- 72472858722304.000
Evaluate (epoch 21) -- logp(x) = -457442.125 +/- 5665.918
Evaluate (epoch 22) -- logp(x) = -8480720.000 +/- 1016275.188
Evaluate (epoch 22) -- logp(x) = -42995916800.000 +/- 1901111808.000
Evaluate (epoch 22) -- logp(x) = -1626460416.000 +/- 321989952.000
Evaluate (epoch 22) -- logp(x) = -529657561088.000 +/- 9950703616.000
Evaluate (epoch 22) -- logp(x) = -534877.250 +/- 6626.074
Evaluate (epoch 23) -- logp(x) = -514272.719 +/- 16118.069
Evaluate (epoch 23) -- logp(x) = -2750758518784.000 +/- 122350755840.000
Evaluate (epoch 23) -- logp(x) = -48009785344.000 +/- 9507273728.000
Evaluate (epoch 23) -- logp(x) = -29392401072128.000 +/- 720113631232.000
Evaluate (epoch 23) -- logp(x) = -138295492608.000 +/- 4875243520.000
Evaluate (epoch 24) -- logp(x) = -440718262272.000 +/- 32057856000.000
Evaluate (epoch 24) -- logp(x) = -96322954723328.000 +/- 3334771834880.000
Evaluate (epoch 24) -- logp(x) = -41925259264.000 +/- 8302367744.000
{'activation_fn': 'relu',
 'batch_size': 4,
 'channel': -1,
 'cond_label_size': None,
 'conditional': False,
 'data_dir': './data/',
 'dataset': '',
 'device': device(type='cuda', index=0),
 'flip_toy_var_order': False,
 'generate': False,
 'hidden_size': 64,
 'input_dims': torch.Size([3, 64, 64]),
 'input_order': 'random',
 'input_size': 12288,
 'log_interval': 1000,
 'lr': 0.0001,
 'model': 'realnvp',
 'n_blocks': 32,
 'n_components': 1,
 'n_epochs': 50,
 'n_hidden': 1,
 'no_batch_norm': False,
 'no_cuda': False,
 'output_dir': './results/mafencoderFalse_channel-1_nblocks32/',
 'restore_file': None,
 'results_file': 'results.txt',
 'seed': 1,
 'start_epoch': 0,
 'train': True,
 'use_encoder': False}
RealNVP(
  (net): FlowSequential(
    (0): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (1): BatchNorm()
    (2): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (3): BatchNorm()
    (4): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (5): BatchNorm()
    (6): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (7): BatchNorm()
    (8): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (9): BatchNorm()
    (10): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (11): BatchNorm()
    (12): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (13): BatchNorm()
    (14): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (15): BatchNorm()
    (16): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (17): BatchNorm()
    (18): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (19): BatchNorm()
    (20): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (21): BatchNorm()
    (22): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (23): BatchNorm()
    (24): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (25): BatchNorm()
    (26): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (27): BatchNorm()
    (28): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (29): BatchNorm()
    (30): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (31): BatchNorm()
    (32): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (33): BatchNorm()
    (34): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (35): BatchNorm()
    (36): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (37): BatchNorm()
    (38): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (39): BatchNorm()
    (40): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (41): BatchNorm()
    (42): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (43): BatchNorm()
    (44): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (45): BatchNorm()
    (46): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (47): BatchNorm()
    (48): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (49): BatchNorm()
    (50): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (51): BatchNorm()
    (52): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (53): BatchNorm()
    (54): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (55): BatchNorm()
    (56): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (57): BatchNorm()
    (58): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (59): BatchNorm()
    (60): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (61): BatchNorm()
    (62): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (63): BatchNorm()
  )
)
Evaluate (epoch 0) -- logp(x) = -14233.260 +/- 78.796
Evaluate (epoch 0) -- logp(x) = -95852.734 +/- 4034.554
Evaluate (epoch 0) -- logp(x) = -126737.078 +/- 6495.576
Evaluate (epoch 0) -- logp(x) = -1099471.000 +/- 22921.166
Evaluate (epoch 0) -- logp(x) = -347987.406 +/- 18191.471
Evaluate (epoch 1) -- logp(x) = -19088.727 +/- 75.089
Evaluate (epoch 1) -- logp(x) = -4693816.500 +/- 179077.203
Evaluate (epoch 1) -- logp(x) = -292434.656 +/- 22280.365
Evaluate (epoch 1) -- logp(x) = -14249788.000 +/- 400618.250
Evaluate (epoch 1) -- logp(x) = -441433.875 +/- 22467.150
Evaluate (epoch 2) -- logp(x) = -28832.061 +/- 41.053
Evaluate (epoch 2) -- logp(x) = -631298752.000 +/- 17419924.000
Evaluate (epoch 2) -- logp(x) = -9173300.000 +/- 1809547.625
Evaluate (epoch 2) -- logp(x) = -647310720.000 +/- 6079487.500
Evaluate (epoch 2) -- logp(x) = -29681.768 +/- 4.991
Evaluate (epoch 3) -- logp(x) = -39935.898 +/- 42.204
Evaluate (epoch 3) -- logp(x) = -34986237952.000 +/- 551839104.000
Evaluate (epoch 3) -- logp(x) = -1813400448.000 +/- 158180336.000
Evaluate (epoch 3) -- logp(x) = -41131180032.000 +/- 673369152.000
Evaluate (epoch 3) -- logp(x) = -216293648.000 +/- 6792292.500
Evaluate (epoch 4) -- logp(x) = -38086.547 +/- 34.144
Evaluate (epoch 4) -- logp(x) = -4750835712000.000 +/- 190485348352.000
Evaluate (epoch 4) -- logp(x) = -38336.242 +/- 15.088
Evaluate (epoch 4) -- logp(x) = -4937246310400.000 +/- 117205958656.000
Evaluate (epoch 4) -- logp(x) = -38070.504 +/- 2.416
Evaluate (epoch 5) -- logp(x) = -51178.898 +/- 18.826
Evaluate (epoch 5) -- logp(x) = -89074827264.000 +/- 1824197888.000
Evaluate (epoch 5) -- logp(x) = -544693056.000 +/- 107855200.000
Evaluate (epoch 5) -- logp(x) = -103721304064.000 +/- 1243331328.000
Evaluate (epoch 5) -- logp(x) = -50699.836 +/- 19.140
Evaluate (epoch 6) -- logp(x) = -38830.312 +/- 39.034
Evaluate (epoch 6) -- logp(x) = -648934785024.000 +/- 20212023296.000
Evaluate (epoch 6) -- logp(x) = -1262976256.000 +/- 250099024.000
Evaluate (epoch 6) -- logp(x) = -667153268736.000 +/- 26287669248.000
Evaluate (epoch 6) -- logp(x) = -37770.984 +/- 41.184
Evaluate (epoch 7) -- logp(x) = -45645.016 +/- 28.231
Evaluate (epoch 7) -- logp(x) = -81659927986176.000 +/- 3119351595008.000
Evaluate (epoch 7) -- logp(x) = -836884889600.000 +/- 165727911936.000
Evaluate (epoch 7) -- logp(x) = -105301172813824.000 +/- 2806763487232.000
Evaluate (epoch 7) -- logp(x) = -45269.715 +/- 24.750
Evaluate (epoch 8) -- logp(x) = -64254.980 +/- 49.084
Evaluate (epoch 8) -- logp(x) = -386161115136.000 +/- 18632673280.000
Evaluate (epoch 8) -- logp(x) = -63440.055 +/- 247.518
Evaluate (epoch 8) -- logp(x) = -648776712192.000 +/- 9791432704.000
Evaluate (epoch 8) -- logp(x) = -64328.004 +/- 3.322
Evaluate (epoch 9) -- logp(x) = -48794.664 +/- 39.890
Evaluate (epoch 9) -- logp(x) = -1866220699648.000 +/- 71771004928.000
Evaluate (epoch 9) -- logp(x) = -813116800.000 +/- 161011488.000
Evaluate (epoch 9) -- logp(x) = -2234922303488.000 +/- 34023424000.000
Evaluate (epoch 9) -- logp(x) = -48367.410 +/- 0.824
Evaluate (epoch 10) -- logp(x) = -42883.035 +/- 25.242
Evaluate (epoch 10) -- logp(x) = -2362530332672.000 +/- 90991484928.000
Evaluate (epoch 10) -- logp(x) = -43452.000 +/- 121.874
Evaluate (epoch 10) -- logp(x) = -2964593311744.000 +/- 141125943296.000
Evaluate (epoch 10) -- logp(x) = -42885.336 +/- 9.000
Evaluate (epoch 11) -- logp(x) = -44064.871 +/- 34.989
Evaluate (epoch 11) -- logp(x) = -12103479984128.000 +/- 459873386496.000
Evaluate (epoch 11) -- logp(x) = -14522398720.000 +/- 2875854848.000
Evaluate (epoch 11) -- logp(x) = -11719296417792.000 +/- 328323563520.000
Evaluate (epoch 11) -- logp(x) = -43622.285 +/- 13.170
Evaluate (epoch 12) -- logp(x) = -128263380992.000 +/- 25493823488.000
Evaluate (epoch 12) -- logp(x) = -60069.289 +/- 21.744
Evaluate (epoch 12) -- logp(x) = -58813.164 +/- 88.700
Evaluate (epoch 12) -- logp(x) = -60622.867 +/- 15.093
Evaluate (epoch 12) -- logp(x) = -58027.977 +/- 9.351
Evaluate (epoch 13) -- logp(x) = -50488.945 +/- 23.780
Evaluate (epoch 13) -- logp(x) = -11873682456576.000 +/- 778756685824.000
Evaluate (epoch 13) -- logp(x) = -663537188864.000 +/- 131377250304.000
Evaluate (epoch 13) -- logp(x) = -7035904065536.000 +/- 721333125120.000
Evaluate (epoch 13) -- logp(x) = -44857.734 +/- 84.091
Evaluate (epoch 14) -- logp(x) = -45558.719 +/- 30.503
Evaluate (epoch 14) -- logp(x) = -15623438467072.000 +/- 1580172050432.000
Evaluate (epoch 14) -- logp(x) = -15855360000.000 +/- 3132137472.000
Evaluate (epoch 14) -- logp(x) = -9899475992576.000 +/- 1078626746368.000
Evaluate (epoch 14) -- logp(x) = -45729.680 +/- 6.334
Evaluate (epoch 15) -- logp(x) = -49084.633 +/- 20.719
Evaluate (epoch 15) -- logp(x) = -58149.000 +/- 850.106
Evaluate (epoch 15) -- logp(x) = -4302258831360.000 +/- 851974225920.000
Evaluate (epoch 15) -- logp(x) = -2238420319469568.000 +/- 425204345995264.000
Evaluate (epoch 15) -- logp(x) = -287758178320384.000 +/- 8539622342656.000
Evaluate (epoch 16) -- logp(x) = -609097351168.000 +/- 39557824512.000
Evaluate (epoch 16) -- logp(x) = -143444.281 +/- 5169.705
Evaluate (epoch 16) -- logp(x) = -6354930176.000 +/- 1258450560.000
Evaluate (epoch 16) -- logp(x) = -160334.000 +/- 7008.995
Evaluate (epoch 16) -- logp(x) = -58275.219 +/- 325.550
Evaluate (epoch 17) -- logp(x) = -52031.234 +/- 267.784
Evaluate (epoch 17) -- logp(x) = -23335972.000 +/- 1742631.250
Evaluate (epoch 17) -- logp(x) = -79681.359 +/- 3627.579
Evaluate (epoch 17) -- logp(x) = -11942572.000 +/- 1150490.250
Evaluate (epoch 17) -- logp(x) = -59808.309 +/- 5.935
Evaluate (epoch 18) -- logp(x) = -62208.285 +/- 16.145
Evaluate (epoch 18) -- logp(x) = -789394496.000 +/- 146016944.000
Evaluate (epoch 18) -- logp(x) = -62447.547 +/- 37.381
Evaluate (epoch 18) -- logp(x) = -5151990272.000 +/- 562299968.000
Evaluate (epoch 18) -- logp(x) = -60606.062 +/- 7.239
Evaluate (epoch 19) -- logp(x) = -57894.332 +/- 29.206
Evaluate (epoch 19) -- logp(x) = -10749530112.000 +/- 1971471360.000
Evaluate (epoch 19) -- logp(x) = -59185.234 +/- 203.923
Evaluate (epoch 19) -- logp(x) = -8903212032.000 +/- 980861632.000
Evaluate (epoch 19) -- logp(x) = -56781.336 +/- 28.918
Evaluate (epoch 20) -- logp(x) = -55283.891 +/- 88.966
Evaluate (epoch 20) -- logp(x) = -751268.500 +/- 79124.141
Evaluate (epoch 20) -- logp(x) = -58095.625 +/- 359.109
Evaluate (epoch 20) -- logp(x) = -1080012.250 +/- 120699.281
Evaluate (epoch 20) -- logp(x) = -56185.656 +/- 24.063
Evaluate (epoch 21) -- logp(x) = -49614.457 +/- 34.862
Evaluate (epoch 21) -- logp(x) = -5621677.500 +/- 990621.500
Evaluate (epoch 21) -- logp(x) = -50764.965 +/- 255.997
Evaluate (epoch 21) -- logp(x) = -6626641.500 +/- 1096745.750
Evaluate (epoch 21) -- logp(x) = -48089.109 +/- 68.797
Evaluate (epoch 22) -- logp(x) = -47866.660 +/- 113.671
Evaluate (epoch 22) -- logp(x) = -175923.312 +/- 14015.790
Evaluate (epoch 22) -- logp(x) = -49147.059 +/- 149.766
Evaluate (epoch 22) -- logp(x) = -286533.344 +/- 25826.025
Evaluate (epoch 22) -- logp(x) = -46854.945 +/- 1.466
Evaluate (epoch 23) -- logp(x) = -52442.645 +/- 73.399
Evaluate (epoch 23) -- logp(x) = -104946.797 +/- 5294.562
Evaluate (epoch 23) -- logp(x) = -53955.516 +/- 122.850
Evaluate (epoch 23) -- logp(x) = -167471.438 +/- 13543.430
Evaluate (epoch 23) -- logp(x) = -52372.480 +/- 21.086
Evaluate (epoch 24) -- logp(x) = -49768.938 +/- 30.434
Evaluate (epoch 24) -- logp(x) = -73997184.000 +/- 14359664.000
Evaluate (epoch 24) -- logp(x) = -51004.996 +/- 193.051
Evaluate (epoch 24) -- logp(x) = -524908608.000 +/- 98720144.000
Evaluate (epoch 24) -- logp(x) = -139569774592.000 +/- 7550397440.000
Evaluate (epoch 25) -- logp(x) = -54657.383 +/- 73.009
Evaluate (epoch 25) -- logp(x) = -3049761792.000 +/- 302628384.000
Evaluate (epoch 25) -- logp(x) = -94958056.000 +/- 18793486.000
Evaluate (epoch 25) -- logp(x) = -8111697408.000 +/- 836966720.000
Evaluate (epoch 25) -- logp(x) = -6856203304960.000 +/- 73044893696.000
Evaluate (epoch 26) -- logp(x) = -52566.617 +/- 96.554
Evaluate (epoch 26) -- logp(x) = -2018697088.000 +/- 254741696.000
Evaluate (epoch 26) -- logp(x) = -46582451929088.000 +/- 7469675315200.000
Evaluate (epoch 26) -- logp(x) = -2440598528.000 +/- 281639936.000
Evaluate (epoch 26) -- logp(x) = -1989549752320.000 +/- 54129692672.000
Evaluate (epoch 27) -- logp(x) = -53669.090 +/- 122.030
Evaluate (epoch 27) -- logp(x) = -298337.375 +/- 34568.172
Evaluate (epoch 27) -- logp(x) = -54316.004 +/- 187.154
Evaluate (epoch 27) -- logp(x) = -767423.375 +/- 111144.969
Evaluate (epoch 27) -- logp(x) = -53543.043 +/- 43.473
Evaluate (epoch 28) -- logp(x) = -54503.730 +/- 68.269
Evaluate (epoch 28) -- logp(x) = -210435407872.000 +/- 22599227392.000
Evaluate (epoch 28) -- logp(x) = -13969073.000 +/- 2755324.500
Evaluate (epoch 28) -- logp(x) = -157702275072.000 +/- 30901143552.000
Evaluate (epoch 28) -- logp(x) = -54570.695 +/- 16.592
Evaluate (epoch 29) -- logp(x) = -77352.406 +/- 43.233
Evaluate (epoch 29) -- logp(x) = -106864885760.000 +/- 8491059200.000
Evaluate (epoch 29) -- logp(x) = -77758.375 +/- 92.375
Evaluate (epoch 29) -- logp(x) = -77473333248.000 +/- 9341949952.000
Evaluate (epoch 29) -- logp(x) = -78218.680 +/- 8.893
Evaluate (epoch 30) -- logp(x) = -53772.098 +/- 55.093
Evaluate (epoch 30) -- logp(x) = -10407103168512.000 +/- 492631097344.000
Evaluate (epoch 30) -- logp(x) = -414446223360.000 +/- 82072576000.000
Evaluate (epoch 30) -- logp(x) = -55350250176512.000 +/- 4174356676608.000
Evaluate (epoch 30) -- logp(x) = -55807.008 +/- 5.027
Evaluate (epoch 31) -- logp(x) = -100084.172 +/- 156.252
Evaluate (epoch 31) -- logp(x) = -5215449.000 +/- 306192.562
Evaluate (epoch 31) -- logp(x) = -473392.562 +/- 73066.000
Evaluate (epoch 31) -- logp(x) = -8931790.000 +/- 777364.000
Evaluate (epoch 31) -- logp(x) = -104841.703 +/- 16.583
Evaluate (epoch 32) -- logp(x) = -51587.836 +/- 48.896
Evaluate (epoch 32) -- logp(x) = -516019328.000 +/- 42322444.000
Evaluate (epoch 32) -- logp(x) = -131144.281 +/- 15824.662
Evaluate (epoch 32) -- logp(x) = -963608320.000 +/- 57590456.000
Evaluate (epoch 32) -- logp(x) = -52295.531 +/- 23.429
Evaluate (epoch 33) -- logp(x) = -50828.312 +/- 18.810
Evaluate (epoch 33) -- logp(x) = -464854144.000 +/- 41582924.000
Evaluate (epoch 33) -- logp(x) = -12199863.000 +/- 2405513.750
Evaluate (epoch 33) -- logp(x) = -643988736.000 +/- 92109264.000
Evaluate (epoch 33) -- logp(x) = -50182.816 +/- 190.846
Evaluate (epoch 34) -- logp(x) = -55631.855 +/- 52.479
Evaluate (epoch 34) -- logp(x) = -405965438976.000 +/- 16876007424.000
Evaluate (epoch 34) -- logp(x) = -14946072576.000 +/- 1758120064.000
Evaluate (epoch 34) -- logp(x) = -517554307072.000 +/- 27909441536.000
Evaluate (epoch 34) -- logp(x) = -1987728128.000 +/- 142183632.000
Evaluate (epoch 35) -- logp(x) = -63654.680 +/- 34.816
Evaluate (epoch 35) -- logp(x) = -22825748480.000 +/- 294730464.000
Evaluate (epoch 35) -- logp(x) = -3791781632.000 +/- 750871424.000
Evaluate (epoch 35) -- logp(x) = -17066455040.000 +/- 1492905600.000
Evaluate (epoch 35) -- logp(x) = -60833.891 +/- 313.477
Evaluate (epoch 36) -- logp(x) = -53838.477 +/- 59.912
Evaluate (epoch 36) -- logp(x) = -30783379456.000 +/- 1486949760.000
Evaluate (epoch 36) -- logp(x) = -2216352256.000 +/- 347028224.000
Evaluate (epoch 36) -- logp(x) = -21545209856.000 +/- 1886462848.000
Evaluate (epoch 36) -- logp(x) = -1114590848.000 +/- 36561332.000
Evaluate (epoch 37) -- logp(x) = -74949.633 +/- 75.084
Evaluate (epoch 37) -- logp(x) = -3244894208.000 +/- 130439512.000
Evaluate (epoch 37) -- logp(x) = -6589461504.000 +/- 792682176.000
Evaluate (epoch 37) -- logp(x) = -3969114880.000 +/- 158796224.000
Evaluate (epoch 37) -- logp(x) = -8492248.000 +/- 132944.812
Evaluate (epoch 38) -- logp(x) = -39996.422 +/- 119.429
Evaluate (epoch 38) -- logp(x) = -340781888.000 +/- 13820339.000
Evaluate (epoch 38) -- logp(x) = -18876914.000 +/- 2786537.250
Evaluate (epoch 38) -- logp(x) = -345089216.000 +/- 24918718.000
Evaluate (epoch 38) -- logp(x) = -15128271.000 +/- 293351.312
Evaluate (epoch 39) -- logp(x) = -44228.746 +/- 64.770
Evaluate (epoch 39) -- logp(x) = -216682987520.000 +/- 7642531328.000
Evaluate (epoch 39) -- logp(x) = -4386127872.000 +/- 825247040.000
Evaluate (epoch 39) -- logp(x) = -309399322624.000 +/- 8306674688.000
Evaluate (epoch 39) -- logp(x) = -205486944.000 +/- 7961779.000
Evaluate (epoch 40) -- logp(x) = -46869.051 +/- 51.989
Evaluate (epoch 40) -- logp(x) = -58519588864.000 +/- 1948071552.000
Evaluate (epoch 40) -- logp(x) = -836083072.000 +/- 158905888.000
Evaluate (epoch 40) -- logp(x) = -97768964096.000 +/- 3883276544.000
Evaluate (epoch 40) -- logp(x) = -50348.359 +/- 22.824
Evaluate (epoch 41) -- logp(x) = -72652.000 +/- 37.380
Evaluate (epoch 41) -- logp(x) = -151367488.000 +/- 5830656.000
Evaluate (epoch 41) -- logp(x) = -90912.500 +/- 3398.725
Evaluate (epoch 41) -- logp(x) = -214747504.000 +/- 2238673.000
Evaluate (epoch 41) -- logp(x) = -73860.711 +/- 12.172
Evaluate (epoch 42) -- logp(x) = -49670.969 +/- 36.517
Evaluate (epoch 42) -- logp(x) = -44589408256.000 +/- 2111306368.000
Evaluate (epoch 42) -- logp(x) = -1602508800.000 +/- 230534704.000
Evaluate (epoch 42) -- logp(x) = -63248982016.000 +/- 2842499840.000
Evaluate (epoch 42) -- logp(x) = -702058432.000 +/- 107480144.000
Evaluate (epoch 43) -- logp(x) = -54213.125 +/- 29.799
Evaluate (epoch 43) -- logp(x) = -18671902720.000 +/- 808195072.000
Evaluate (epoch 43) -- logp(x) = -144088704.000 +/- 28522740.000
Evaluate (epoch 43) -- logp(x) = -29290481664.000 +/- 874718976.000
Evaluate (epoch 43) -- logp(x) = -56293.820 +/- 25.987
Evaluate (epoch 44) -- logp(x) = -47961.852 +/- 22.685
Evaluate (epoch 44) -- logp(x) = -135573200896.000 +/- 4970395648.000
Evaluate (epoch 44) -- logp(x) = -828767808.000 +/- 164101840.000
Evaluate (epoch 44) -- logp(x) = -162092728320.000 +/- 2253871616.000
Evaluate (epoch 44) -- logp(x) = -51736.555 +/- 5.469
Evaluate (epoch 45) -- logp(x) = -50168.961 +/- 22.179
Evaluate (epoch 45) -- logp(x) = -1766628524032.000 +/- 85352652800.000
Evaluate (epoch 45) -- logp(x) = -16965986304.000 +/- 2018073088.000
Evaluate (epoch 45) -- logp(x) = -3279696166912.000 +/- 100077617152.000
Evaluate (epoch 45) -- logp(x) = -52004.164 +/- 38.942
Evaluate (epoch 46) -- logp(x) = -49827.676 +/- 32.825
Evaluate (epoch 46) -- logp(x) = -138780737536.000 +/- 5109817856.000
Evaluate (epoch 46) -- logp(x) = -223069306880.000 +/- 43336216576.000
Evaluate (epoch 46) -- logp(x) = -258282291200.000 +/- 2927352320.000
Evaluate (epoch 46) -- logp(x) = -52999.332 +/- 501.904
Evaluate (epoch 47) -- logp(x) = -90323.906 +/- 26.292
Evaluate (epoch 47) -- logp(x) = -423109248.000 +/- 24915940.000
Evaluate (epoch 47) -- logp(x) = -113127.125 +/- 4310.743
Evaluate (epoch 47) -- logp(x) = -734534656.000 +/- 22779288.000
Evaluate (epoch 47) -- logp(x) = -91220.625 +/- 11.993
Evaluate (epoch 48) -- logp(x) = -51020.262 +/- 15.773
Evaluate (epoch 48) -- logp(x) = -35879276544.000 +/- 2848555264.000
Evaluate (epoch 48) -- logp(x) = -348155776.000 +/- 66310392.000
Evaluate (epoch 48) -- logp(x) = -123800158208.000 +/- 9605967872.000
Evaluate (epoch 48) -- logp(x) = -53922.582 +/- 43.121
Evaluate (epoch 49) -- logp(x) = -50673.410 +/- 23.494
Evaluate (epoch 49) -- logp(x) = -1193507553280.000 +/- 70682042368.000
Evaluate (epoch 49) -- logp(x) = -1203192448.000 +/- 238257200.000
Evaluate (epoch 49) -- logp(x) = -1793651376128.000 +/- 66928644096.000
Evaluate (epoch 49) -- logp(x) = -52126.098 +/- 17.824
{'activation_fn': 'relu',
 'batch_size': 4,
 'channel': -1,
 'cond_label_size': None,
 'conditional': False,
 'data_dir': './data/',
 'dataset': '',
 'device': device(type='cuda', index=0),
 'flip_toy_var_order': False,
 'generate': False,
 'hidden_size': 64,
 'input_dims': torch.Size([3, 64, 64]),
 'input_order': 'random',
 'input_size': 12288,
 'log_interval': 1000,
 'lr': 0.0001,
 'model': 'realnvp',
 'n_blocks': 32,
 'n_components': 1,
 'n_epochs': 50,
 'n_hidden': 1,
 'no_batch_norm': False,
 'no_cuda': False,
 'output_dir': './results/mafencoderFalse_channel-1_nblocks32/',
 'restore_file': None,
 'results_file': 'results.txt',
 'seed': 1,
 'start_epoch': 0,
 'train': False,
 'use_encoder': False}
RealNVP(
  (net): FlowSequential(
    (0): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (1): BatchNorm()
    (2): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (3): BatchNorm()
    (4): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (5): BatchNorm()
    (6): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (7): BatchNorm()
    (8): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (9): BatchNorm()
    (10): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (11): BatchNorm()
    (12): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (13): BatchNorm()
    (14): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (15): BatchNorm()
    (16): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (17): BatchNorm()
    (18): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (19): BatchNorm()
    (20): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (21): BatchNorm()
    (22): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (23): BatchNorm()
    (24): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (25): BatchNorm()
    (26): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (27): BatchNorm()
    (28): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (29): BatchNorm()
    (30): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (31): BatchNorm()
    (32): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (33): BatchNorm()
    (34): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (35): BatchNorm()
    (36): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (37): BatchNorm()
    (38): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (39): BatchNorm()
    (40): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (41): BatchNorm()
    (42): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (43): BatchNorm()
    (44): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (45): BatchNorm()
    (46): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (47): BatchNorm()
    (48): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (49): BatchNorm()
    (50): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (51): BatchNorm()
    (52): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (53): BatchNorm()
    (54): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (55): BatchNorm()
    (56): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (57): BatchNorm()
    (58): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (59): BatchNorm()
    (60): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (61): BatchNorm()
    (62): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (63): BatchNorm()
  )
)
Evaluate logp(x) = -18857.855 +/- 79.836
Evaluate logp(x) = -25219.863 +/- 472.002
Evaluate logp(x) = -14047.594 +/- 39.657
Evaluate logp(x) = -72365.195 +/- 1665.288
Evaluate logp(x) = -13811.287 +/- 3.786
{'activation_fn': 'relu',
 'batch_size': 4,
 'channel': -1,
 'cond_label_size': None,
 'conditional': False,
 'data_dir': './data/',
 'dataset': '',
 'device': device(type='cuda', index=0),
 'flip_toy_var_order': False,
 'generate': False,
 'hidden_size': 64,
 'input_dims': torch.Size([3, 64, 64]),
 'input_order': 'random',
 'input_size': 12288,
 'log_interval': 1000,
 'lr': 0.0001,
 'model': 'realnvp',
 'n_blocks': 32,
 'n_components': 1,
 'n_epochs': 50,
 'n_hidden': 1,
 'no_batch_norm': False,
 'no_cuda': False,
 'output_dir': './results/mafencoderFalse_channel-1_nblocks32/',
 'restore_file': None,
 'results_file': 'results.txt',
 'seed': 1,
 'start_epoch': 0,
 'train': False,
 'use_encoder': False}
RealNVP(
  (net): FlowSequential(
    (0): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (1): BatchNorm()
    (2): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (3): BatchNorm()
    (4): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (5): BatchNorm()
    (6): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (7): BatchNorm()
    (8): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (9): BatchNorm()
    (10): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (11): BatchNorm()
    (12): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (13): BatchNorm()
    (14): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (15): BatchNorm()
    (16): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (17): BatchNorm()
    (18): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (19): BatchNorm()
    (20): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (21): BatchNorm()
    (22): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (23): BatchNorm()
    (24): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (25): BatchNorm()
    (26): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (27): BatchNorm()
    (28): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (29): BatchNorm()
    (30): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (31): BatchNorm()
    (32): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (33): BatchNorm()
    (34): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (35): BatchNorm()
    (36): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (37): BatchNorm()
    (38): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (39): BatchNorm()
    (40): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (41): BatchNorm()
    (42): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (43): BatchNorm()
    (44): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (45): BatchNorm()
    (46): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (47): BatchNorm()
    (48): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (49): BatchNorm()
    (50): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (51): BatchNorm()
    (52): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (53): BatchNorm()
    (54): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (55): BatchNorm()
    (56): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (57): BatchNorm()
    (58): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (59): BatchNorm()
    (60): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (61): BatchNorm()
    (62): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (63): BatchNorm()
  )
)
Evaluate logp(x) = -18857.855 +/- 79.836
Evaluate logp(x) = -25219.863 +/- 472.002
Evaluate logp(x) = -14047.594 +/- 39.657
Evaluate logp(x) = -71847.672 +/- 1932.372
Evaluate logp(x) = -13811.287 +/- 3.786
{'activation_fn': 'relu',
 'batch_size': 4,
 'channel': -1,
 'cond_label_size': None,
 'conditional': False,
 'data_dir': './data/',
 'dataset': '',
 'device': device(type='cuda', index=0),
 'flip_toy_var_order': False,
 'generate': False,
 'hidden_size': 64,
 'input_dims': torch.Size([3, 64, 64]),
 'input_order': 'random',
 'input_size': 12288,
 'log_interval': 1000,
 'lr': 0.0001,
 'model': 'realnvp',
 'n_blocks': 32,
 'n_components': 1,
 'n_epochs': 50,
 'n_hidden': 1,
 'no_batch_norm': False,
 'no_cuda': False,
 'output_dir': './results/mafencoderFalse_channel-1_nblocks32/',
 'restore_file': None,
 'results_file': 'results.txt',
 'seed': 1,
 'start_epoch': 0,
 'train': False,
 'use_encoder': False}
RealNVP(
  (net): FlowSequential(
    (0): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (1): BatchNorm()
    (2): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (3): BatchNorm()
    (4): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (5): BatchNorm()
    (6): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (7): BatchNorm()
    (8): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (9): BatchNorm()
    (10): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (11): BatchNorm()
    (12): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (13): BatchNorm()
    (14): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (15): BatchNorm()
    (16): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (17): BatchNorm()
    (18): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (19): BatchNorm()
    (20): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (21): BatchNorm()
    (22): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (23): BatchNorm()
    (24): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (25): BatchNorm()
    (26): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (27): BatchNorm()
    (28): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (29): BatchNorm()
    (30): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (31): BatchNorm()
    (32): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (33): BatchNorm()
    (34): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (35): BatchNorm()
    (36): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (37): BatchNorm()
    (38): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (39): BatchNorm()
    (40): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (41): BatchNorm()
    (42): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (43): BatchNorm()
    (44): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (45): BatchNorm()
    (46): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (47): BatchNorm()
    (48): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (49): BatchNorm()
    (50): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (51): BatchNorm()
    (52): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (53): BatchNorm()
    (54): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (55): BatchNorm()
    (56): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (57): BatchNorm()
    (58): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (59): BatchNorm()
    (60): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (61): BatchNorm()
    (62): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (63): BatchNorm()
  )
)
Evaluate logp(x) = -18857.855 +/- 79.836
Evaluate logp(x) = -25219.863 +/- 472.002
Evaluate logp(x) = -14047.594 +/- 39.657
Evaluate logp(x) = -75883.586 +/- 1699.976
Evaluate logp(x) = -13811.287 +/- 3.786
{'activation_fn': 'relu',
 'batch_size': 4,
 'channel': -1,
 'cond_label_size': None,
 'conditional': False,
 'data_dir': './data/',
 'dataset': '',
 'device': device(type='cuda', index=0),
 'flip_toy_var_order': False,
 'generate': False,
 'hidden_size': 64,
 'input_dims': torch.Size([3, 64, 64]),
 'input_order': 'random',
 'input_size': 12288,
 'log_interval': 1000,
 'lr': 0.0001,
 'model': 'realnvp',
 'n_blocks': 32,
 'n_components': 1,
 'n_epochs': 50,
 'n_hidden': 1,
 'no_batch_norm': False,
 'no_cuda': False,
 'output_dir': './results/mafencoderFalse_channel-1_nblocks32/',
 'restore_file': None,
 'results_file': 'results.txt',
 'seed': 1,
 'start_epoch': 0,
 'train': False,
 'use_encoder': False}
RealNVP(
  (net): FlowSequential(
    (0): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (1): BatchNorm()
    (2): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (3): BatchNorm()
    (4): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (5): BatchNorm()
    (6): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (7): BatchNorm()
    (8): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (9): BatchNorm()
    (10): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (11): BatchNorm()
    (12): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (13): BatchNorm()
    (14): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (15): BatchNorm()
    (16): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (17): BatchNorm()
    (18): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (19): BatchNorm()
    (20): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (21): BatchNorm()
    (22): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (23): BatchNorm()
    (24): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (25): BatchNorm()
    (26): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (27): BatchNorm()
    (28): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (29): BatchNorm()
    (30): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (31): BatchNorm()
    (32): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (33): BatchNorm()
    (34): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (35): BatchNorm()
    (36): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (37): BatchNorm()
    (38): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (39): BatchNorm()
    (40): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (41): BatchNorm()
    (42): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (43): BatchNorm()
    (44): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (45): BatchNorm()
    (46): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (47): BatchNorm()
    (48): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (49): BatchNorm()
    (50): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (51): BatchNorm()
    (52): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (53): BatchNorm()
    (54): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (55): BatchNorm()
    (56): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (57): BatchNorm()
    (58): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (59): BatchNorm()
    (60): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (61): BatchNorm()
    (62): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (63): BatchNorm()
  )
)
Evaluate logp(x) = -18857.855 +/- 79.836
Evaluate logp(x) = -25219.863 +/- 472.002
Evaluate logp(x) = -14047.594 +/- 39.657
Evaluate logp(x) = -73438.547 +/- 1732.882
Evaluate logp(x) = -13811.287 +/- 3.786
{'activation_fn': 'relu',
 'batch_size': 4,
 'channel': -1,
 'cond_label_size': None,
 'conditional': False,
 'data_dir': './data/',
 'dataset': '',
 'device': device(type='cuda', index=0),
 'flip_toy_var_order': False,
 'generate': False,
 'hidden_size': 64,
 'input_dims': torch.Size([3, 64, 64]),
 'input_order': 'random',
 'input_size': 12288,
 'log_interval': 1000,
 'lr': 0.0001,
 'model': 'realnvp',
 'n_blocks': 32,
 'n_components': 1,
 'n_epochs': 50,
 'n_hidden': 1,
 'no_batch_norm': False,
 'no_cuda': False,
 'output_dir': './results/mafencoderFalse_channel-1_nblocks32/',
 'restore_file': None,
 'results_file': 'results.txt',
 'seed': 1,
 'start_epoch': 0,
 'train': False,
 'use_encoder': False}
RealNVP(
  (net): FlowSequential(
    (0): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (1): BatchNorm()
    (2): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (3): BatchNorm()
    (4): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (5): BatchNorm()
    (6): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (7): BatchNorm()
    (8): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (9): BatchNorm()
    (10): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (11): BatchNorm()
    (12): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (13): BatchNorm()
    (14): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (15): BatchNorm()
    (16): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (17): BatchNorm()
    (18): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (19): BatchNorm()
    (20): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (21): BatchNorm()
    (22): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (23): BatchNorm()
    (24): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (25): BatchNorm()
    (26): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (27): BatchNorm()
    (28): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (29): BatchNorm()
    (30): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (31): BatchNorm()
    (32): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (33): BatchNorm()
    (34): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (35): BatchNorm()
    (36): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (37): BatchNorm()
    (38): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (39): BatchNorm()
    (40): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (41): BatchNorm()
    (42): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (43): BatchNorm()
    (44): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (45): BatchNorm()
    (46): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (47): BatchNorm()
    (48): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (49): BatchNorm()
    (50): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (51): BatchNorm()
    (52): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (53): BatchNorm()
    (54): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (55): BatchNorm()
    (56): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (57): BatchNorm()
    (58): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (59): BatchNorm()
    (60): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (61): BatchNorm()
    (62): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (63): BatchNorm()
  )
)
Evaluate logp(x) = -18857.855 +/- 79.836
Evaluate logp(x) = -25219.863 +/- 472.002
Evaluate logp(x) = -14047.594 +/- 39.657
Evaluate logp(x) = -73615.414 +/- 1704.671
Evaluate logp(x) = -13811.287 +/- 3.786
{'activation_fn': 'relu',
 'batch_size': 4,
 'channel': -1,
 'cond_label_size': None,
 'conditional': False,
 'data_dir': './data/',
 'dataset': '',
 'device': device(type='cuda', index=0),
 'flip_toy_var_order': False,
 'generate': False,
 'hidden_size': 64,
 'input_dims': torch.Size([3, 64, 64]),
 'input_order': 'random',
 'input_size': 12288,
 'log_interval': 1000,
 'lr': 0.0001,
 'model': 'realnvp',
 'n_blocks': 32,
 'n_components': 1,
 'n_epochs': 50,
 'n_hidden': 1,
 'no_batch_norm': False,
 'no_cuda': False,
 'output_dir': './results/mafencoderFalse_channel-1_nblocks32/',
 'restore_file': None,
 'results_file': 'results.txt',
 'seed': 1,
 'start_epoch': 0,
 'train': False,
 'use_encoder': False}
RealNVP(
  (net): FlowSequential(
    (0): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (1): BatchNorm()
    (2): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (3): BatchNorm()
    (4): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (5): BatchNorm()
    (6): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (7): BatchNorm()
    (8): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (9): BatchNorm()
    (10): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (11): BatchNorm()
    (12): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (13): BatchNorm()
    (14): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (15): BatchNorm()
    (16): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (17): BatchNorm()
    (18): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (19): BatchNorm()
    (20): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (21): BatchNorm()
    (22): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (23): BatchNorm()
    (24): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (25): BatchNorm()
    (26): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (27): BatchNorm()
    (28): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (29): BatchNorm()
    (30): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (31): BatchNorm()
    (32): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (33): BatchNorm()
    (34): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (35): BatchNorm()
    (36): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (37): BatchNorm()
    (38): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (39): BatchNorm()
    (40): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (41): BatchNorm()
    (42): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (43): BatchNorm()
    (44): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (45): BatchNorm()
    (46): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (47): BatchNorm()
    (48): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (49): BatchNorm()
    (50): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (51): BatchNorm()
    (52): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (53): BatchNorm()
    (54): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (55): BatchNorm()
    (56): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (57): BatchNorm()
    (58): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (59): BatchNorm()
    (60): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (61): BatchNorm()
    (62): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (63): BatchNorm()
  )
)
Evaluate logp(x) = -18857.855 +/- 79.836
Evaluate logp(x) = -25219.863 +/- 472.002
Evaluate logp(x) = -14047.594 +/- 39.657
Evaluate logp(x) = -78263.711 +/- 1651.986
Evaluate logp(x) = -13811.287 +/- 3.786
{'activation_fn': 'relu',
 'batch_size': 4,
 'channel': -1,
 'cond_label_size': None,
 'conditional': False,
 'data_dir': './data/',
 'dataset': '',
 'device': device(type='cuda', index=0),
 'flip_toy_var_order': False,
 'generate': False,
 'hidden_size': 64,
 'input_dims': torch.Size([3, 64, 64]),
 'input_order': 'random',
 'input_size': 12288,
 'log_interval': 1000,
 'lr': 0.0001,
 'model': 'realnvp',
 'n_blocks': 32,
 'n_components': 1,
 'n_epochs': 50,
 'n_hidden': 1,
 'no_batch_norm': False,
 'no_cuda': False,
 'output_dir': './results/mafencoderFalse_channel-1_nblocks32/',
 'restore_file': None,
 'results_file': 'results.txt',
 'seed': 1,
 'start_epoch': 0,
 'train': False,
 'use_encoder': False}
RealNVP(
  (net): FlowSequential(
    (0): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (1): BatchNorm()
    (2): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (3): BatchNorm()
    (4): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (5): BatchNorm()
    (6): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (7): BatchNorm()
    (8): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (9): BatchNorm()
    (10): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (11): BatchNorm()
    (12): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (13): BatchNorm()
    (14): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (15): BatchNorm()
    (16): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (17): BatchNorm()
    (18): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (19): BatchNorm()
    (20): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (21): BatchNorm()
    (22): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (23): BatchNorm()
    (24): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (25): BatchNorm()
    (26): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (27): BatchNorm()
    (28): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (29): BatchNorm()
    (30): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (31): BatchNorm()
    (32): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (33): BatchNorm()
    (34): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (35): BatchNorm()
    (36): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (37): BatchNorm()
    (38): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (39): BatchNorm()
    (40): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (41): BatchNorm()
    (42): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (43): BatchNorm()
    (44): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (45): BatchNorm()
    (46): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (47): BatchNorm()
    (48): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (49): BatchNorm()
    (50): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (51): BatchNorm()
    (52): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (53): BatchNorm()
    (54): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (55): BatchNorm()
    (56): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (57): BatchNorm()
    (58): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (59): BatchNorm()
    (60): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (61): BatchNorm()
    (62): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (63): BatchNorm()
  )
)
Evaluate logp(x) = -18857.855 +/- 79.836
Evaluate logp(x) = -25219.863 +/- 472.002
Evaluate logp(x) = -14047.594 +/- 39.657
Evaluate logp(x) = -75972.047 +/- 1544.228
Evaluate logp(x) = -13811.287 +/- 3.786
{'activation_fn': 'relu',
 'batch_size': 4,
 'channel': -1,
 'cond_label_size': None,
 'conditional': False,
 'data_dir': './data/',
 'dataset': '',
 'device': device(type='cuda', index=0),
 'flip_toy_var_order': False,
 'generate': False,
 'hidden_size': 64,
 'input_dims': torch.Size([3, 64, 64]),
 'input_order': 'random',
 'input_size': 12288,
 'log_interval': 1000,
 'lr': 0.0001,
 'model': 'realnvp',
 'n_blocks': 32,
 'n_components': 1,
 'n_epochs': 50,
 'n_hidden': 1,
 'no_batch_norm': False,
 'no_cuda': False,
 'output_dir': './results/mafencoderFalse_channel-1_nblocks32/',
 'restore_file': None,
 'results_file': 'results.txt',
 'seed': 1,
 'start_epoch': 0,
 'train': False,
 'use_encoder': False}
RealNVP(
  (net): FlowSequential(
    (0): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (1): BatchNorm()
    (2): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (3): BatchNorm()
    (4): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (5): BatchNorm()
    (6): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (7): BatchNorm()
    (8): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (9): BatchNorm()
    (10): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (11): BatchNorm()
    (12): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (13): BatchNorm()
    (14): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (15): BatchNorm()
    (16): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (17): BatchNorm()
    (18): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (19): BatchNorm()
    (20): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (21): BatchNorm()
    (22): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (23): BatchNorm()
    (24): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (25): BatchNorm()
    (26): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (27): BatchNorm()
    (28): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (29): BatchNorm()
    (30): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (31): BatchNorm()
    (32): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (33): BatchNorm()
    (34): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (35): BatchNorm()
    (36): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (37): BatchNorm()
    (38): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (39): BatchNorm()
    (40): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (41): BatchNorm()
    (42): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (43): BatchNorm()
    (44): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (45): BatchNorm()
    (46): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (47): BatchNorm()
    (48): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (49): BatchNorm()
    (50): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (51): BatchNorm()
    (52): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (53): BatchNorm()
    (54): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (55): BatchNorm()
    (56): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (57): BatchNorm()
    (58): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (59): BatchNorm()
    (60): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (61): BatchNorm()
    (62): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (63): BatchNorm()
  )
)
Evaluate logp(x) = -18857.855 +/- 79.836
Evaluate logp(x) = -25219.863 +/- 472.002
Evaluate logp(x) = -14047.594 +/- 39.657
Evaluate logp(x) = -74795.297 +/- 1254.588
Evaluate logp(x) = -13811.287 +/- 3.786
{'activation_fn': 'relu',
 'batch_size': 4,
 'channel': -1,
 'cond_label_size': None,
 'conditional': False,
 'data_dir': './data/',
 'dataset': '',
 'device': device(type='cuda', index=0),
 'flip_toy_var_order': False,
 'generate': False,
 'hidden_size': 64,
 'input_dims': torch.Size([3, 64, 64]),
 'input_order': 'random',
 'input_size': 12288,
 'log_interval': 1000,
 'lr': 0.0001,
 'model': 'realnvp',
 'n_blocks': 32,
 'n_components': 1,
 'n_epochs': 50,
 'n_hidden': 1,
 'no_batch_norm': False,
 'no_cuda': False,
 'output_dir': './results/mafencoderFalse_channel-1_nblocks32/',
 'restore_file': None,
 'results_file': 'results.txt',
 'seed': 1,
 'start_epoch': 0,
 'train': False,
 'use_encoder': False}
RealNVP(
  (net): FlowSequential(
    (0): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (1): BatchNorm()
    (2): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (3): BatchNorm()
    (4): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (5): BatchNorm()
    (6): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (7): BatchNorm()
    (8): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (9): BatchNorm()
    (10): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (11): BatchNorm()
    (12): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (13): BatchNorm()
    (14): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (15): BatchNorm()
    (16): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (17): BatchNorm()
    (18): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (19): BatchNorm()
    (20): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (21): BatchNorm()
    (22): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (23): BatchNorm()
    (24): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (25): BatchNorm()
    (26): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (27): BatchNorm()
    (28): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (29): BatchNorm()
    (30): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (31): BatchNorm()
    (32): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (33): BatchNorm()
    (34): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (35): BatchNorm()
    (36): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (37): BatchNorm()
    (38): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (39): BatchNorm()
    (40): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (41): BatchNorm()
    (42): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (43): BatchNorm()
    (44): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (45): BatchNorm()
    (46): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (47): BatchNorm()
    (48): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (49): BatchNorm()
    (50): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (51): BatchNorm()
    (52): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (53): BatchNorm()
    (54): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (55): BatchNorm()
    (56): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (57): BatchNorm()
    (58): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (59): BatchNorm()
    (60): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (61): BatchNorm()
    (62): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (63): BatchNorm()
  )
)
Evaluate logp(x) = -18857.855 +/- 79.836
Evaluate logp(x) = -25219.863 +/- 472.002
Evaluate logp(x) = -14047.594 +/- 39.657
Evaluate logp(x) = -77696.109 +/- 1257.078
Evaluate logp(x) = -13811.287 +/- 3.786
{'activation_fn': 'relu',
 'batch_size': 4,
 'channel': -1,
 'cond_label_size': None,
 'conditional': False,
 'data_dir': './data/',
 'dataset': '',
 'device': device(type='cuda', index=0),
 'flip_toy_var_order': False,
 'generate': False,
 'hidden_size': 64,
 'input_dims': torch.Size([3, 64, 64]),
 'input_order': 'random',
 'input_size': 12288,
 'log_interval': 1000,
 'lr': 0.0001,
 'model': 'realnvp',
 'n_blocks': 32,
 'n_components': 1,
 'n_epochs': 50,
 'n_hidden': 1,
 'no_batch_norm': False,
 'no_cuda': False,
 'output_dir': './results/mafencoderFalse_channel-1_nblocks32/',
 'restore_file': None,
 'results_file': 'results.txt',
 'seed': 1,
 'start_epoch': 0,
 'train': False,
 'use_encoder': False}
RealNVP(
  (net): FlowSequential(
    (0): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (1): BatchNorm()
    (2): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (3): BatchNorm()
    (4): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (5): BatchNorm()
    (6): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (7): BatchNorm()
    (8): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (9): BatchNorm()
    (10): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (11): BatchNorm()
    (12): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (13): BatchNorm()
    (14): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (15): BatchNorm()
    (16): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (17): BatchNorm()
    (18): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (19): BatchNorm()
    (20): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (21): BatchNorm()
    (22): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (23): BatchNorm()
    (24): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (25): BatchNorm()
    (26): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (27): BatchNorm()
    (28): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (29): BatchNorm()
    (30): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (31): BatchNorm()
    (32): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (33): BatchNorm()
    (34): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (35): BatchNorm()
    (36): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (37): BatchNorm()
    (38): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (39): BatchNorm()
    (40): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (41): BatchNorm()
    (42): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (43): BatchNorm()
    (44): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (45): BatchNorm()
    (46): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (47): BatchNorm()
    (48): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (49): BatchNorm()
    (50): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (51): BatchNorm()
    (52): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (53): BatchNorm()
    (54): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (55): BatchNorm()
    (56): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (57): BatchNorm()
    (58): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (59): BatchNorm()
    (60): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (61): BatchNorm()
    (62): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (63): BatchNorm()
  )
)
Evaluate logp(x) = -18857.855 +/- 79.836
Evaluate logp(x) = -25219.863 +/- 472.002
Evaluate logp(x) = -14047.594 +/- 39.657
Evaluate logp(x) = -77205.070 +/- 2099.006
Evaluate logp(x) = -13811.287 +/- 3.786
{'activation_fn': 'relu',
 'batch_size': 4,
 'channel': -1,
 'cond_label_size': None,
 'conditional': False,
 'data_dir': './data/',
 'dataset': '',
 'device': device(type='cuda', index=0),
 'flip_toy_var_order': False,
 'generate': False,
 'hidden_size': 64,
 'input_dims': torch.Size([3, 64, 64]),
 'input_order': 'random',
 'input_size': 12288,
 'log_interval': 1000,
 'lr': 0.0001,
 'model': 'realnvp',
 'n_blocks': 32,
 'n_components': 1,
 'n_epochs': 50,
 'n_hidden': 1,
 'no_batch_norm': False,
 'no_cuda': False,
 'output_dir': './results/mafencoderFalse_channel-1_nblocks32/',
 'restore_file': None,
 'results_file': 'results.txt',
 'seed': 1,
 'start_epoch': 0,
 'train': False,
 'use_encoder': False}
RealNVP(
  (net): FlowSequential(
    (0): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (1): BatchNorm()
    (2): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (3): BatchNorm()
    (4): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (5): BatchNorm()
    (6): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (7): BatchNorm()
    (8): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (9): BatchNorm()
    (10): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (11): BatchNorm()
    (12): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (13): BatchNorm()
    (14): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (15): BatchNorm()
    (16): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (17): BatchNorm()
    (18): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (19): BatchNorm()
    (20): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (21): BatchNorm()
    (22): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (23): BatchNorm()
    (24): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (25): BatchNorm()
    (26): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (27): BatchNorm()
    (28): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (29): BatchNorm()
    (30): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (31): BatchNorm()
    (32): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (33): BatchNorm()
    (34): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (35): BatchNorm()
    (36): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (37): BatchNorm()
    (38): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (39): BatchNorm()
    (40): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (41): BatchNorm()
    (42): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (43): BatchNorm()
    (44): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (45): BatchNorm()
    (46): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (47): BatchNorm()
    (48): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (49): BatchNorm()
    (50): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (51): BatchNorm()
    (52): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (53): BatchNorm()
    (54): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (55): BatchNorm()
    (56): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (57): BatchNorm()
    (58): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (59): BatchNorm()
    (60): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (61): BatchNorm()
    (62): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (63): BatchNorm()
  )
)
Evaluate logp(x) = -18857.855 +/- 79.836
Evaluate logp(x) = -25219.863 +/- 472.002
Evaluate logp(x) = -14047.594 +/- 39.657
Evaluate logp(x) = -79116.453 +/- 1776.399
Evaluate logp(x) = -13811.287 +/- 3.786
{'activation_fn': 'relu',
 'batch_size': 4,
 'channel': -1,
 'cond_label_size': None,
 'conditional': False,
 'data_dir': './data/',
 'dataset': '',
 'device': device(type='cuda', index=0),
 'flip_toy_var_order': False,
 'generate': False,
 'hidden_size': 64,
 'input_dims': torch.Size([3, 64, 64]),
 'input_order': 'random',
 'input_size': 12288,
 'log_interval': 1000,
 'lr': 0.0001,
 'model': 'realnvp',
 'n_blocks': 32,
 'n_components': 1,
 'n_epochs': 50,
 'n_hidden': 1,
 'no_batch_norm': False,
 'no_cuda': False,
 'output_dir': './results/mafencoderFalse_channel-1_nblocks32/',
 'restore_file': None,
 'results_file': 'results.txt',
 'seed': 1,
 'start_epoch': 0,
 'train': False,
 'use_encoder': False}
RealNVP(
  (net): FlowSequential(
    (0): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (1): BatchNorm()
    (2): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (3): BatchNorm()
    (4): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (5): BatchNorm()
    (6): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (7): BatchNorm()
    (8): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (9): BatchNorm()
    (10): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (11): BatchNorm()
    (12): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (13): BatchNorm()
    (14): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (15): BatchNorm()
    (16): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (17): BatchNorm()
    (18): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (19): BatchNorm()
    (20): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (21): BatchNorm()
    (22): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (23): BatchNorm()
    (24): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (25): BatchNorm()
    (26): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (27): BatchNorm()
    (28): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (29): BatchNorm()
    (30): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (31): BatchNorm()
    (32): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (33): BatchNorm()
    (34): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (35): BatchNorm()
    (36): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (37): BatchNorm()
    (38): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (39): BatchNorm()
    (40): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (41): BatchNorm()
    (42): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (43): BatchNorm()
    (44): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (45): BatchNorm()
    (46): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (47): BatchNorm()
    (48): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (49): BatchNorm()
    (50): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (51): BatchNorm()
    (52): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (53): BatchNorm()
    (54): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (55): BatchNorm()
    (56): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (57): BatchNorm()
    (58): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (59): BatchNorm()
    (60): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (61): BatchNorm()
    (62): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (63): BatchNorm()
  )
)
Evaluate logp(x) = -18857.855 +/- 79.836
Evaluate logp(x) = -25219.863 +/- 472.002
Evaluate logp(x) = -14047.594 +/- 39.657
Evaluate logp(x) = -73704.062 +/- 1325.354
Evaluate logp(x) = -13811.287 +/- 3.786
{'activation_fn': 'relu',
 'batch_size': 4,
 'channel': -1,
 'cond_label_size': None,
 'conditional': False,
 'data_dir': './data/',
 'dataset': '',
 'device': device(type='cuda', index=0),
 'flip_toy_var_order': False,
 'generate': False,
 'hidden_size': 64,
 'input_dims': torch.Size([3, 64, 64]),
 'input_order': 'random',
 'input_size': 12288,
 'log_interval': 1000,
 'lr': 0.0001,
 'model': 'realnvp',
 'n_blocks': 32,
 'n_components': 1,
 'n_epochs': 50,
 'n_hidden': 1,
 'no_batch_norm': False,
 'no_cuda': False,
 'output_dir': './results/mafencoderFalse_channel-1_nblocks32/',
 'restore_file': None,
 'results_file': 'results.txt',
 'seed': 1,
 'start_epoch': 0,
 'train': False,
 'use_encoder': False}
RealNVP(
  (net): FlowSequential(
    (0): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (1): BatchNorm()
    (2): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (3): BatchNorm()
    (4): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (5): BatchNorm()
    (6): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (7): BatchNorm()
    (8): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (9): BatchNorm()
    (10): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (11): BatchNorm()
    (12): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (13): BatchNorm()
    (14): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (15): BatchNorm()
    (16): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (17): BatchNorm()
    (18): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (19): BatchNorm()
    (20): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (21): BatchNorm()
    (22): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (23): BatchNorm()
    (24): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (25): BatchNorm()
    (26): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (27): BatchNorm()
    (28): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (29): BatchNorm()
    (30): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (31): BatchNorm()
    (32): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (33): BatchNorm()
    (34): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (35): BatchNorm()
    (36): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (37): BatchNorm()
    (38): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (39): BatchNorm()
    (40): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (41): BatchNorm()
    (42): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (43): BatchNorm()
    (44): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (45): BatchNorm()
    (46): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (47): BatchNorm()
    (48): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (49): BatchNorm()
    (50): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (51): BatchNorm()
    (52): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (53): BatchNorm()
    (54): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (55): BatchNorm()
    (56): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (57): BatchNorm()
    (58): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (59): BatchNorm()
    (60): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (61): BatchNorm()
    (62): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (63): BatchNorm()
  )
)
Evaluate logp(x) = -18857.855 +/- 79.836
Evaluate logp(x) = -25219.863 +/- 472.002
Evaluate logp(x) = -14047.594 +/- 39.657
Evaluate logp(x) = -76606.859 +/- 1711.805
Evaluate logp(x) = -13811.287 +/- 3.786
{'activation_fn': 'relu',
 'batch_size': 4,
 'channel': -1,
 'cond_label_size': None,
 'conditional': False,
 'data_dir': './data/',
 'dataset': '',
 'device': device(type='cuda', index=0),
 'flip_toy_var_order': False,
 'generate': False,
 'hidden_size': 64,
 'input_dims': torch.Size([3, 64, 64]),
 'input_order': 'random',
 'input_size': 12288,
 'log_interval': 1000,
 'lr': 0.0001,
 'model': 'realnvp',
 'n_blocks': 32,
 'n_components': 1,
 'n_epochs': 50,
 'n_hidden': 1,
 'no_batch_norm': False,
 'no_cuda': False,
 'output_dir': './results/mafencoderFalse_channel-1_nblocks32/',
 'restore_file': None,
 'results_file': 'results.txt',
 'seed': 1,
 'start_epoch': 0,
 'train': False,
 'use_encoder': False}
RealNVP(
  (net): FlowSequential(
    (0): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (1): BatchNorm()
    (2): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (3): BatchNorm()
    (4): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (5): BatchNorm()
    (6): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (7): BatchNorm()
    (8): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (9): BatchNorm()
    (10): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (11): BatchNorm()
    (12): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (13): BatchNorm()
    (14): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (15): BatchNorm()
    (16): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (17): BatchNorm()
    (18): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (19): BatchNorm()
    (20): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (21): BatchNorm()
    (22): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (23): BatchNorm()
    (24): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (25): BatchNorm()
    (26): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (27): BatchNorm()
    (28): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (29): BatchNorm()
    (30): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (31): BatchNorm()
    (32): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (33): BatchNorm()
    (34): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (35): BatchNorm()
    (36): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (37): BatchNorm()
    (38): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (39): BatchNorm()
    (40): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (41): BatchNorm()
    (42): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (43): BatchNorm()
    (44): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (45): BatchNorm()
    (46): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (47): BatchNorm()
    (48): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (49): BatchNorm()
    (50): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (51): BatchNorm()
    (52): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (53): BatchNorm()
    (54): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (55): BatchNorm()
    (56): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (57): BatchNorm()
    (58): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (59): BatchNorm()
    (60): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (61): BatchNorm()
    (62): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (63): BatchNorm()
  )
)
Evaluate logp(x) = -18857.855 +/- 79.836
Evaluate logp(x) = -25219.863 +/- 472.002
Evaluate logp(x) = -14047.594 +/- 39.657
Evaluate logp(x) = -74984.656 +/- 1556.892
Evaluate logp(x) = -13811.287 +/- 3.786
{'activation_fn': 'relu',
 'batch_size': 4,
 'channel': -1,
 'cond_label_size': None,
 'conditional': False,
 'data_dir': './data/',
 'dataset': '',
 'device': device(type='cuda', index=0),
 'flip_toy_var_order': False,
 'generate': False,
 'hidden_size': 64,
 'input_dims': torch.Size([3, 64, 64]),
 'input_order': 'sequential',
 'input_size': 12288,
 'log_interval': 1000,
 'lr': 0.0001,
 'model': 'realnvp',
 'n_blocks': 32,
 'n_components': 1,
 'n_epochs': 50,
 'n_hidden': 1,
 'no_batch_norm': False,
 'no_cuda': False,
 'output_dir': './results/mafencoderFalse_channel-1_nblocks32/',
 'restore_file': None,
 'results_file': 'results.txt',
 'seed': 1,
 'start_epoch': 0,
 'train': False,
 'use_encoder': False}
RealNVP(
  (net): FlowSequential(
    (0): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (1): BatchNorm()
    (2): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (3): BatchNorm()
    (4): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (5): BatchNorm()
    (6): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (7): BatchNorm()
    (8): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (9): BatchNorm()
    (10): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (11): BatchNorm()
    (12): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (13): BatchNorm()
    (14): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (15): BatchNorm()
    (16): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (17): BatchNorm()
    (18): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (19): BatchNorm()
    (20): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (21): BatchNorm()
    (22): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (23): BatchNorm()
    (24): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (25): BatchNorm()
    (26): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (27): BatchNorm()
    (28): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (29): BatchNorm()
    (30): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (31): BatchNorm()
    (32): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (33): BatchNorm()
    (34): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (35): BatchNorm()
    (36): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (37): BatchNorm()
    (38): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (39): BatchNorm()
    (40): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (41): BatchNorm()
    (42): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (43): BatchNorm()
    (44): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (45): BatchNorm()
    (46): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (47): BatchNorm()
    (48): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (49): BatchNorm()
    (50): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (51): BatchNorm()
    (52): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (53): BatchNorm()
    (54): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (55): BatchNorm()
    (56): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (57): BatchNorm()
    (58): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (59): BatchNorm()
    (60): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (61): BatchNorm()
    (62): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (63): BatchNorm()
  )
)
Evaluate logp(x) = -18857.855 +/- 79.836
Evaluate logp(x) = -25219.863 +/- 472.002
Evaluate logp(x) = -14047.594 +/- 39.657
Evaluate logp(x) = -73430.445 +/- 1266.532
Evaluate logp(x) = -13811.287 +/- 3.786
{'activation_fn': 'relu',
 'batch_size': 4,
 'channel': -1,
 'cond_label_size': None,
 'conditional': False,
 'data_dir': './data/',
 'dataset': '',
 'device': device(type='cuda', index=0),
 'flip_toy_var_order': False,
 'generate': False,
 'hidden_size': 64,
 'input_dims': torch.Size([3, 64, 64]),
 'input_order': 'sequential',
 'input_size': 12288,
 'log_interval': 1000,
 'lr': 0.0001,
 'model': 'realnvp',
 'n_blocks': 32,
 'n_components': 1,
 'n_epochs': 50,
 'n_hidden': 1,
 'no_batch_norm': False,
 'no_cuda': False,
 'output_dir': './results/mafencoderFalse_channel-1_nblocks32/',
 'restore_file': None,
 'results_file': 'results.txt',
 'seed': 1,
 'start_epoch': 0,
 'train': True,
 'use_encoder': False}
RealNVP(
  (net): FlowSequential(
    (0): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (1): BatchNorm()
    (2): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (3): BatchNorm()
    (4): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (5): BatchNorm()
    (6): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (7): BatchNorm()
    (8): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (9): BatchNorm()
    (10): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (11): BatchNorm()
    (12): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (13): BatchNorm()
    (14): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (15): BatchNorm()
    (16): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (17): BatchNorm()
    (18): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (19): BatchNorm()
    (20): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (21): BatchNorm()
    (22): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (23): BatchNorm()
    (24): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (25): BatchNorm()
    (26): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (27): BatchNorm()
    (28): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (29): BatchNorm()
    (30): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (31): BatchNorm()
    (32): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (33): BatchNorm()
    (34): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (35): BatchNorm()
    (36): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (37): BatchNorm()
    (38): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (39): BatchNorm()
    (40): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (41): BatchNorm()
    (42): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (43): BatchNorm()
    (44): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (45): BatchNorm()
    (46): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (47): BatchNorm()
    (48): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (49): BatchNorm()
    (50): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (51): BatchNorm()
    (52): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (53): BatchNorm()
    (54): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (55): BatchNorm()
    (56): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (57): BatchNorm()
    (58): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (59): BatchNorm()
    (60): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (61): BatchNorm()
    (62): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (63): BatchNorm()
  )
)
{'activation_fn': 'relu',
 'batch_size': 4,
 'channel': -1,
 'cond_label_size': None,
 'conditional': False,
 'data_dir': './data/',
 'dataset': 'toy',
 'device': device(type='cuda', index=0),
 'flip_toy_var_order': False,
 'generate': False,
 'hidden_size': 64,
 'input_dims': torch.Size([3, 64, 64]),
 'input_order': 'sequential',
 'input_size': 12288,
 'log_interval': 1000,
 'lr': 0.0001,
 'model': 'realnvp',
 'n_blocks': 64,
 'n_components': 1,
 'n_epochs': 50,
 'n_hidden': 1,
 'no_batch_norm': False,
 'no_cuda': False,
 'output_dir': './results/mafencoderFalse_channel-1_nblocks64/',
 'restore_file': None,
 'results_file': 'results.txt',
 'seed': 1,
 'start_epoch': 0,
 'train': True,
 'use_encoder': False}
RealNVP(
  (net): FlowSequential(
    (0): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (1): BatchNorm()
    (2): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (3): BatchNorm()
    (4): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (5): BatchNorm()
    (6): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (7): BatchNorm()
    (8): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (9): BatchNorm()
    (10): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (11): BatchNorm()
    (12): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (13): BatchNorm()
    (14): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (15): BatchNorm()
    (16): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (17): BatchNorm()
    (18): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (19): BatchNorm()
    (20): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (21): BatchNorm()
    (22): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (23): BatchNorm()
    (24): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (25): BatchNorm()
    (26): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (27): BatchNorm()
    (28): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (29): BatchNorm()
    (30): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (31): BatchNorm()
    (32): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (33): BatchNorm()
    (34): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (35): BatchNorm()
    (36): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (37): BatchNorm()
    (38): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (39): BatchNorm()
    (40): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (41): BatchNorm()
    (42): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (43): BatchNorm()
    (44): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (45): BatchNorm()
    (46): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (47): BatchNorm()
    (48): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (49): BatchNorm()
    (50): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (51): BatchNorm()
    (52): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (53): BatchNorm()
    (54): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (55): BatchNorm()
    (56): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (57): BatchNorm()
    (58): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (59): BatchNorm()
    (60): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (61): BatchNorm()
    (62): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (63): BatchNorm()
    (64): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (65): BatchNorm()
    (66): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (67): BatchNorm()
    (68): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (69): BatchNorm()
    (70): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (71): BatchNorm()
    (72): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (73): BatchNorm()
    (74): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (75): BatchNorm()
    (76): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (77): BatchNorm()
    (78): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (79): BatchNorm()
    (80): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (81): BatchNorm()
    (82): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (83): BatchNorm()
    (84): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (85): BatchNorm()
    (86): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (87): BatchNorm()
    (88): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (89): BatchNorm()
    (90): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (91): BatchNorm()
    (92): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (93): BatchNorm()
    (94): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (95): BatchNorm()
    (96): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (97): BatchNorm()
    (98): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (99): BatchNorm()
    (100): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (101): BatchNorm()
    (102): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (103): BatchNorm()
    (104): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (105): BatchNorm()
    (106): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (107): BatchNorm()
    (108): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (109): BatchNorm()
    (110): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (111): BatchNorm()
    (112): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (113): BatchNorm()
    (114): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (115): BatchNorm()
    (116): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (117): BatchNorm()
    (118): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (119): BatchNorm()
    (120): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (121): BatchNorm()
    (122): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (123): BatchNorm()
    (124): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (125): BatchNorm()
    (126): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (127): BatchNorm()
  )
)
Evaluate (epoch 0) -- logp(x) = -14233.260 +/- 78.796
Evaluate (epoch 0) -- logp(x) = -95852.734 +/- 4034.554
Evaluate (epoch 0) -- logp(x) = -126737.078 +/- 6495.576
Evaluate (epoch 0) -- logp(x) = -7271756.000 +/- 502271.344
Evaluate (epoch 0) -- logp(x) = -347987.406 +/- 18191.471
{'activation_fn': 'relu',
 'batch_size': 4,
 'channel': -1,
 'cond_label_size': None,
 'conditional': False,
 'data_dir': './data/',
 'dataset': '',
 'device': device(type='cuda', index=0),
 'flip_toy_var_order': False,
 'generate': False,
 'hidden_size': 64,
 'input_dims': torch.Size([3, 64, 64]),
 'input_order': 'sequential',
 'input_size': 12288,
 'log_interval': 1000,
 'lr': 0.0001,
 'model': 'realnvp',
 'n_blocks': 128,
 'n_components': 1,
 'n_epochs': 50,
 'n_hidden': 1,
 'no_batch_norm': False,
 'no_cuda': False,
 'output_dir': './results/mafencoderFalse_channel-1_nblocks128/',
 'restore_file': None,
 'results_file': 'results.txt',
 'seed': 1,
 'start_epoch': 0,
 'train': True,
 'use_encoder': False}
RealNVP(
  (net): FlowSequential(
    (0): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (1): BatchNorm()
    (2): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (3): BatchNorm()
    (4): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (5): BatchNorm()
    (6): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (7): BatchNorm()
    (8): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (9): BatchNorm()
    (10): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (11): BatchNorm()
    (12): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (13): BatchNorm()
    (14): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (15): BatchNorm()
    (16): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (17): BatchNorm()
    (18): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (19): BatchNorm()
    (20): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (21): BatchNorm()
    (22): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (23): BatchNorm()
    (24): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (25): BatchNorm()
    (26): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (27): BatchNorm()
    (28): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (29): BatchNorm()
    (30): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (31): BatchNorm()
    (32): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (33): BatchNorm()
    (34): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (35): BatchNorm()
    (36): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (37): BatchNorm()
    (38): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (39): BatchNorm()
    (40): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (41): BatchNorm()
    (42): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (43): BatchNorm()
    (44): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (45): BatchNorm()
    (46): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (47): BatchNorm()
    (48): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (49): BatchNorm()
    (50): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (51): BatchNorm()
    (52): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (53): BatchNorm()
    (54): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (55): BatchNorm()
    (56): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (57): BatchNorm()
    (58): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (59): BatchNorm()
    (60): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (61): BatchNorm()
    (62): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (63): BatchNorm()
    (64): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (65): BatchNorm()
    (66): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (67): BatchNorm()
    (68): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (69): BatchNorm()
    (70): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (71): BatchNorm()
    (72): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (73): BatchNorm()
    (74): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (75): BatchNorm()
    (76): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (77): BatchNorm()
    (78): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (79): BatchNorm()
    (80): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (81): BatchNorm()
    (82): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (83): BatchNorm()
    (84): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (85): BatchNorm()
    (86): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (87): BatchNorm()
    (88): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (89): BatchNorm()
    (90): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (91): BatchNorm()
    (92): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (93): BatchNorm()
    (94): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (95): BatchNorm()
    (96): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (97): BatchNorm()
    (98): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (99): BatchNorm()
    (100): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (101): BatchNorm()
    (102): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (103): BatchNorm()
    (104): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (105): BatchNorm()
    (106): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (107): BatchNorm()
    (108): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (109): BatchNorm()
    (110): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (111): BatchNorm()
    (112): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (113): BatchNorm()
    (114): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (115): BatchNorm()
    (116): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (117): BatchNorm()
    (118): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (119): BatchNorm()
    (120): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (121): BatchNorm()
    (122): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (123): BatchNorm()
    (124): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (125): BatchNorm()
    (126): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (127): BatchNorm()
    (128): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (129): BatchNorm()
    (130): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (131): BatchNorm()
    (132): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (133): BatchNorm()
    (134): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (135): BatchNorm()
    (136): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (137): BatchNorm()
    (138): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (139): BatchNorm()
    (140): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (141): BatchNorm()
    (142): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (143): BatchNorm()
    (144): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (145): BatchNorm()
    (146): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (147): BatchNorm()
    (148): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (149): BatchNorm()
    (150): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (151): BatchNorm()
    (152): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (153): BatchNorm()
    (154): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (155): BatchNorm()
    (156): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (157): BatchNorm()
    (158): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (159): BatchNorm()
    (160): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (161): BatchNorm()
    (162): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (163): BatchNorm()
    (164): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (165): BatchNorm()
    (166): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (167): BatchNorm()
    (168): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (169): BatchNorm()
    (170): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (171): BatchNorm()
    (172): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (173): BatchNorm()
    (174): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (175): BatchNorm()
    (176): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (177): BatchNorm()
    (178): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (179): BatchNorm()
    (180): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (181): BatchNorm()
    (182): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (183): BatchNorm()
    (184): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (185): BatchNorm()
    (186): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (187): BatchNorm()
    (188): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (189): BatchNorm()
    (190): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (191): BatchNorm()
    (192): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (193): BatchNorm()
    (194): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (195): BatchNorm()
    (196): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (197): BatchNorm()
    (198): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (199): BatchNorm()
    (200): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (201): BatchNorm()
    (202): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (203): BatchNorm()
    (204): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (205): BatchNorm()
    (206): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (207): BatchNorm()
    (208): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (209): BatchNorm()
    (210): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (211): BatchNorm()
    (212): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (213): BatchNorm()
    (214): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (215): BatchNorm()
    (216): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (217): BatchNorm()
    (218): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (219): BatchNorm()
    (220): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (221): BatchNorm()
    (222): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (223): BatchNorm()
    (224): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (225): BatchNorm()
    (226): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (227): BatchNorm()
    (228): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (229): BatchNorm()
    (230): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (231): BatchNorm()
    (232): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (233): BatchNorm()
    (234): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (235): BatchNorm()
    (236): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (237): BatchNorm()
    (238): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (239): BatchNorm()
    (240): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (241): BatchNorm()
    (242): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (243): BatchNorm()
    (244): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (245): BatchNorm()
    (246): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (247): BatchNorm()
    (248): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (249): BatchNorm()
    (250): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (251): BatchNorm()
    (252): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (253): BatchNorm()
    (254): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (255): BatchNorm()
  )
)
{'activation_fn': 'relu',
 'batch_size': 4,
 'channel': -1,
 'cond_label_size': None,
 'conditional': False,
 'data_dir': './data/',
 'dataset': '',
 'device': device(type='cuda', index=0),
 'flip_toy_var_order': False,
 'generate': False,
 'hidden_size': 64,
 'input_dims': torch.Size([3, 64, 64]),
 'input_order': 'sequential',
 'input_size': 12288,
 'log_interval': 1000,
 'lr': 0.0001,
 'model': 'realnvp',
 'n_blocks': 32,
 'n_components': 1,
 'n_epochs': 50,
 'n_hidden': 1,
 'no_batch_norm': False,
 'no_cuda': False,
 'output_dir': './results/mafencoderFalse_channel-1_nblocks32/',
 'restore_file': None,
 'results_file': 'results.txt',
 'seed': 1,
 'start_epoch': 0,
 'train': True,
 'use_encoder': False}
RealNVP(
  (net): FlowSequential(
    (0): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (1): BatchNorm()
    (2): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (3): BatchNorm()
    (4): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (5): BatchNorm()
    (6): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (7): BatchNorm()
    (8): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (9): BatchNorm()
    (10): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (11): BatchNorm()
    (12): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (13): BatchNorm()
    (14): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (15): BatchNorm()
    (16): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (17): BatchNorm()
    (18): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (19): BatchNorm()
    (20): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (21): BatchNorm()
    (22): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (23): BatchNorm()
    (24): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (25): BatchNorm()
    (26): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (27): BatchNorm()
    (28): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (29): BatchNorm()
    (30): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (31): BatchNorm()
    (32): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (33): BatchNorm()
    (34): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (35): BatchNorm()
    (36): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (37): BatchNorm()
    (38): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (39): BatchNorm()
    (40): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (41): BatchNorm()
    (42): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (43): BatchNorm()
    (44): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (45): BatchNorm()
    (46): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (47): BatchNorm()
    (48): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (49): BatchNorm()
    (50): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (51): BatchNorm()
    (52): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (53): BatchNorm()
    (54): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (55): BatchNorm()
    (56): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (57): BatchNorm()
    (58): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (59): BatchNorm()
    (60): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (61): BatchNorm()
    (62): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (63): BatchNorm()
  )
)
Evaluate (epoch 0) -- logp(x) = -19446.477 +/- 72.984
Evaluate (epoch 0) -- logp(x) = -66651.953 +/- 822.298
Evaluate (epoch 0) -- logp(x) = -19471.102 +/- 256.410
Evaluate (epoch 0) -- logp(x) = -389688.688 +/- 12196.767
Evaluate (epoch 0) -- logp(x) = -21075.203 +/- 29.892
Evaluate (epoch 0) -- logp(x) = -22930.232 +/- 261.390
Evaluate (epoch 0) -- logp(x) = -321594.438 +/- 8853.442
Evaluate (epoch 0) -- logp(x) = -21738.574 +/- 183.664
Evaluate (epoch 0) -- logp(x) = -4992805.000 +/- 135622.766
Evaluate (epoch 0) -- logp(x) = -26198.580 +/- 88.170
{'activation_fn': 'relu',
 'batch_size': 4,
 'channel': -1,
 'cond_label_size': None,
 'conditional': False,
 'data_dir': './data/',
 'dataset': '',
 'device': device(type='cuda', index=0),
 'flip_toy_var_order': False,
 'generate': False,
 'hidden_size': 64,
 'input_dims': torch.Size([3, 64, 64]),
 'input_order': 'sequential',
 'input_size': 12288,
 'log_interval': 1000,
 'lr': 0.0001,
 'model': 'realnvp',
 'n_blocks': 32,
 'n_components': 1,
 'n_epochs': 50,
 'n_hidden': 1,
 'no_batch_norm': False,
 'no_cuda': False,
 'output_dir': './results/mafencoderFalse_channel-1_nblocks32/',
 'restore_file': None,
 'results_file': 'results.txt',
 'seed': 1,
 'start_epoch': 0,
 'train': True,
 'use_encoder': False}
RealNVP(
  (net): FlowSequential(
    (0): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (1): BatchNorm()
    (2): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (3): BatchNorm()
    (4): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (5): BatchNorm()
    (6): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (7): BatchNorm()
    (8): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (9): BatchNorm()
    (10): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (11): BatchNorm()
    (12): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (13): BatchNorm()
    (14): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (15): BatchNorm()
    (16): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (17): BatchNorm()
    (18): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (19): BatchNorm()
    (20): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (21): BatchNorm()
    (22): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (23): BatchNorm()
    (24): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (25): BatchNorm()
    (26): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (27): BatchNorm()
    (28): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (29): BatchNorm()
    (30): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (31): BatchNorm()
    (32): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (33): BatchNorm()
    (34): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (35): BatchNorm()
    (36): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (37): BatchNorm()
    (38): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (39): BatchNorm()
    (40): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (41): BatchNorm()
    (42): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (43): BatchNorm()
    (44): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (45): BatchNorm()
    (46): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (47): BatchNorm()
    (48): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (49): BatchNorm()
    (50): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (51): BatchNorm()
    (52): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (53): BatchNorm()
    (54): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (55): BatchNorm()
    (56): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (57): BatchNorm()
    (58): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (59): BatchNorm()
    (60): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (61): BatchNorm()
    (62): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (63): BatchNorm()
  )
)
Evaluate (epoch 1) -- logp(x) = -20524.352 +/- 45.743
Evaluate (epoch 1) -- logp(x) = -135652288.000 +/- 3379906.250
Evaluate (epoch 1) -- logp(x) = -1697101.250 +/- 328654.969
Evaluate (epoch 1) -- logp(x) = -360368256.000 +/- 2732061.750
Evaluate (epoch 1) -- logp(x) = -135716.281 +/- 5711.109
Evaluate (epoch 0) -- logp(x) = -14233.260 +/- 78.796
Evaluate (epoch 0) -- logp(x) = -95852.734 +/- 4034.554
Evaluate (epoch 0) -- logp(x) = -126737.078 +/- 6495.576
Evaluate (epoch 0) -- logp(x) = -4364396.000 +/- 275732.750
Evaluate (epoch 0) -- logp(x) = -347987.406 +/- 18191.471
Evaluate (epoch 1) -- logp(x) = -21252.156 +/- 137.720
Evaluate (epoch 1) -- logp(x) = -1027895.812 +/- 39545.059
Evaluate (epoch 1) -- logp(x) = -22173.693 +/- 203.633
Evaluate (epoch 1) -- logp(x) = -11314223.000 +/- 361476.844
Evaluate (epoch 1) -- logp(x) = -23441.705 +/- 117.485
Evaluate (epoch 2) -- logp(x) = -48944.379 +/- 8.039
Evaluate (epoch 2) -- logp(x) = -4075944.250 +/- 809240.125
Evaluate (epoch 2) -- logp(x) = -49760.090 +/- 11.323
Evaluate (epoch 2) -- logp(x) = -5748597248.000 +/- 777700416.000
Evaluate (epoch 2) -- logp(x) = -49502.633 +/- 0.453
Evaluate (epoch 1) -- logp(x) = -19088.727 +/- 75.089
Evaluate (epoch 1) -- logp(x) = -4693816.500 +/- 179077.203
Evaluate (epoch 1) -- logp(x) = -292434.656 +/- 22280.365
Evaluate (epoch 1) -- logp(x) = -13716103.000 +/- 539027.250
Evaluate (epoch 1) -- logp(x) = -441433.875 +/- 22467.150
Evaluate (epoch 2) -- logp(x) = -28832.061 +/- 41.053
Evaluate (epoch 2) -- logp(x) = -631298752.000 +/- 17419924.000
Evaluate (epoch 3) -- logp(x) = -68223.344 +/- 8.907
Evaluate (epoch 2) -- logp(x) = -9173300.000 +/- 1809547.625
Evaluate (epoch 3) -- logp(x) = -13772877987840.000 +/- 950997483520.000
Evaluate (epoch 3) -- logp(x) = -68935.250 +/- 14.031
Evaluate (epoch 2) -- logp(x) = -595006016.000 +/- 6270863.000
Evaluate (epoch 2) -- logp(x) = -29681.768 +/- 4.991
Evaluate (epoch 3) -- logp(x) = -54850716958720.000 +/- 2998940729344.000
Evaluate (epoch 3) -- logp(x) = -68721.047 +/- 3.880
Evaluate (epoch 2) -- logp(x) = -58235.055 +/- 13.749
Evaluate (epoch 2) -- logp(x) = -26914932736.000 +/- 1910452224.000
Evaluate (epoch 2) -- logp(x) = -58129.676 +/- 138.905
Evaluate (epoch 2) -- logp(x) = -97723916288.000 +/- 4530989568.000
Evaluate (epoch 2) -- logp(x) = -57725.469 +/- 19.065
Evaluate (epoch 3) -- logp(x) = -39935.898 +/- 42.204
Evaluate (epoch 3) -- logp(x) = -34986237952.000 +/- 551839104.000
Evaluate (epoch 3) -- logp(x) = -1813400448.000 +/- 158180336.000
Evaluate (epoch 3) -- logp(x) = -40253124608.000 +/- 331489632.000
Evaluate (epoch 3) -- logp(x) = -216293648.000 +/- 6792292.500
Evaluate (epoch 4) -- logp(x) = -72778.406 +/- 14.101
Evaluate (epoch 4) -- logp(x) = -138572925501440.000 +/- 8128032669696.000
Evaluate (epoch 4) -- logp(x) = -73672.047 +/- 13.716
Evaluate (epoch 4) -- logp(x) = -6774105097371648.000 +/- 654050169192448.000
Evaluate (epoch 4) -- logp(x) = -73492.297 +/- 2.363
Evaluate (epoch 4) -- logp(x) = -38086.547 +/- 34.144
Evaluate (epoch 4) -- logp(x) = -4750835712000.000 +/- 190485348352.000
Evaluate (epoch 4) -- logp(x) = -38336.242 +/- 15.088
Evaluate (epoch 4) -- logp(x) = -5858289254400.000 +/- 236848644096.000
Evaluate (epoch 4) -- logp(x) = -38070.504 +/- 2.416
Evaluate (epoch 3) -- logp(x) = -22484057325568.000 +/- 4468967473152.000
Evaluate (epoch 3) -- logp(x) = -104192.391 +/- 27.020
Evaluate (epoch 3) -- logp(x) = -104913.875 +/- 12.832
Evaluate (epoch 3) -- logp(x) = -102202.117 +/- 125.108
Evaluate (epoch 3) -- logp(x) = -104600.984 +/- 3.901
Evaluate (epoch 5) -- logp(x) = -61538.629 +/- 22.874
Evaluate (epoch 5) -- logp(x) = -232029536256.000 +/- 46580903936.000
Evaluate (epoch 5) -- logp(x) = -62089.703 +/- 23.644
Evaluate (epoch 5) -- logp(x) = -1044690933645312.000 +/- 28180326907904.000
Evaluate (epoch 5) -- logp(x) = -61814.414 +/- 2.869
Evaluate (epoch 5) -- logp(x) = -51178.898 +/- 18.826
Evaluate (epoch 5) -- logp(x) = -89074827264.000 +/- 1824197888.000
Evaluate (epoch 5) -- logp(x) = -544693056.000 +/- 107855200.000
Evaluate (epoch 5) -- logp(x) = -103072137216.000 +/- 2164016896.000
Evaluate (epoch 5) -- logp(x) = -50699.836 +/- 19.140
Evaluate (epoch 6) -- logp(x) = -92228.633 +/- 14.269
Evaluate (epoch 6) -- logp(x) = -4981259763712.000 +/- 886407495680.000
Evaluate (epoch 6) -- logp(x) = -92593.867 +/- 16.245
Evaluate (epoch 6) -- logp(x) = -38924034506752.000 +/- 4791019765760.000
Evaluate (epoch 6) -- logp(x) = -92242.203 +/- 2.582
Evaluate (epoch 4) -- logp(x) = -125377.156 +/- 35.091
Evaluate (epoch 4) -- logp(x) = -126469.641 +/- 30.649
Evaluate (epoch 4) -- logp(x) = -127186.914 +/- 36.910
Evaluate (epoch 4) -- logp(x) = -118267.438 +/- 944.300
Evaluate (epoch 4) -- logp(x) = -126573.328 +/- 4.678
Evaluate (epoch 6) -- logp(x) = -38830.312 +/- 39.034
Evaluate (epoch 6) -- logp(x) = -648934785024.000 +/- 20212023296.000
Evaluate (epoch 6) -- logp(x) = -1262976256.000 +/- 250099024.000
Evaluate (epoch 6) -- logp(x) = -659858391040.000 +/- 11071069184.000
Evaluate (epoch 6) -- logp(x) = -37770.984 +/- 41.184
Evaluate (epoch 7) -- logp(x) = -100113.953 +/- 20.431
Evaluate (epoch 7) -- logp(x) = -99992.984 +/- 64.628
Evaluate (epoch 7) -- logp(x) = -100162.609 +/- 12.220
Evaluate (epoch 7) -- logp(x) = -45645.016 +/- 28.231
Evaluate (epoch 7) -- logp(x) = -81659927986176.000 +/- 3119351595008.000
Evaluate (epoch 7) -- logp(x) = -93554.922 +/- 159.229
Evaluate (epoch 7) -- logp(x) = -836884889600.000 +/- 165727911936.000
Evaluate (epoch 7) -- logp(x) = -99879.078 +/- 1.347
Evaluate (epoch 7) -- logp(x) = -106749549871104.000 +/- 3327304663040.000
Evaluate (epoch 7) -- logp(x) = -45269.715 +/- 24.750
Evaluate (epoch 5) -- logp(x) = -147263.172 +/- 27.794
Evaluate (epoch 5) -- logp(x) = -75605282390016.000 +/- 10652991094784.000
Evaluate (epoch 5) -- logp(x) = -1789758464.000 +/- 354396000.000
Evaluate (epoch 5) -- logp(x) = -578601619554304.000 +/- 57745684299776.000
Evaluate (epoch 5) -- logp(x) = -136408.094 +/- 362.845
Evaluate (epoch 8) -- logp(x) = -64254.980 +/- 49.084
Evaluate (epoch 8) -- logp(x) = -386161115136.000 +/- 18632673280.000
Evaluate (epoch 8) -- logp(x) = -63440.055 +/- 247.518
Evaluate (epoch 8) -- logp(x) = -713427058688.000 +/- 28966877184.000
Evaluate (epoch 8) -- logp(x) = -64328.004 +/- 3.322
Evaluate (epoch 8) -- logp(x) = -64553.418 +/- 15.977
Evaluate (epoch 8) -- logp(x) = -64670.438 +/- 32.045
Evaluate (epoch 8) -- logp(x) = -64662.973 +/- 9.931
Evaluate (epoch 8) -- logp(x) = -63550.297 +/- 55.783
Evaluate (epoch 8) -- logp(x) = -64426.734 +/- 1.004
Evaluate (epoch 9) -- logp(x) = -48794.664 +/- 39.890
Evaluate (epoch 9) -- logp(x) = -1866220699648.000 +/- 71771004928.000
Evaluate (epoch 9) -- logp(x) = -813116800.000 +/- 161011488.000
Evaluate (epoch 9) -- logp(x) = -2350446542848.000 +/- 92289351680.000
Evaluate (epoch 9) -- logp(x) = -48367.410 +/- 0.824
Evaluate (epoch 9) -- logp(x) = -86880.266 +/- 37.329
Evaluate (epoch 9) -- logp(x) = -88028.281 +/- 43.485
Evaluate (epoch 9) -- logp(x) = -87176.820 +/- 23.012
Evaluate (epoch 9) -- logp(x) = -87600.234 +/- 76.583
Evaluate (epoch 9) -- logp(x) = -86678.438 +/- 6.418
Evaluate (epoch 6) -- logp(x) = -115351.531 +/- 100.597
Evaluate (epoch 6) -- logp(x) = -118229.797 +/- 20.010
Evaluate (epoch 6) -- logp(x) = -116930.797 +/- 96.502
Evaluate (epoch 6) -- logp(x) = -115816.133 +/- 38.411
Evaluate (epoch 6) -- logp(x) = -530821886771200.000 +/- 30456615534592.000
Evaluate (epoch 10) -- logp(x) = -42883.035 +/- 25.242
Evaluate (epoch 10) -- logp(x) = -2362530332672.000 +/- 90991484928.000
Evaluate (epoch 10) -- logp(x) = -43452.000 +/- 121.874
Evaluate (epoch 10) -- logp(x) = -2357245247488.000 +/- 27401250816.000
Evaluate (epoch 10) -- logp(x) = -42885.336 +/- 9.000
Evaluate (epoch 11) -- logp(x) = -44064.871 +/- 34.989
Evaluate (epoch 11) -- logp(x) = -12103479984128.000 +/- 459873386496.000
Evaluate (epoch 11) -- logp(x) = -14522398720.000 +/- 2875854848.000
Evaluate (epoch 11) -- logp(x) = -13114860896256.000 +/- 654039252992.000
Evaluate (epoch 11) -- logp(x) = -43622.285 +/- 13.170
Evaluate (epoch 10) -- logp(x) = -64930.816 +/- 21.634
Evaluate (epoch 10) -- logp(x) = -65933.836 +/- 23.701
Evaluate (epoch 10) -- logp(x) = -65196.047 +/- 17.197
Evaluate (epoch 10) -- logp(x) = -66305.172 +/- 17.331
Evaluate (epoch 10) -- logp(x) = -64930.578 +/- 3.012
Evaluate (epoch 7) -- logp(x) = -93145.633 +/- 41.147
Evaluate (epoch 7) -- logp(x) = -8920253173751349248.000 +/- 225870766728544256.000
Evaluate (epoch 7) -- logp(x) = -92760.656 +/- 196.381
Evaluate (epoch 7) -- logp(x) = -16154493227238424576.000 +/- inf
Evaluate (epoch 7) -- logp(x) = -86431.906 +/- 106.816
Evaluate (epoch 12) -- logp(x) = -128263380992.000 +/- 25493823488.000
Evaluate (epoch 12) -- logp(x) = -60069.289 +/- 21.744
Evaluate (epoch 12) -- logp(x) = -58813.164 +/- 88.700
Evaluate (epoch 12) -- logp(x) = -60577.629 +/- 8.232
Evaluate (epoch 12) -- logp(x) = -58027.977 +/- 9.351
Evaluate (epoch 11) -- logp(x) = -70340.641 +/- 17.360
Evaluate (epoch 11) -- logp(x) = -71106.344 +/- 22.562
Evaluate (epoch 11) -- logp(x) = -70579.773 +/- 12.468
Evaluate (epoch 11) -- logp(x) = -71242.938 +/- 25.534
Evaluate (epoch 11) -- logp(x) = -70374.125 +/- 2.125
Evaluate (epoch 13) -- logp(x) = -50488.945 +/- 23.780
Evaluate (epoch 13) -- logp(x) = -11873682456576.000 +/- 778756685824.000
Evaluate (epoch 13) -- logp(x) = -663537188864.000 +/- 131377250304.000
Evaluate (epoch 13) -- logp(x) = -13032418705408.000 +/- 1985944748032.000
Evaluate (epoch 13) -- logp(x) = -44857.734 +/- 84.091
Evaluate (epoch 12) -- logp(x) = -142666.547 +/- 25.482
Evaluate (epoch 12) -- logp(x) = -179185.969 +/- 7572.319
Evaluate (epoch 12) -- logp(x) = -142666.016 +/- 13.042
Evaluate (epoch 12) -- logp(x) = -11753242624.000 +/- 1530349312.000
Evaluate (epoch 12) -- logp(x) = -142440.172 +/- 7.665
Evaluate (epoch 8) -- logp(x) = -100173.359 +/- 1186.906
Evaluate (epoch 8) -- logp(x) = -60156518400.000 +/- 2974123776.000
Evaluate (epoch 8) -- logp(x) = -105503.805 +/- 594.417
Evaluate (epoch 8) -- logp(x) = -180822867968.000 +/- 3353264128.000
Evaluate (epoch 8) -- logp(x) = -108214.703 +/- 30.269
Evaluate (epoch 14) -- logp(x) = -45558.719 +/- 30.503
Evaluate (epoch 14) -- logp(x) = -15623438467072.000 +/- 1580172050432.000
Evaluate (epoch 14) -- logp(x) = -15855360000.000 +/- 3132137472.000
Evaluate (epoch 14) -- logp(x) = -9508288987136.000 +/- 1116397633536.000
Evaluate (epoch 14) -- logp(x) = -45729.680 +/- 6.334
Evaluate (epoch 13) -- logp(x) = -52491.164 +/- 53.638
Evaluate (epoch 13) -- logp(x) = -60049.258 +/- 778.303
Evaluate (epoch 13) -- logp(x) = -53048.789 +/- 53.476
Evaluate (epoch 13) -- logp(x) = -567418945536.000 +/- 29595846656.000
Evaluate (epoch 13) -- logp(x) = -52523.156 +/- 10.200
Evaluate (epoch 15) -- logp(x) = -49084.633 +/- 20.719
Evaluate (epoch 15) -- logp(x) = -58149.000 +/- 850.106
Evaluate (epoch 15) -- logp(x) = -4302258831360.000 +/- 851974225920.000
Evaluate (epoch 15) -- logp(x) = -542464435814400.000 +/- 58875650768896.000
Evaluate (epoch 15) -- logp(x) = -287758178320384.000 +/- 8539622342656.000
Evaluate (epoch 9) -- logp(x) = -131354.250 +/- 164.644
Evaluate (epoch 9) -- logp(x) = -121024.273 +/- 325.259
Evaluate (epoch 9) -- logp(x) = -13312277504.000 +/- 261603600.000
Evaluate (epoch 9) -- logp(x) = -125027.344 +/- 104.512
Evaluate (epoch 9) -- logp(x) = -3544707840.000 +/- 88241112.000
Evaluate (epoch 14) -- logp(x) = -51563.836 +/- 42.228
Evaluate (epoch 14) -- logp(x) = -10201756336128.000 +/- 2048046006272.000
Evaluate (epoch 14) -- logp(x) = -51831.055 +/- 25.201
Evaluate (epoch 16) -- logp(x) = -609097351168.000 +/- 39557824512.000
Evaluate (epoch 16) -- logp(x) = -143444.281 +/- 5169.705
Evaluate (epoch 14) -- logp(x) = -751753091874816.000 +/- 62318901723136.000
Evaluate (epoch 16) -- logp(x) = -6354930176.000 +/- 1258450560.000
Evaluate (epoch 14) -- logp(x) = -51483.637 +/- 4.785
Evaluate (epoch 16) -- logp(x) = -152325.031 +/- 6486.388
Evaluate (epoch 16) -- logp(x) = -58275.219 +/- 325.550
Evaluate (epoch 17) -- logp(x) = -52031.234 +/- 267.784
Evaluate (epoch 17) -- logp(x) = -23335972.000 +/- 1742631.250
Evaluate (epoch 17) -- logp(x) = -79681.359 +/- 3627.579
Evaluate (epoch 17) -- logp(x) = -13472558.000 +/- 1667664.750
Evaluate (epoch 17) -- logp(x) = -59808.309 +/- 5.935
Evaluate (epoch 10) -- logp(x) = -104034.125 +/- 1125.630
Evaluate (epoch 10) -- logp(x) = -98625.953 +/- 104.698
Evaluate (epoch 10) -- logp(x) = -114841.062 +/- 73.341
Evaluate (epoch 10) -- logp(x) = -2824604.250 +/- 541396.500
Evaluate (epoch 10) -- logp(x) = -110355.789 +/- 123.504
Evaluate (epoch 15) -- logp(x) = -99711.906 +/- 34.772
Evaluate (epoch 15) -- logp(x) = -98396.078 +/- 458.096
Evaluate (epoch 15) -- logp(x) = -99922.984 +/- 13.674
Evaluate (epoch 15) -- logp(x) = -67108456.000 +/- 8440215.000
Evaluate (epoch 15) -- logp(x) = -99675.562 +/- 5.092
Evaluate (epoch 18) -- logp(x) = -62208.285 +/- 16.145
Evaluate (epoch 18) -- logp(x) = -789394496.000 +/- 146016944.000
Evaluate (epoch 18) -- logp(x) = -62447.547 +/- 37.381
Evaluate (epoch 18) -- logp(x) = -1774501632.000 +/- 179650032.000
Evaluate (epoch 18) -- logp(x) = -60606.062 +/- 7.239
Evaluate (epoch 16) -- logp(x) = -73180.461 +/- 393.978
Evaluate (epoch 16) -- logp(x) = -3624472.000 +/- 345275.250
Evaluate (epoch 16) -- logp(x) = -301804.750 +/- 27214.373
Evaluate (epoch 16) -- logp(x) = -74021.727 +/- 117.587
Evaluate (epoch 16) -- logp(x) = -69948.391 +/- 105.891
Evaluate (epoch 19) -- logp(x) = -57894.332 +/- 29.206
Evaluate (epoch 19) -- logp(x) = -10749530112.000 +/- 1971471360.000
Evaluate (epoch 19) -- logp(x) = -59185.234 +/- 203.923
Evaluate (epoch 19) -- logp(x) = -14922194944.000 +/- 2253240832.000
Evaluate (epoch 19) -- logp(x) = -56781.336 +/- 28.918
Evaluate (epoch 11) -- logp(x) = -100793.688 +/- 611.632
Evaluate (epoch 11) -- logp(x) = -379401568.000 +/- 47761724.000
Evaluate (epoch 11) -- logp(x) = -110799.531 +/- 17.546
Evaluate (epoch 11) -- logp(x) = -83825582080.000 +/- 7755590656.000
Evaluate (epoch 11) -- logp(x) = -109018.117 +/- 10.244
Evaluate (epoch 17) -- logp(x) = -80917.781 +/- 218.202
Evaluate (epoch 17) -- logp(x) = -85081.773 +/- 63.864
Evaluate (epoch 17) -- logp(x) = -82448.828 +/- 89.303
Evaluate (epoch 17) -- logp(x) = -87982.359 +/- 94.131
Evaluate (epoch 20) -- logp(x) = -55283.891 +/- 88.966
Evaluate (epoch 17) -- logp(x) = -81390.562 +/- 9.800
Evaluate (epoch 20) -- logp(x) = -751268.500 +/- 79124.141
Evaluate (epoch 20) -- logp(x) = -58095.625 +/- 359.109
Evaluate (epoch 20) -- logp(x) = -719894.125 +/- 71789.344
Evaluate (epoch 20) -- logp(x) = -56185.656 +/- 24.063
Evaluate (epoch 12) -- logp(x) = -167490.812 +/- 43.076
Evaluate (epoch 12) -- logp(x) = -169852.188 +/- 271.777
Evaluate (epoch 12) -- logp(x) = -624144.438 +/- 90785.211
Evaluate (epoch 12) -- logp(x) = -323843.031 +/- 26766.965
Evaluate (epoch 12) -- logp(x) = -612694.000 +/- 14926.239
Evaluate (epoch 21) -- logp(x) = -49614.457 +/- 34.862
Evaluate (epoch 21) -- logp(x) = -5621677.500 +/- 990621.500
Evaluate (epoch 21) -- logp(x) = -50764.965 +/- 255.997
Evaluate (epoch 21) -- logp(x) = -44415868.000 +/- 8658682.000
Evaluate (epoch 21) -- logp(x) = -48089.109 +/- 68.797
Evaluate (epoch 18) -- logp(x) = -68182.188 +/- 49.645
Evaluate (epoch 18) -- logp(x) = -73433.844 +/- 1132.371
Evaluate (epoch 18) -- logp(x) = -68066.000 +/- 23.380
Evaluate (epoch 18) -- logp(x) = -91847.266 +/- 1372.963
Evaluate (epoch 18) -- logp(x) = -68044.156 +/- 10.337
Evaluate (epoch 22) -- logp(x) = -47866.660 +/- 113.671
Evaluate (epoch 22) -- logp(x) = -175923.312 +/- 14015.790
Evaluate (epoch 22) -- logp(x) = -49147.059 +/- 149.766
Evaluate (epoch 22) -- logp(x) = -269463.969 +/- 16245.252
Evaluate (epoch 22) -- logp(x) = -46854.945 +/- 1.466
Evaluate (epoch 13) -- logp(x) = -118775.242 +/- 69.794
Evaluate (epoch 13) -- logp(x) = -1344945.500 +/- 147109.781
Evaluate (epoch 13) -- logp(x) = -121405.984 +/- 104.233
Evaluate (epoch 13) -- logp(x) = -5376341504.000 +/- 528334848.000
Evaluate (epoch 13) -- logp(x) = -121082.234 +/- 5.582
Evaluate (epoch 19) -- logp(x) = -65568.398 +/- 52.605
Evaluate (epoch 19) -- logp(x) = -2304597.500 +/- 169184.422
Evaluate (epoch 19) -- logp(x) = -65688.625 +/- 22.103
Evaluate (epoch 19) -- logp(x) = -10893355008.000 +/- 1489704192.000
Evaluate (epoch 19) -- logp(x) = -65291.441 +/- 10.363
Evaluate (epoch 23) -- logp(x) = -52442.645 +/- 73.399
Evaluate (epoch 23) -- logp(x) = -104946.797 +/- 5294.562
Evaluate (epoch 23) -- logp(x) = -53955.516 +/- 122.850
Evaluate (epoch 23) -- logp(x) = -143575.828 +/- 9212.242
Evaluate (epoch 23) -- logp(x) = -52372.480 +/- 21.086
Evaluate (epoch 20) -- logp(x) = -80075.922 +/- 36.296
Evaluate (epoch 20) -- logp(x) = -494222.406 +/- 82922.797
Evaluate (epoch 20) -- logp(x) = -80725.859 +/- 16.318
Evaluate (epoch 20) -- logp(x) = -266662800.000 +/- 10207734.000
Evaluate (epoch 20) -- logp(x) = -80194.359 +/- 5.232
Evaluate (epoch 24) -- logp(x) = -49768.938 +/- 30.434
Evaluate (epoch 24) -- logp(x) = -73997184.000 +/- 14359664.000
Evaluate (epoch 24) -- logp(x) = -51004.996 +/- 193.051
Evaluate (epoch 24) -- logp(x) = -679563136.000 +/- 126916792.000
Evaluate (epoch 24) -- logp(x) = -139569774592.000 +/- 7550397440.000
Evaluate (epoch 14) -- logp(x) = -91446.297 +/- 406.028
Evaluate (epoch 14) -- logp(x) = -2287699297828864.000 +/- 197969487855616.000
Evaluate (epoch 14) -- logp(x) = -1182404968448.000 +/- 234151084032.000
Evaluate (epoch 14) -- logp(x) = -2438039259316224.000 +/- 186116451860480.000
Evaluate (epoch 14) -- logp(x) = -96496.586 +/- 12.488
Evaluate (epoch 25) -- logp(x) = -54657.383 +/- 73.009
Evaluate (epoch 21) -- logp(x) = -120653.078 +/- 103.875
Evaluate (epoch 25) -- logp(x) = -3049761792.000 +/- 302628384.000
Evaluate (epoch 21) -- logp(x) = -947879424.000 +/- 12756137.000
Evaluate (epoch 25) -- logp(x) = -94958056.000 +/- 18793486.000
Evaluate (epoch 21) -- logp(x) = -1875473.375 +/- 347870.156
Evaluate (epoch 25) -- logp(x) = -9156088832.000 +/- 680494720.000
Evaluate (epoch 25) -- logp(x) = -6856203304960.000 +/- 73044893696.000
Evaluate (epoch 21) -- logp(x) = -1707261696.000 +/- 17688454.000
Evaluate (epoch 21) -- logp(x) = -118941.156 +/- 11.545
Evaluate (epoch 15) -- logp(x) = -279385376.000 +/- 55510804.000
Evaluate (epoch 15) -- logp(x) = -1551469559939072.000 +/- 159375633154048.000
Evaluate (epoch 15) -- logp(x) = -2900299087872.000 +/- 543737741312.000
Evaluate (epoch 15) -- logp(x) = -2819862925672448.000 +/- 360709405802496.000
Evaluate (epoch 15) -- logp(x) = -108714.742 +/- 2.670
Evaluate (epoch 26) -- logp(x) = -52566.617 +/- 96.554
Evaluate (epoch 26) -- logp(x) = -2018697088.000 +/- 254741696.000
Evaluate (epoch 26) -- logp(x) = -46582451929088.000 +/- 7469675315200.000
Evaluate (epoch 26) -- logp(x) = -6290081280.000 +/- 1067741248.000
Evaluate (epoch 26) -- logp(x) = -1989549752320.000 +/- 54129692672.000
Evaluate (epoch 22) -- logp(x) = -71867.477 +/- 33.359
Evaluate (epoch 22) -- logp(x) = -3138099085312.000 +/- 74061471744.000
Evaluate (epoch 22) -- logp(x) = -74601.727 +/- 357.041
Evaluate (epoch 22) -- logp(x) = -4154975059968.000 +/- 356601364480.000
Evaluate (epoch 22) -- logp(x) = -71686.703 +/- 6.381
Evaluate (epoch 27) -- logp(x) = -53669.090 +/- 122.030
Evaluate (epoch 27) -- logp(x) = -298337.375 +/- 34568.172
Evaluate (epoch 27) -- logp(x) = -54316.004 +/- 187.154
Evaluate (epoch 27) -- logp(x) = -430666.312 +/- 62409.277
Evaluate (epoch 27) -- logp(x) = -53543.043 +/- 43.473
Evaluate (epoch 23) -- logp(x) = -70675.695 +/- 36.518
Evaluate (epoch 23) -- logp(x) = -5296108339200.000 +/- 205162184704.000
Evaluate (epoch 23) -- logp(x) = -33194895360.000 +/- 6573554176.000
Evaluate (epoch 23) -- logp(x) = -8043646418944.000 +/- 157327474688.000
Evaluate (epoch 23) -- logp(x) = -71093.078 +/- 26.410
Evaluate (epoch 16) -- logp(x) = -150455.828 +/- 408.972
Evaluate (epoch 16) -- logp(x) = -120100093952.000 +/- 8165872640.000
Evaluate (epoch 16) -- logp(x) = -146518.406 +/- 1252.624
Evaluate (epoch 16) -- logp(x) = -1142467461120.000 +/- 112095985664.000
Evaluate (epoch 16) -- logp(x) = -153784.188 +/- 9.419
Evaluate (epoch 28) -- logp(x) = -54503.730 +/- 68.269
Evaluate (epoch 28) -- logp(x) = -210435407872.000 +/- 22599227392.000
Evaluate (epoch 28) -- logp(x) = -13969073.000 +/- 2755324.500
Evaluate (epoch 28) -- logp(x) = -191007424512.000 +/- 36540989440.000
Evaluate (epoch 28) -- logp(x) = -54570.695 +/- 16.592
Evaluate (epoch 24) -- logp(x) = -61331.320 +/- 23.930
Evaluate (epoch 24) -- logp(x) = -43958298214400.000 +/- 1712414916608.000
Evaluate (epoch 24) -- logp(x) = -516261.906 +/- 90047.242
Evaluate (epoch 24) -- logp(x) = -224567733780480.000 +/- 12799709282304.000
Evaluate (epoch 24) -- logp(x) = -61697.316 +/- 11.472
Evaluate (epoch 29) -- logp(x) = -77352.406 +/- 43.233
Evaluate (epoch 29) -- logp(x) = -106864885760.000 +/- 8491059200.000
Evaluate (epoch 29) -- logp(x) = -77758.375 +/- 92.375
Evaluate (epoch 29) -- logp(x) = -96495452160.000 +/- 10146377728.000
Evaluate (epoch 29) -- logp(x) = -78218.680 +/- 8.893
Evaluate (epoch 17) -- logp(x) = -105312.367 +/- 82.520
Evaluate (epoch 17) -- logp(x) = -147303923712.000 +/- 22753091584.000
Evaluate (epoch 17) -- logp(x) = -103553.375 +/- 472.314
Evaluate (epoch 17) -- logp(x) = -13438384128.000 +/- 906745856.000
Evaluate (epoch 17) -- logp(x) = -105573.109 +/- 59.499
Evaluate (epoch 25) -- logp(x) = -71932.984 +/- 11.290
Evaluate (epoch 25) -- logp(x) = -23924159021056.000 +/- 1000363065344.000
Evaluate (epoch 25) -- logp(x) = -40114827264.000 +/- 7943906816.000
Evaluate (epoch 30) -- logp(x) = -53772.098 +/- 55.093
Evaluate (epoch 25) -- logp(x) = -98766036140032.000 +/- 4337710137344.000
Evaluate (epoch 30) -- logp(x) = -10407103168512.000 +/- 492631097344.000
Evaluate (epoch 25) -- logp(x) = -71795.844 +/- 6.853
Evaluate (epoch 30) -- logp(x) = -414446223360.000 +/- 82072576000.000
Evaluate (epoch 30) -- logp(x) = -61854491607040.000 +/- 2764356190208.000
Evaluate (epoch 30) -- logp(x) = -55807.008 +/- 5.027
Evaluate (epoch 31) -- logp(x) = -100084.172 +/- 156.252
Evaluate (epoch 31) -- logp(x) = -5215449.000 +/- 306192.562
Evaluate (epoch 31) -- logp(x) = -473392.562 +/- 73066.000
Evaluate (epoch 31) -- logp(x) = -8365273.000 +/- 442940.062
Evaluate (epoch 31) -- logp(x) = -104841.703 +/- 16.583
Evaluate (epoch 26) -- logp(x) = -62963.363 +/- 55.086
Evaluate (epoch 26) -- logp(x) = -115632985079808.000 +/- 7760981786624.000
Evaluate (epoch 26) -- logp(x) = -65139.777 +/- 444.582
Evaluate (epoch 26) -- logp(x) = -233563106574336.000 +/- 9118197743616.000
Evaluate (epoch 26) -- logp(x) = -62472.121 +/- 10.041
Evaluate (epoch 18) -- logp(x) = -136606.125 +/- 199.509
Evaluate (epoch 18) -- logp(x) = -1978244085579776.000 +/- 331091781091328.000
Evaluate (epoch 18) -- logp(x) = -142477.938 +/- 110.924
Evaluate (epoch 18) -- logp(x) = -3617558611623936.000 +/- 300105034891264.000
Evaluate (epoch 18) -- logp(x) = -140143.312 +/- 32.959
Evaluate (epoch 32) -- logp(x) = -51587.836 +/- 48.896
Evaluate (epoch 32) -- logp(x) = -516019328.000 +/- 42322444.000
Evaluate (epoch 32) -- logp(x) = -131144.281 +/- 15824.662
Evaluate (epoch 32) -- logp(x) = -532082240.000 +/- 25994860.000
Evaluate (epoch 32) -- logp(x) = -52295.531 +/- 23.429
Evaluate (epoch 27) -- logp(x) = -57736.961 +/- 60.365
Evaluate (epoch 27) -- logp(x) = -1923614383276032.000 +/- 209078823419904.000
Evaluate (epoch 27) -- logp(x) = -12582060.000 +/- 2480173.250
Evaluate (epoch 27) -- logp(x) = -1862524110635008.000 +/- 22398445289472.000
Evaluate (epoch 27) -- logp(x) = -57435.727 +/- 14.973
Evaluate (epoch 33) -- logp(x) = -50828.312 +/- 18.810
Evaluate (epoch 33) -- logp(x) = -464854144.000 +/- 41582924.000
Evaluate (epoch 33) -- logp(x) = -12199863.000 +/- 2405513.750
Evaluate (epoch 33) -- logp(x) = -582855296.000 +/- 70435312.000
Evaluate (epoch 33) -- logp(x) = -50182.816 +/- 190.846
Evaluate (epoch 19) -- logp(x) = -114043.297 +/- 202.015
Evaluate (epoch 19) -- logp(x) = -306384209969152.000 +/- 28647710785536.000
Evaluate (epoch 19) -- logp(x) = -113359.875 +/- 452.680
Evaluate (epoch 19) -- logp(x) = -1217815159767040.000 +/- 84425341665280.000
Evaluate (epoch 19) -- logp(x) = -114778.922 +/- 13.856
Evaluate (epoch 28) -- logp(x) = -73544.961 +/- 31.895
Evaluate (epoch 28) -- logp(x) = -728244454162432.000 +/- 30568582479872.000
Evaluate (epoch 28) -- logp(x) = -75005.688 +/- 245.077
Evaluate (epoch 28) -- logp(x) = -3129462958850048.000 +/- 95172012539904.000
Evaluate (epoch 28) -- logp(x) = -73306.859 +/- 5.516
Evaluate (epoch 34) -- logp(x) = -55631.855 +/- 52.479
Evaluate (epoch 34) -- logp(x) = -405965438976.000 +/- 16876007424.000
Evaluate (epoch 34) -- logp(x) = -14946072576.000 +/- 1758120064.000
Evaluate (epoch 34) -- logp(x) = -580459298816.000 +/- 44177268736.000
Evaluate (epoch 34) -- logp(x) = -1987728128.000 +/- 142183632.000
Evaluate (epoch 35) -- logp(x) = -63654.680 +/- 34.816
Evaluate (epoch 35) -- logp(x) = -22825748480.000 +/- 294730464.000
Evaluate (epoch 35) -- logp(x) = -3791781632.000 +/- 750871424.000
Evaluate (epoch 35) -- logp(x) = -14167107584.000 +/- 1539297280.000
Evaluate (epoch 35) -- logp(x) = -60833.891 +/- 313.477
Evaluate (epoch 29) -- logp(x) = -52222.109 +/- 37.693
Evaluate (epoch 29) -- logp(x) = -12521323242717184.000 +/- 1327420443459584.000
Evaluate (epoch 29) -- logp(x) = -3533138944.000 +/- 699655424.000
Evaluate (epoch 29) -- logp(x) = -33728341668265984.000 +/- 1446508343853056.000
Evaluate (epoch 29) -- logp(x) = -52472.824 +/- 4.785
Evaluate (epoch 20) -- logp(x) = -108637.094 +/- 695.886
Evaluate (epoch 20) -- logp(x) = -1686352873652224.000 +/- 213400265162752.000
Evaluate (epoch 20) -- logp(x) = -839182450688.000 +/- 166182879232.000
Evaluate (epoch 20) -- logp(x) = -1127253626847232.000 +/- 50683164229632.000
Evaluate (epoch 20) -- logp(x) = -112913.625 +/- 1.427
Evaluate (epoch 36) -- logp(x) = -53838.477 +/- 59.912
Evaluate (epoch 36) -- logp(x) = -30783379456.000 +/- 1486949760.000
Evaluate (epoch 36) -- logp(x) = -2216352256.000 +/- 347028224.000
Evaluate (epoch 36) -- logp(x) = -29964412928.000 +/- 2937771520.000
Evaluate (epoch 36) -- logp(x) = -1114590848.000 +/- 36561332.000
Evaluate (epoch 30) -- logp(x) = -61486.863 +/- 21.691
Evaluate (epoch 30) -- logp(x) = -13786154604167168.000 +/- 2425085369516032.000
Evaluate (epoch 30) -- logp(x) = -62881.016 +/- 160.429
Evaluate (epoch 30) -- logp(x) = -33580163149070336.000 +/- 1448378667892736.000
Evaluate (epoch 30) -- logp(x) = -61921.988 +/- 17.905
Evaluate (epoch 37) -- logp(x) = -74949.633 +/- 75.084
Evaluate (epoch 37) -- logp(x) = -3244894208.000 +/- 130439512.000
Evaluate (epoch 37) -- logp(x) = -6589461504.000 +/- 792682176.000
Evaluate (epoch 37) -- logp(x) = -4839341568.000 +/- 245660544.000
Evaluate (epoch 21) -- logp(x) = -105646.273 +/- 189.389
Evaluate (epoch 37) -- logp(x) = -8492248.000 +/- 132944.812
Evaluate (epoch 21) -- logp(x) = -122612523008.000 +/- 12853921792.000
Evaluate (epoch 21) -- logp(x) = -55286896.000 +/- 10927297.000
Evaluate (epoch 21) -- logp(x) = -253433511936.000 +/- 44481257472.000
Evaluate (epoch 21) -- logp(x) = -106095.688 +/- 8.059
Evaluate (epoch 31) -- logp(x) = -55644.531 +/- 24.997
Evaluate (epoch 31) -- logp(x) = -88910324152926208.000 +/- 7342362019758080.000
Evaluate (epoch 31) -- logp(x) = -56926.680 +/- 132.564
Evaluate (epoch 31) -- logp(x) = -369822179111469056.000 +/- 18164078119747584.000
Evaluate (epoch 31) -- logp(x) = -56247.301 +/- 15.771
Evaluate (epoch 38) -- logp(x) = -39996.422 +/- 119.429
Evaluate (epoch 38) -- logp(x) = -340781888.000 +/- 13820339.000
Evaluate (epoch 38) -- logp(x) = -18876914.000 +/- 2786537.250
Evaluate (epoch 38) -- logp(x) = -331095616.000 +/- 22117714.000
Evaluate (epoch 38) -- logp(x) = -15128271.000 +/- 293351.312
Evaluate (epoch 22) -- logp(x) = -104536.133 +/- 132.681
Evaluate (epoch 22) -- logp(x) = -225995961401344.000 +/- 26448091938816.000
Evaluate (epoch 22) -- logp(x) = -7391112704.000 +/- 1463637760.000
Evaluate (epoch 22) -- logp(x) = -35398650691584.000 +/- 2011670249472.000
Evaluate (epoch 22) -- logp(x) = -104396.547 +/- 3.389
Evaluate (epoch 32) -- logp(x) = -70423.352 +/- 21.536
Evaluate (epoch 32) -- logp(x) = -166340200723644416.000 +/- 12291106553200640.000
Evaluate (epoch 32) -- logp(x) = -71265.648 +/- 101.301
Evaluate (epoch 32) -- logp(x) = -923244932997578752.000 +/- 50101777055350784.000
Evaluate (epoch 32) -- logp(x) = -71024.164 +/- 6.086
Evaluate (epoch 39) -- logp(x) = -44228.746 +/- 64.770
Evaluate (epoch 39) -- logp(x) = -216682987520.000 +/- 7642531328.000
Evaluate (epoch 39) -- logp(x) = -4386127872.000 +/- 825247040.000
Evaluate (epoch 39) -- logp(x) = -267774787584.000 +/- 8653494272.000
Evaluate (epoch 39) -- logp(x) = -205486944.000 +/- 7961779.000
Evaluate (epoch 40) -- logp(x) = -46869.051 +/- 51.989
Evaluate (epoch 40) -- logp(x) = -58519588864.000 +/- 1948071552.000
Evaluate (epoch 40) -- logp(x) = -836083072.000 +/- 158905888.000
Evaluate (epoch 40) -- logp(x) = -111974121472.000 +/- 4438305280.000
Evaluate (epoch 40) -- logp(x) = -50348.359 +/- 22.824
Evaluate (epoch 33) -- logp(x) = -50221.484 +/- 40.166
Evaluate (epoch 33) -- logp(x) = -33826574012776448.000 +/- 1277216168083456.000
Evaluate (epoch 33) -- logp(x) = -68992.828 +/- 3638.841
Evaluate (epoch 33) -- logp(x) = -63863079905001472.000 +/- 2395265411579904.000
Evaluate (epoch 33) -- logp(x) = -51087.141 +/- 9.425
Evaluate (epoch 23) -- logp(x) = -106188.750 +/- 119.510
Evaluate (epoch 23) -- logp(x) = -43049883271168.000 +/- 7852621561856.000
Evaluate (epoch 23) -- logp(x) = -4910385463296.000 +/- 972401278976.000
Evaluate (epoch 23) -- logp(x) = -124385415397376.000 +/- 19030222045184.000
Evaluate (epoch 23) -- logp(x) = -106174.812 +/- 5.608
Evaluate (epoch 41) -- logp(x) = -72652.000 +/- 37.380
Evaluate (epoch 41) -- logp(x) = -151367488.000 +/- 5830656.000
Evaluate (epoch 41) -- logp(x) = -90912.500 +/- 3398.725
Evaluate (epoch 41) -- logp(x) = -244859648.000 +/- 5251327.500
Evaluate (epoch 41) -- logp(x) = -73860.711 +/- 12.172
Evaluate (epoch 34) -- logp(x) = -75683.055 +/- 19.474
Evaluate (epoch 34) -- logp(x) = -3711478808969216.000 +/- 267450968965120.000
Evaluate (epoch 34) -- logp(x) = -75824.812 +/- 65.568
Evaluate (epoch 34) -- logp(x) = -4356810530095104.000 +/- 95925334704128.000
Evaluate (epoch 34) -- logp(x) = -76086.039 +/- 6.259
Evaluate (epoch 42) -- logp(x) = -49670.969 +/- 36.517
Evaluate (epoch 42) -- logp(x) = -44589408256.000 +/- 2111306368.000
Evaluate (epoch 42) -- logp(x) = -1602508800.000 +/- 230534704.000
Evaluate (epoch 42) -- logp(x) = -64619327488.000 +/- 4310965248.000
Evaluate (epoch 42) -- logp(x) = -702058432.000 +/- 107480144.000
Evaluate (epoch 24) -- logp(x) = -187162.969 +/- 72.170
Evaluate (epoch 24) -- logp(x) = -149485.469 +/- 1171.909
Evaluate (epoch 24) -- logp(x) = -187610.250 +/- 63.735
Evaluate (epoch 24) -- logp(x) = -258470.812 +/- 14010.500
Evaluate (epoch 24) -- logp(x) = -187551.969 +/- 6.889
Evaluate (epoch 35) -- logp(x) = -87967.805 +/- 19.166
Evaluate (epoch 35) -- logp(x) = -19885717907832832.000 +/- 660928961970176.000
Evaluate (epoch 35) -- logp(x) = -88630.891 +/- 22.643
Evaluate (epoch 35) -- logp(x) = -16043626995908608.000 +/- 995560131657728.000
Evaluate (epoch 35) -- logp(x) = -88661.523 +/- 8.263
Evaluate (epoch 43) -- logp(x) = -54213.125 +/- 29.799
Evaluate (epoch 43) -- logp(x) = -18671902720.000 +/- 808195072.000
Evaluate (epoch 43) -- logp(x) = -144088704.000 +/- 28522740.000
Evaluate (epoch 43) -- logp(x) = -28660129792.000 +/- 252396688.000
Evaluate (epoch 43) -- logp(x) = -56293.820 +/- 25.987
Evaluate (epoch 44) -- logp(x) = -47961.852 +/- 22.685
Evaluate (epoch 44) -- logp(x) = -135573200896.000 +/- 4970395648.000
Evaluate (epoch 44) -- logp(x) = -828767808.000 +/- 164101840.000
Evaluate (epoch 44) -- logp(x) = -225908015104.000 +/- 9107750912.000
Evaluate (epoch 44) -- logp(x) = -51736.555 +/- 5.469
Evaluate (epoch 36) -- logp(x) = -51066.211 +/- 62.720
Evaluate (epoch 36) -- logp(x) = -45017792512.000 +/- 4028755968.000
Evaluate (epoch 36) -- logp(x) = -53824.309 +/- 394.793
Evaluate (epoch 36) -- logp(x) = -59909844992.000 +/- 6816031232.000
Evaluate (epoch 36) -- logp(x) = -51449.699 +/- 7.461
Evaluate (epoch 25) -- logp(x) = -102792.039 +/- 77.186
Evaluate (epoch 25) -- logp(x) = -2338804989952.000 +/- 451082682368.000
Evaluate (epoch 25) -- logp(x) = -100852.203 +/- 384.447
Evaluate (epoch 25) -- logp(x) = -849600446464.000 +/- 137829302272.000
Evaluate (epoch 25) -- logp(x) = -102501.688 +/- 4.093
Evaluate (epoch 45) -- logp(x) = -50168.961 +/- 22.179
Evaluate (epoch 45) -- logp(x) = -1766628524032.000 +/- 85352652800.000
Evaluate (epoch 45) -- logp(x) = -16965986304.000 +/- 2018073088.000
Evaluate (epoch 45) -- logp(x) = -2983418920960.000 +/- 87855783936.000
Evaluate (epoch 45) -- logp(x) = -52004.164 +/- 38.942
Evaluate (epoch 37) -- logp(x) = -61873.344 +/- 20.637
Evaluate (epoch 37) -- logp(x) = -1349526749184.000 +/- 117886967808.000
Evaluate (epoch 37) -- logp(x) = -64030.453 +/- 75.363
Evaluate (epoch 37) -- logp(x) = -12493725696.000 +/- 2461034240.000
Evaluate (epoch 37) -- logp(x) = -63183.516 +/- 6.533
Evaluate (epoch 46) -- logp(x) = -49827.676 +/- 32.825
Evaluate (epoch 46) -- logp(x) = -138780737536.000 +/- 5109817856.000
Evaluate (epoch 46) -- logp(x) = -223069306880.000 +/- 43336216576.000
Evaluate (epoch 46) -- logp(x) = -270382170112.000 +/- 3351739648.000
Evaluate (epoch 46) -- logp(x) = -52999.332 +/- 501.904
Evaluate (epoch 26) -- logp(x) = -113712.219 +/- 92.119
Evaluate (epoch 26) -- logp(x) = -14704315465728.000 +/- 1090758508544.000
Evaluate (epoch 26) -- logp(x) = -4599984422912.000 +/- 910932705280.000
Evaluate (epoch 26) -- logp(x) = -6029881376768.000 +/- 833630109696.000
Evaluate (epoch 26) -- logp(x) = -113579.031 +/- 12.323
Evaluate (epoch 38) -- logp(x) = -1991450034176.000 +/- 158930894848.000
Evaluate (epoch 38) -- logp(x) = -84064.984 +/- 13.507
Evaluate (epoch 38) -- logp(x) = -79303.336 +/- 865.882
Evaluate (epoch 38) -- logp(x) = -84642.562 +/- 41.096
Evaluate (epoch 38) -- logp(x) = -396392202240.000 +/- 7514509824.000
Evaluate (epoch 47) -- logp(x) = -90323.906 +/- 26.292
Evaluate (epoch 47) -- logp(x) = -423109248.000 +/- 24915940.000
Evaluate (epoch 47) -- logp(x) = -113127.125 +/- 4310.743
Evaluate (epoch 47) -- logp(x) = -758134208.000 +/- 38926872.000
Evaluate (epoch 47) -- logp(x) = -91220.625 +/- 11.993
Evaluate (epoch 39) -- logp(x) = -16303582609408.000 +/- 1130582245376.000
Evaluate (epoch 39) -- logp(x) = -66157.938 +/- 27.870
Evaluate (epoch 39) -- logp(x) = -62795.547 +/- 148.014
Evaluate (epoch 39) -- logp(x) = -69038.375 +/- 42.938
Evaluate (epoch 39) -- logp(x) = -64611.996 +/- 33.497
Evaluate (epoch 48) -- logp(x) = -51020.262 +/- 15.773
Evaluate (epoch 48) -- logp(x) = -35879276544.000 +/- 2848555264.000
Evaluate (epoch 48) -- logp(x) = -348155776.000 +/- 66310392.000
Evaluate (epoch 48) -- logp(x) = -86377709568.000 +/- 6676609024.000
Evaluate (epoch 48) -- logp(x) = -53922.582 +/- 43.121
Evaluate (epoch 27) -- logp(x) = -97733.133 +/- 21.493
Evaluate (epoch 27) -- logp(x) = -999141530402816.000 +/- 104998981599232.000
Evaluate (epoch 27) -- logp(x) = -632375079862272.000 +/- 123840399147008.000
Evaluate (epoch 27) -- logp(x) = -7391603819806720.000 +/- 1387377113169920.000
Evaluate (epoch 27) -- logp(x) = -92114.844 +/- 84.287
Evaluate (epoch 49) -- logp(x) = -50673.410 +/- 23.494
Evaluate (epoch 49) -- logp(x) = -1193507553280.000 +/- 70682042368.000
Evaluate (epoch 49) -- logp(x) = -1203192448.000 +/- 238257200.000
Evaluate (epoch 49) -- logp(x) = -2285303234560.000 +/- 48032137216.000
Evaluate (epoch 49) -- logp(x) = -52126.098 +/- 17.824
Evaluate (epoch 40) -- logp(x) = -5307127824384.000 +/- 1054853300224.000
Evaluate (epoch 40) -- logp(x) = -72195.594 +/- 11.524
Evaluate (epoch 40) -- logp(x) = -71158.141 +/- 44.904
Evaluate (epoch 40) -- logp(x) = -73544.016 +/- 7.025
Evaluate (epoch 40) -- logp(x) = -70854.164 +/- 9.517
Evaluate (epoch 28) -- logp(x) = -104081.297 +/- 59.323
Evaluate (epoch 28) -- logp(x) = -24767225265127424.000 +/- 1565272779522048.000
Evaluate (epoch 28) -- logp(x) = -1723822168342528.000 +/- 341367658119168.000
Evaluate (epoch 28) -- logp(x) = -45234638511144960.000 +/- 4476238538211328.000
Evaluate (epoch 28) -- logp(x) = -104397.750 +/- 4.174
Evaluate (epoch 41) -- logp(x) = -260410343424.000 +/- 51752890368.000
Evaluate (epoch 41) -- logp(x) = -99693.984 +/- 15.814
Evaluate (epoch 41) -- logp(x) = -98492.227 +/- 26.522
Evaluate (epoch 41) -- logp(x) = -99675.578 +/- 56.705
Evaluate (epoch 41) -- logp(x) = -99181.641 +/- 17.316
{'activation_fn': 'relu',
 'batch_size': 4,
 'channel': -1,
 'cond_label_size': None,
 'conditional': False,
 'data_dir': './data/',
 'dataset': '',
 'device': device(type='cuda', index=0),
 'flip_toy_var_order': False,
 'generate': False,
 'hidden_size': 64,
 'input_dims': torch.Size([3, 64, 64]),
 'input_order': 'sequential',
 'input_size': 12288,
 'log_interval': 1000,
 'lr': 0.0001,
 'model': 'realnvp',
 'n_blocks': 32,
 'n_components': 1,
 'n_epochs': 50,
 'n_hidden': 1,
 'no_batch_norm': False,
 'no_cuda': False,
 'output_dir': './results/mafencoderFalse_channel-1_nblocks32/',
 'restore_file': None,
 'results_file': 'results.txt',
 'seed': 1,
 'start_epoch': 0,
 'train': False,
 'use_encoder': False}
RealNVP(
  (net): FlowSequential(
    (0): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (1): BatchNorm()
    (2): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (3): BatchNorm()
    (4): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (5): BatchNorm()
    (6): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (7): BatchNorm()
    (8): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (9): BatchNorm()
    (10): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (11): BatchNorm()
    (12): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (13): BatchNorm()
    (14): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (15): BatchNorm()
    (16): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (17): BatchNorm()
    (18): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (19): BatchNorm()
    (20): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (21): BatchNorm()
    (22): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (23): BatchNorm()
    (24): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (25): BatchNorm()
    (26): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (27): BatchNorm()
    (28): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (29): BatchNorm()
    (30): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (31): BatchNorm()
    (32): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (33): BatchNorm()
    (34): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (35): BatchNorm()
    (36): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (37): BatchNorm()
    (38): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (39): BatchNorm()
    (40): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (41): BatchNorm()
    (42): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (43): BatchNorm()
    (44): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (45): BatchNorm()
    (46): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (47): BatchNorm()
    (48): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (49): BatchNorm()
    (50): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (51): BatchNorm()
    (52): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (53): BatchNorm()
    (54): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (55): BatchNorm()
    (56): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (57): BatchNorm()
    (58): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (59): BatchNorm()
    (60): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (61): BatchNorm()
    (62): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (63): BatchNorm()
  )
)
Evaluate logp(x) = -18857.855 +/- 79.836
Evaluate logp(x) = -25219.863 +/- 472.002
Evaluate logp(x) = -14047.594 +/- 39.657
Evaluate logp(x) = -76226.938 +/- 2264.610
Evaluate logp(x) = -13811.287 +/- 3.786
Evaluate (epoch 42) -- logp(x) = -307422528.000 +/- 34969700.000
Evaluate (epoch 42) -- logp(x) = -60748.344 +/- 122.513
Evaluate (epoch 42) -- logp(x) = -53686.898 +/- 160.084
Evaluate (epoch 42) -- logp(x) = -131718.250 +/- 5968.695
Evaluate (epoch 42) -- logp(x) = -53560.973 +/- 42.324
Evaluate (epoch 29) -- logp(x) = -124884.672 +/- 20.773
Evaluate (epoch 29) -- logp(x) = -3481346768896.000 +/- 509857398784.000
Evaluate (epoch 29) -- logp(x) = -516267584.000 +/- 102212680.000
Evaluate (epoch 29) -- logp(x) = -32519727104.000 +/- 6359631872.000
Evaluate (epoch 29) -- logp(x) = -125063.406 +/- 11.808
Evaluate (epoch 43) -- logp(x) = -207646982144.000 +/- 36750532608.000
Evaluate (epoch 43) -- logp(x) = -68168.945 +/- 17.770
Evaluate (epoch 43) -- logp(x) = -64856.617 +/- 102.553
Evaluate (epoch 43) -- logp(x) = -71629.609 +/- 184.367
Evaluate (epoch 43) -- logp(x) = -64056.508 +/- 16.274
Evaluate (epoch 30) -- logp(x) = -100154.953 +/- 31.821
Evaluate (epoch 30) -- logp(x) = -4621395820544.000 +/- 731248984064.000
Evaluate (epoch 30) -- logp(x) = -99408.062 +/- 298.927
Evaluate (epoch 30) -- logp(x) = -727795826688.000 +/- 135461355520.000
Evaluate (epoch 30) -- logp(x) = -100612.094 +/- 0.917
Evaluate (epoch 44) -- logp(x) = -3835202304.000 +/- 762279360.000
Evaluate (epoch 44) -- logp(x) = -61973.109 +/- 56.437
Evaluate (epoch 44) -- logp(x) = -57297.566 +/- 85.351
Evaluate (epoch 44) -- logp(x) = -617980.812 +/- 105834.758
Evaluate (epoch 44) -- logp(x) = -56700.215 +/- 13.910
Evaluate (epoch 45) -- logp(x) = -13110436954112.000 +/- 2089657696256.000
Evaluate (epoch 45) -- logp(x) = -64280.516 +/- 60.257
Evaluate (epoch 45) -- logp(x) = -61186.562 +/- 47.833
Evaluate (epoch 45) -- logp(x) = -68744.109 +/- 185.790
Evaluate (epoch 45) -- logp(x) = -61422.254 +/- 16.794
Evaluate (epoch 31) -- logp(x) = -111844.562 +/- 39.400
Evaluate (epoch 31) -- logp(x) = -130906084876484608.000 +/- 18630831543156736.000
Evaluate (epoch 31) -- logp(x) = -110255.500 +/- 367.298
Evaluate (epoch 31) -- logp(x) = -5435182328840192.000 +/- 723750844628992.000
Evaluate (epoch 31) -- logp(x) = -111976.750 +/- 4.848
Evaluate (epoch 46) -- logp(x) = -1966344765440.000 +/- 273834835968.000
Evaluate (epoch 46) -- logp(x) = -65897.383 +/- 33.513
Evaluate (epoch 46) -- logp(x) = -62196.582 +/- 97.337
Evaluate (epoch 46) -- logp(x) = -68110.078 +/- 213.308
Evaluate (epoch 46) -- logp(x) = -63071.133 +/- 14.986
Evaluate (epoch 32) -- logp(x) = -104972.094 +/- 52.769
Evaluate (epoch 32) -- logp(x) = -3669686629695488.000 +/- 706310391726080.000
Evaluate (epoch 32) -- logp(x) = -103869.859 +/- 11.573
Evaluate (epoch 32) -- logp(x) = -5104548230201344.000 +/- 388917710815232.000
Evaluate (epoch 32) -- logp(x) = -105032.641 +/- 6.823
Evaluate (epoch 47) -- logp(x) = -577713702174720.000 +/- 99722371006464.000
Evaluate (epoch 47) -- logp(x) = -65139.816 +/- 35.660
Evaluate (epoch 47) -- logp(x) = -62063.078 +/- 105.623
Evaluate (epoch 47) -- logp(x) = -69187.016 +/- 379.075
Evaluate (epoch 47) -- logp(x) = -62278.836 +/- 14.852
{'activation_fn': 'relu',
 'batch_size': 4,
 'channel': -1,
 'cond_label_size': None,
 'conditional': False,
 'data_dir': './data/',
 'dataset': '',
 'device': device(type='cuda', index=0),
 'flip_toy_var_order': False,
 'generate': False,
 'hidden_size': 64,
 'input_dims': torch.Size([3, 64, 64]),
 'input_order': 'sequential',
 'input_size': 12288,
 'log_interval': 1000,
 'lr': 0.0001,
 'model': 'realnvp',
 'n_blocks': 32,
 'n_components': 1,
 'n_epochs': 50,
 'n_hidden': 1,
 'no_batch_norm': False,
 'no_cuda': False,
 'output_dir': './results/mafencoderFalse_channel-1_nblocks32/',
 'restore_file': None,
 'results_file': 'results.txt',
 'seed': 1,
 'start_epoch': 0,
 'train': True,
 'use_encoder': False}
RealNVP(
  (net): FlowSequential(
    (0): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (1): BatchNorm()
    (2): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (3): BatchNorm()
    (4): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (5): BatchNorm()
    (6): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (7): BatchNorm()
    (8): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (9): BatchNorm()
    (10): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (11): BatchNorm()
    (12): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (13): BatchNorm()
    (14): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (15): BatchNorm()
    (16): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (17): BatchNorm()
    (18): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (19): BatchNorm()
    (20): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (21): BatchNorm()
    (22): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (23): BatchNorm()
    (24): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (25): BatchNorm()
    (26): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (27): BatchNorm()
    (28): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (29): BatchNorm()
    (30): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (31): BatchNorm()
    (32): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (33): BatchNorm()
    (34): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (35): BatchNorm()
    (36): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (37): BatchNorm()
    (38): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (39): BatchNorm()
    (40): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (41): BatchNorm()
    (42): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (43): BatchNorm()
    (44): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (45): BatchNorm()
    (46): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (47): BatchNorm()
    (48): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (49): BatchNorm()
    (50): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (51): BatchNorm()
    (52): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (53): BatchNorm()
    (54): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (55): BatchNorm()
    (56): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (57): BatchNorm()
    (58): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (59): BatchNorm()
    (60): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (61): BatchNorm()
    (62): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (63): BatchNorm()
  )
)
Evaluate (epoch 0) -- logp(x) = -14233.260 +/- 78.796
Evaluate (epoch 0) -- logp(x) = -95852.734 +/- 4034.554
Evaluate (epoch 0) -- logp(x) = -126737.078 +/- 6495.576
Evaluate (epoch 0) -- logp(x) = -3088695.000 +/- 158325.438
Evaluate (epoch 0) -- logp(x) = -347987.406 +/- 18191.471
Evaluate (epoch 48) -- logp(x) = -249834192.000 +/- 40726568.000
Evaluate (epoch 48) -- logp(x) = -95699.219 +/- 25.237
Evaluate (epoch 48) -- logp(x) = -91742.672 +/- 31.226
Evaluate (epoch 48) -- logp(x) = -93520.930 +/- 87.227
{'activation_fn': 'relu',
 'batch_size': 4,
 'channel': -1,
 'cond_label_size': None,
 'conditional': False,
 'data_dir': './data/',
 'dataset': '',
 'device': device(type='cuda', index=0),
 'flip_toy_var_order': False,
 'generate': False,
 'hidden_size': 64,
 'input_dims': torch.Size([3, 64, 64]),
 'input_order': 'sequential',
 'input_size': 12288,
 'log_interval': 1000,
 'lr': 0.0001,
 'model': 'realnvp',
 'n_blocks': 32,
 'n_components': 1,
 'n_epochs': 50,
 'n_hidden': 1,
 'no_batch_norm': False,
 'no_cuda': False,
 'output_dir': './results/mafencoderFalse_channel-1_nblocks32/',
 'restore_file': None,
 'results_file': 'results.txt',
 'seed': 1,
 'start_epoch': 0,
 'train': True,
 'use_encoder': False}
RealNVP(
  (net): FlowSequential(
    (0): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (1): BatchNorm()
    (2): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (3): BatchNorm()
    (4): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (5): BatchNorm()
    (6): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (7): BatchNorm()
    (8): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (9): BatchNorm()
    (10): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (11): BatchNorm()
    (12): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (13): BatchNorm()
    (14): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (15): BatchNorm()
    (16): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (17): BatchNorm()
    (18): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (19): BatchNorm()
    (20): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (21): BatchNorm()
    (22): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (23): BatchNorm()
    (24): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (25): BatchNorm()
    (26): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (27): BatchNorm()
    (28): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (29): BatchNorm()
    (30): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (31): BatchNorm()
    (32): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (33): BatchNorm()
    (34): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (35): BatchNorm()
    (36): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (37): BatchNorm()
    (38): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (39): BatchNorm()
    (40): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (41): BatchNorm()
    (42): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (43): BatchNorm()
    (44): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (45): BatchNorm()
    (46): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (47): BatchNorm()
    (48): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (49): BatchNorm()
    (50): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (51): BatchNorm()
    (52): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (53): BatchNorm()
    (54): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (55): BatchNorm()
    (56): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (57): BatchNorm()
    (58): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (59): BatchNorm()
    (60): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (61): BatchNorm()
    (62): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (63): BatchNorm()
  )
)
{'activation_fn': 'relu',
 'batch_size': 4,
 'channel': -1,
 'cond_label_size': None,
 'conditional': False,
 'data_dir': './data/',
 'dataset': 'toy',
 'device': device(type='cuda', index=0),
 'flip_toy_var_order': False,
 'generate': False,
 'hidden_size': 64,
 'input_dims': torch.Size([3, 64, 64]),
 'input_order': 'sequential',
 'input_size': 12288,
 'log_interval': 1000,
 'lr': 0.0001,
 'model': 'realnvp',
 'n_blocks': 64,
 'n_components': 1,
 'n_epochs': 50,
 'n_hidden': 1,
 'no_batch_norm': False,
 'no_cuda': False,
 'output_dir': './results/mafencoderFalse_channel-1_nblocks64/',
 'restore_file': None,
 'results_file': 'results.txt',
 'seed': 1,
 'start_epoch': 0,
 'train': True,
 'use_encoder': False}
RealNVP(
  (net): FlowSequential(
    (0): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (1): BatchNorm()
    (2): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (3): BatchNorm()
    (4): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (5): BatchNorm()
    (6): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (7): BatchNorm()
    (8): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (9): BatchNorm()
    (10): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (11): BatchNorm()
    (12): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (13): BatchNorm()
    (14): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (15): BatchNorm()
    (16): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (17): BatchNorm()
    (18): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (19): BatchNorm()
    (20): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (21): BatchNorm()
    (22): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (23): BatchNorm()
    (24): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (25): BatchNorm()
    (26): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (27): BatchNorm()
    (28): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (29): BatchNorm()
    (30): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (31): BatchNorm()
    (32): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (33): BatchNorm()
    (34): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (35): BatchNorm()
    (36): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (37): BatchNorm()
    (38): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (39): BatchNorm()
    (40): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (41): BatchNorm()
    (42): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (43): BatchNorm()
    (44): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (45): BatchNorm()
    (46): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (47): BatchNorm()
    (48): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (49): BatchNorm()
    (50): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (51): BatchNorm()
    (52): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (53): BatchNorm()
    (54): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (55): BatchNorm()
    (56): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (57): BatchNorm()
    (58): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (59): BatchNorm()
    (60): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (61): BatchNorm()
    (62): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (63): BatchNorm()
    (64): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (65): BatchNorm()
    (66): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (67): BatchNorm()
    (68): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (69): BatchNorm()
    (70): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (71): BatchNorm()
    (72): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (73): BatchNorm()
    (74): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (75): BatchNorm()
    (76): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (77): BatchNorm()
    (78): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (79): BatchNorm()
    (80): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (81): BatchNorm()
    (82): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (83): BatchNorm()
    (84): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (85): BatchNorm()
    (86): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (87): BatchNorm()
    (88): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (89): BatchNorm()
    (90): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (91): BatchNorm()
    (92): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (93): BatchNorm()
    (94): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (95): BatchNorm()
    (96): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (97): BatchNorm()
    (98): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (99): BatchNorm()
    (100): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (101): BatchNorm()
    (102): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (103): BatchNorm()
    (104): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (105): BatchNorm()
    (106): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (107): BatchNorm()
    (108): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (109): BatchNorm()
    (110): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (111): BatchNorm()
    (112): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (113): BatchNorm()
    (114): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (115): BatchNorm()
    (116): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (117): BatchNorm()
    (118): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (119): BatchNorm()
    (120): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (121): BatchNorm()
    (122): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (123): BatchNorm()
    (124): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (125): BatchNorm()
    (126): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (127): BatchNorm()
  )
)
Evaluate (epoch 0) -- logp(x) = -14233.260 +/- 78.796
Evaluate (epoch 0) -- logp(x) = -95852.734 +/- 4034.554
Evaluate (epoch 0) -- logp(x) = -126737.078 +/- 6495.576
Evaluate (epoch 0) -- logp(x) = -2164383.500 +/- 177631.016
Evaluate (epoch 0) -- logp(x) = -347987.406 +/- 18191.471
Evaluate (epoch 0) -- logp(x) = -19446.477 +/- 72.984
Evaluate (epoch 0) -- logp(x) = -66651.953 +/- 822.298
Evaluate (epoch 0) -- logp(x) = -19471.102 +/- 256.410
Evaluate (epoch 0) -- logp(x) = -442470.844 +/- 8876.946
Evaluate (epoch 0) -- logp(x) = -21075.203 +/- 29.892
Evaluate (epoch 1) -- logp(x) = -19088.727 +/- 75.089
Evaluate (epoch 1) -- logp(x) = -4693816.500 +/- 179077.203
Evaluate (epoch 1) -- logp(x) = -292434.656 +/- 22280.365
Evaluate (epoch 1) -- logp(x) = -10345944.000 +/- 76794.977
Evaluate (epoch 1) -- logp(x) = -441433.875 +/- 22467.150
Evaluate (epoch 1) -- logp(x) = -20524.352 +/- 45.743
Evaluate (epoch 1) -- logp(x) = -135652288.000 +/- 3379906.250
Evaluate (epoch 1) -- logp(x) = -1697101.250 +/- 328654.969
Evaluate (epoch 1) -- logp(x) = -375686144.000 +/- 857157.438
Evaluate (epoch 1) -- logp(x) = -135716.281 +/- 5711.109
Evaluate (epoch 2) -- logp(x) = -28832.061 +/- 41.053
Evaluate (epoch 2) -- logp(x) = -631298752.000 +/- 17419924.000
Evaluate (epoch 2) -- logp(x) = -9173300.000 +/- 1809547.625
Evaluate (epoch 2) -- logp(x) = -605001536.000 +/- 9744560.000
Evaluate (epoch 2) -- logp(x) = -29681.768 +/- 4.991
Evaluate (epoch 3) -- logp(x) = -39935.898 +/- 42.204
Evaluate (epoch 3) -- logp(x) = -34986237952.000 +/- 551839104.000
Evaluate (epoch 3) -- logp(x) = -1813400448.000 +/- 158180336.000
Evaluate (epoch 3) -- logp(x) = -39464173568.000 +/- 808376128.000
Evaluate (epoch 3) -- logp(x) = -216293648.000 +/- 6792292.500
Evaluate (epoch 2) -- logp(x) = -48944.379 +/- 8.039
Evaluate (epoch 2) -- logp(x) = -4075944.250 +/- 809240.125
Evaluate (epoch 2) -- logp(x) = -49760.090 +/- 11.323
Evaluate (epoch 2) -- logp(x) = -7813554176.000 +/- 572379008.000
Evaluate (epoch 2) -- logp(x) = -49502.633 +/- 0.453
Evaluate (epoch 4) -- logp(x) = -38086.547 +/- 34.144
Evaluate (epoch 4) -- logp(x) = -4750835712000.000 +/- 190485348352.000
Evaluate (epoch 4) -- logp(x) = -38336.242 +/- 15.088
Evaluate (epoch 4) -- logp(x) = -5732103618560.000 +/- 101528543232.000
Evaluate (epoch 4) -- logp(x) = -38070.504 +/- 2.416
Evaluate (epoch 3) -- logp(x) = -68223.344 +/- 8.907
Evaluate (epoch 3) -- logp(x) = -13772877987840.000 +/- 950997483520.000
Evaluate (epoch 3) -- logp(x) = -68935.250 +/- 14.031
Evaluate (epoch 3) -- logp(x) = -123324315205632.000 +/- 14085445910528.000
Evaluate (epoch 3) -- logp(x) = -68721.047 +/- 3.880
Evaluate (epoch 5) -- logp(x) = -51178.898 +/- 18.826
Evaluate (epoch 5) -- logp(x) = -89074827264.000 +/- 1824197888.000
Evaluate (epoch 5) -- logp(x) = -544693056.000 +/- 107855200.000
Evaluate (epoch 5) -- logp(x) = -106064388096.000 +/- 1318016896.000
Evaluate (epoch 5) -- logp(x) = -50699.836 +/- 19.140
Evaluate (epoch 4) -- logp(x) = -72778.406 +/- 14.101
Evaluate (epoch 4) -- logp(x) = -138572925501440.000 +/- 8128032669696.000
Evaluate (epoch 4) -- logp(x) = -73672.047 +/- 13.716
Evaluate (epoch 4) -- logp(x) = -3325978650607616.000 +/- 154317470302208.000
Evaluate (epoch 4) -- logp(x) = -73492.297 +/- 2.363
Evaluate (epoch 6) -- logp(x) = -38830.312 +/- 39.034
Evaluate (epoch 6) -- logp(x) = -648934785024.000 +/- 20212023296.000
Evaluate (epoch 6) -- logp(x) = -1262976256.000 +/- 250099024.000
Evaluate (epoch 6) -- logp(x) = -501170110464.000 +/- 24300410880.000
Evaluate (epoch 6) -- logp(x) = -37770.984 +/- 41.184
Evaluate (epoch 7) -- logp(x) = -45645.016 +/- 28.231
Evaluate (epoch 7) -- logp(x) = -81659927986176.000 +/- 3119351595008.000
Evaluate (epoch 7) -- logp(x) = -836884889600.000 +/- 165727911936.000
Evaluate (epoch 7) -- logp(x) = -113219163127808.000 +/- 1638837125120.000
Evaluate (epoch 7) -- logp(x) = -45269.715 +/- 24.750
Evaluate (epoch 5) -- logp(x) = -61538.629 +/- 22.874
Evaluate (epoch 5) -- logp(x) = -232029536256.000 +/- 46580903936.000
Evaluate (epoch 5) -- logp(x) = -62089.703 +/- 23.644
Evaluate (epoch 5) -- logp(x) = -748235782094848.000 +/- 34997272576000.000
Evaluate (epoch 5) -- logp(x) = -61814.414 +/- 2.869
Evaluate (epoch 8) -- logp(x) = -64254.980 +/- 49.084
Evaluate (epoch 8) -- logp(x) = -386161115136.000 +/- 18632673280.000
Evaluate (epoch 8) -- logp(x) = -63440.055 +/- 247.518
Evaluate (epoch 8) -- logp(x) = -535549149184.000 +/- 17773242368.000
Evaluate (epoch 8) -- logp(x) = -64328.004 +/- 3.322
Evaluate (epoch 6) -- logp(x) = -92228.633 +/- 14.269
Evaluate (epoch 6) -- logp(x) = -4981259763712.000 +/- 886407495680.000
Evaluate (epoch 6) -- logp(x) = -92593.867 +/- 16.245
Evaluate (epoch 6) -- logp(x) = -213202243682304.000 +/- 29639980351488.000
Evaluate (epoch 6) -- logp(x) = -92242.203 +/- 2.582
Evaluate (epoch 9) -- logp(x) = -48794.664 +/- 39.890
Evaluate (epoch 9) -- logp(x) = -1866220699648.000 +/- 71771004928.000
Evaluate (epoch 9) -- logp(x) = -813116800.000 +/- 161011488.000
Evaluate (epoch 9) -- logp(x) = -2125740113920.000 +/- 59132063744.000
Evaluate (epoch 9) -- logp(x) = -48367.410 +/- 0.824
Evaluate (epoch 7) -- logp(x) = -100113.953 +/- 20.431
Evaluate (epoch 7) -- logp(x) = -99992.984 +/- 64.628
Evaluate (epoch 7) -- logp(x) = -100162.609 +/- 12.220
Evaluate (epoch 7) -- logp(x) = -276756.906 +/- 36077.359
Evaluate (epoch 7) -- logp(x) = -99879.078 +/- 1.347
Evaluate (epoch 10) -- logp(x) = -42883.035 +/- 25.242
Evaluate (epoch 10) -- logp(x) = -2362530332672.000 +/- 90991484928.000
Evaluate (epoch 10) -- logp(x) = -43452.000 +/- 121.874
Evaluate (epoch 10) -- logp(x) = -2600152858624.000 +/- 116810760192.000
Evaluate (epoch 10) -- logp(x) = -42885.336 +/- 9.000
Evaluate (epoch 11) -- logp(x) = -44064.871 +/- 34.989
Evaluate (epoch 11) -- logp(x) = -12103479984128.000 +/- 459873386496.000
Evaluate (epoch 11) -- logp(x) = -14522398720.000 +/- 2875854848.000
Evaluate (epoch 11) -- logp(x) = -10337836335104.000 +/- 530897829888.000
Evaluate (epoch 11) -- logp(x) = -43622.285 +/- 13.170
Evaluate (epoch 8) -- logp(x) = -64553.418 +/- 15.977
Evaluate (epoch 8) -- logp(x) = -64670.438 +/- 32.045
Evaluate (epoch 8) -- logp(x) = -64662.973 +/- 9.931
Evaluate (epoch 8) -- logp(x) = -63690.859 +/- 63.095
Evaluate (epoch 8) -- logp(x) = -64426.734 +/- 1.004
Evaluate (epoch 12) -- logp(x) = -128263380992.000 +/- 25493823488.000
Evaluate (epoch 12) -- logp(x) = -60069.289 +/- 21.744
Evaluate (epoch 12) -- logp(x) = -58813.164 +/- 88.700
Evaluate (epoch 12) -- logp(x) = -60685.105 +/- 17.232
Evaluate (epoch 12) -- logp(x) = -58027.977 +/- 9.351
Evaluate (epoch 9) -- logp(x) = -86880.266 +/- 37.329
Evaluate (epoch 9) -- logp(x) = -88028.281 +/- 43.485
Evaluate (epoch 9) -- logp(x) = -87176.820 +/- 23.012
Evaluate (epoch 9) -- logp(x) = -87611.039 +/- 37.728
Evaluate (epoch 9) -- logp(x) = -86678.438 +/- 6.418
Evaluate (epoch 13) -- logp(x) = -50488.945 +/- 23.780
Evaluate (epoch 13) -- logp(x) = -11873682456576.000 +/- 778756685824.000
Evaluate (epoch 13) -- logp(x) = -663537188864.000 +/- 131377250304.000
Evaluate (epoch 13) -- logp(x) = -10054921093120.000 +/- 1409922236416.000
Evaluate (epoch 13) -- logp(x) = -44857.734 +/- 84.091
Evaluate (epoch 10) -- logp(x) = -64930.816 +/- 21.634
Evaluate (epoch 10) -- logp(x) = -65933.836 +/- 23.701
Evaluate (epoch 10) -- logp(x) = -65196.047 +/- 17.197
Evaluate (epoch 10) -- logp(x) = -66188.500 +/- 16.891
Evaluate (epoch 10) -- logp(x) = -64930.578 +/- 3.012
Evaluate (epoch 14) -- logp(x) = -45558.719 +/- 30.503
Evaluate (epoch 14) -- logp(x) = -15623438467072.000 +/- 1580172050432.000
Evaluate (epoch 14) -- logp(x) = -15855360000.000 +/- 3132137472.000
Evaluate (epoch 14) -- logp(x) = -10468575936512.000 +/- 638681022464.000
Evaluate (epoch 14) -- logp(x) = -45729.680 +/- 6.334
Evaluate (epoch 15) -- logp(x) = -49084.633 +/- 20.719
Evaluate (epoch 15) -- logp(x) = -58149.000 +/- 850.106
Evaluate (epoch 15) -- logp(x) = -4302258831360.000 +/- 851974225920.000
Evaluate (epoch 15) -- logp(x) = -333843781386240.000 +/- 60175805317120.000
Evaluate (epoch 11) -- logp(x) = -70340.641 +/- 17.360
Evaluate (epoch 15) -- logp(x) = -287758178320384.000 +/- 8539622342656.000
Evaluate (epoch 11) -- logp(x) = -71106.344 +/- 22.562
Evaluate (epoch 11) -- logp(x) = -70579.773 +/- 12.468
Evaluate (epoch 11) -- logp(x) = -71160.844 +/- 17.749
Evaluate (epoch 11) -- logp(x) = -70374.125 +/- 2.125
Evaluate (epoch 16) -- logp(x) = -609097351168.000 +/- 39557824512.000
Evaluate (epoch 16) -- logp(x) = -143444.281 +/- 5169.705
Evaluate (epoch 16) -- logp(x) = -6354930176.000 +/- 1258450560.000
Evaluate (epoch 16) -- logp(x) = -160009.375 +/- 5585.060
Evaluate (epoch 16) -- logp(x) = -58275.219 +/- 325.550
Evaluate (epoch 12) -- logp(x) = -142666.547 +/- 25.482
Evaluate (epoch 12) -- logp(x) = -179185.969 +/- 7572.319
Evaluate (epoch 12) -- logp(x) = -142666.016 +/- 13.042
Evaluate (epoch 12) -- logp(x) = -8972750848.000 +/- 1028729728.000
Evaluate (epoch 12) -- logp(x) = -142440.172 +/- 7.665
Evaluate (epoch 17) -- logp(x) = -52031.234 +/- 267.784
Evaluate (epoch 17) -- logp(x) = -23335972.000 +/- 1742631.250
Evaluate (epoch 17) -- logp(x) = -79681.359 +/- 3627.579
Evaluate (epoch 17) -- logp(x) = -16135870.000 +/- 2063218.125
Evaluate (epoch 17) -- logp(x) = -59808.309 +/- 5.935
Evaluate (epoch 13) -- logp(x) = -52491.164 +/- 53.638
Evaluate (epoch 13) -- logp(x) = -60049.258 +/- 778.303
Evaluate (epoch 13) -- logp(x) = -53048.789 +/- 53.476
Evaluate (epoch 13) -- logp(x) = -10239467323392.000 +/- 899487432704.000
Evaluate (epoch 13) -- logp(x) = -52523.156 +/- 10.200
Evaluate (epoch 18) -- logp(x) = -62208.285 +/- 16.145
Evaluate (epoch 18) -- logp(x) = -789394496.000 +/- 146016944.000
Evaluate (epoch 18) -- logp(x) = -62447.547 +/- 37.381
Evaluate (epoch 18) -- logp(x) = -2511868672.000 +/- 119536064.000
Evaluate (epoch 18) -- logp(x) = -60606.062 +/- 7.239
Evaluate (epoch 19) -- logp(x) = -57894.332 +/- 29.206
Evaluate (epoch 14) -- logp(x) = -51563.836 +/- 42.228
Evaluate (epoch 19) -- logp(x) = -10749530112.000 +/- 1971471360.000
Evaluate (epoch 14) -- logp(x) = -10201756336128.000 +/- 2048046006272.000
Evaluate (epoch 19) -- logp(x) = -59185.234 +/- 203.923
Evaluate (epoch 14) -- logp(x) = -51831.055 +/- 25.201
Evaluate (epoch 19) -- logp(x) = -38231457792.000 +/- 6386996736.000
Evaluate (epoch 14) -- logp(x) = -551668886274048.000 +/- 62298827784192.000
Evaluate (epoch 19) -- logp(x) = -56781.336 +/- 28.918
Evaluate (epoch 14) -- logp(x) = -51483.637 +/- 4.785
Evaluate (epoch 20) -- logp(x) = -55283.891 +/- 88.966
Evaluate (epoch 20) -- logp(x) = -751268.500 +/- 79124.141
Evaluate (epoch 20) -- logp(x) = -58095.625 +/- 359.109
Evaluate (epoch 20) -- logp(x) = -555531.812 +/- 8452.709
Evaluate (epoch 20) -- logp(x) = -56185.656 +/- 24.063
Evaluate (epoch 15) -- logp(x) = -99711.906 +/- 34.772
Evaluate (epoch 15) -- logp(x) = -98396.078 +/- 458.096
Evaluate (epoch 15) -- logp(x) = -99922.984 +/- 13.674
Evaluate (epoch 15) -- logp(x) = -53030472.000 +/- 5063627.000
Evaluate (epoch 15) -- logp(x) = -99675.562 +/- 5.092
Evaluate (epoch 21) -- logp(x) = -49614.457 +/- 34.862
Evaluate (epoch 21) -- logp(x) = -5621677.500 +/- 990621.500
Evaluate (epoch 21) -- logp(x) = -50764.965 +/- 255.997
Evaluate (epoch 21) -- logp(x) = -380966976.000 +/- 75391256.000
Evaluate (epoch 21) -- logp(x) = -48089.109 +/- 68.797
Evaluate (epoch 16) -- logp(x) = -73180.461 +/- 393.978
Evaluate (epoch 16) -- logp(x) = -3624472.000 +/- 345275.250
Evaluate (epoch 16) -- logp(x) = -301804.750 +/- 27214.373
Evaluate (epoch 16) -- logp(x) = -73213.469 +/- 210.228
Evaluate (epoch 16) -- logp(x) = -69948.391 +/- 105.891
Evaluate (epoch 22) -- logp(x) = -47866.660 +/- 113.671
Evaluate (epoch 22) -- logp(x) = -175923.312 +/- 14015.790
Evaluate (epoch 22) -- logp(x) = -49147.059 +/- 149.766
Evaluate (epoch 22) -- logp(x) = -240608.500 +/- 18675.779
Evaluate (epoch 22) -- logp(x) = -46854.945 +/- 1.466
Evaluate (epoch 17) -- logp(x) = -80917.781 +/- 218.202
Evaluate (epoch 17) -- logp(x) = -85081.773 +/- 63.864
Evaluate (epoch 17) -- logp(x) = -82448.828 +/- 89.303
Evaluate (epoch 17) -- logp(x) = -87686.844 +/- 122.184
Evaluate (epoch 17) -- logp(x) = -81390.562 +/- 9.800
Evaluate (epoch 23) -- logp(x) = -52442.645 +/- 73.399
Evaluate (epoch 23) -- logp(x) = -104946.797 +/- 5294.562
Evaluate (epoch 23) -- logp(x) = -53955.516 +/- 122.850
Evaluate (epoch 23) -- logp(x) = -215955.438 +/- 22540.594
Evaluate (epoch 23) -- logp(x) = -52372.480 +/- 21.086
Evaluate (epoch 24) -- logp(x) = -49768.938 +/- 30.434
Evaluate (epoch 24) -- logp(x) = -73997184.000 +/- 14359664.000
Evaluate (epoch 18) -- logp(x) = -68182.188 +/- 49.645
Evaluate (epoch 24) -- logp(x) = -51004.996 +/- 193.051
Evaluate (epoch 18) -- logp(x) = -73433.844 +/- 1132.371
Evaluate (epoch 18) -- logp(x) = -68066.000 +/- 23.380
Evaluate (epoch 24) -- logp(x) = -467570432.000 +/- 91540224.000
Evaluate (epoch 24) -- logp(x) = -139569774592.000 +/- 7550397440.000
Evaluate (epoch 18) -- logp(x) = -93663.094 +/- 1619.728
Evaluate (epoch 18) -- logp(x) = -68044.156 +/- 10.337
Evaluate (epoch 25) -- logp(x) = -54657.383 +/- 73.009
Evaluate (epoch 25) -- logp(x) = -3049761792.000 +/- 302628384.000
Evaluate (epoch 25) -- logp(x) = -94958056.000 +/- 18793486.000
Evaluate (epoch 25) -- logp(x) = -7994857984.000 +/- 614906048.000
Evaluate (epoch 25) -- logp(x) = -6856203304960.000 +/- 73044893696.000
Evaluate (epoch 19) -- logp(x) = -65568.398 +/- 52.605
Evaluate (epoch 19) -- logp(x) = -2304597.500 +/- 169184.422
Evaluate (epoch 19) -- logp(x) = -65688.625 +/- 22.103
Evaluate (epoch 19) -- logp(x) = -14792099840.000 +/- 2769687040.000
Evaluate (epoch 19) -- logp(x) = -65291.441 +/- 10.363
Evaluate (epoch 26) -- logp(x) = -52566.617 +/- 96.554
Evaluate (epoch 26) -- logp(x) = -2018697088.000 +/- 254741696.000
Evaluate (epoch 26) -- logp(x) = -46582451929088.000 +/- 7469675315200.000
Evaluate (epoch 26) -- logp(x) = -5198900224.000 +/- 937579904.000
Evaluate (epoch 26) -- logp(x) = -1989549752320.000 +/- 54129692672.000
Evaluate (epoch 20) -- logp(x) = -80075.922 +/- 36.296
Evaluate (epoch 20) -- logp(x) = -494222.406 +/- 82922.797
Evaluate (epoch 20) -- logp(x) = -80725.859 +/- 16.318
Evaluate (epoch 20) -- logp(x) = -293343104.000 +/- 7592619.500
Evaluate (epoch 20) -- logp(x) = -80194.359 +/- 5.232
Evaluate (epoch 27) -- logp(x) = -53669.090 +/- 122.030
Evaluate (epoch 27) -- logp(x) = -298337.375 +/- 34568.172
Evaluate (epoch 27) -- logp(x) = -54316.004 +/- 187.154
Evaluate (epoch 27) -- logp(x) = -526258.250 +/- 67748.773
Evaluate (epoch 27) -- logp(x) = -53543.043 +/- 43.473
Evaluate (epoch 21) -- logp(x) = -120653.078 +/- 103.875
Evaluate (epoch 21) -- logp(x) = -947879424.000 +/- 12756137.000
Evaluate (epoch 21) -- logp(x) = -1875473.375 +/- 347870.156
Evaluate (epoch 21) -- logp(x) = -1991011584.000 +/- 33650648.000
Evaluate (epoch 21) -- logp(x) = -118941.156 +/- 11.545
Evaluate (epoch 28) -- logp(x) = -54503.730 +/- 68.269
Evaluate (epoch 28) -- logp(x) = -210435407872.000 +/- 22599227392.000
Evaluate (epoch 28) -- logp(x) = -13969073.000 +/- 2755324.500
Evaluate (epoch 28) -- logp(x) = -310650011648.000 +/- 61592989696.000
Evaluate (epoch 28) -- logp(x) = -54570.695 +/- 16.592
Evaluate (epoch 29) -- logp(x) = -77352.406 +/- 43.233
Evaluate (epoch 29) -- logp(x) = -106864885760.000 +/- 8491059200.000
Evaluate (epoch 29) -- logp(x) = -77758.375 +/- 92.375
Evaluate (epoch 29) -- logp(x) = -111160655872.000 +/- 15548092416.000
Evaluate (epoch 29) -- logp(x) = -78218.680 +/- 8.893
Evaluate (epoch 22) -- logp(x) = -71867.477 +/- 33.359
Evaluate (epoch 22) -- logp(x) = -3138099085312.000 +/- 74061471744.000
Evaluate (epoch 22) -- logp(x) = -74601.727 +/- 357.041
Evaluate (epoch 22) -- logp(x) = -5561541197824.000 +/- 93571694592.000
Evaluate (epoch 22) -- logp(x) = -71686.703 +/- 6.381
Evaluate (epoch 30) -- logp(x) = -53772.098 +/- 55.093
Evaluate (epoch 30) -- logp(x) = -10407103168512.000 +/- 492631097344.000
Evaluate (epoch 30) -- logp(x) = -414446223360.000 +/- 82072576000.000
Evaluate (epoch 30) -- logp(x) = -42488098193408.000 +/- 2473987145728.000
Evaluate (epoch 30) -- logp(x) = -55807.008 +/- 5.027
Evaluate (epoch 23) -- logp(x) = -70675.695 +/- 36.518
Evaluate (epoch 23) -- logp(x) = -5296108339200.000 +/- 205162184704.000
Evaluate (epoch 23) -- logp(x) = -33194895360.000 +/- 6573554176.000
Evaluate (epoch 23) -- logp(x) = -8847250948096.000 +/- 278130032640.000
Evaluate (epoch 23) -- logp(x) = -71093.078 +/- 26.410
Evaluate (epoch 31) -- logp(x) = -100084.172 +/- 156.252
Evaluate (epoch 31) -- logp(x) = -5215449.000 +/- 306192.562
Evaluate (epoch 31) -- logp(x) = -473392.562 +/- 73066.000
Evaluate (epoch 31) -- logp(x) = -8639220.000 +/- 455968.625
Evaluate (epoch 31) -- logp(x) = -104841.703 +/- 16.583
Evaluate (epoch 24) -- logp(x) = -61331.320 +/- 23.930
Evaluate (epoch 24) -- logp(x) = -43958298214400.000 +/- 1712414916608.000
Evaluate (epoch 24) -- logp(x) = -516261.906 +/- 90047.242
Evaluate (epoch 24) -- logp(x) = -141620171243520.000 +/- 4969488973824.000
Evaluate (epoch 24) -- logp(x) = -61697.316 +/- 11.472
Evaluate (epoch 32) -- logp(x) = -51587.836 +/- 48.896
Evaluate (epoch 32) -- logp(x) = -516019328.000 +/- 42322444.000
Evaluate (epoch 32) -- logp(x) = -131144.281 +/- 15824.662
Evaluate (epoch 32) -- logp(x) = -726586176.000 +/- 62479044.000
Evaluate (epoch 32) -- logp(x) = -52295.531 +/- 23.429
Evaluate (epoch 25) -- logp(x) = -71932.984 +/- 11.290
Evaluate (epoch 25) -- logp(x) = -23924159021056.000 +/- 1000363065344.000
Evaluate (epoch 25) -- logp(x) = -40114827264.000 +/- 7943906816.000
Evaluate (epoch 25) -- logp(x) = -113379360374784.000 +/- 5526911975424.000
Evaluate (epoch 25) -- logp(x) = -71795.844 +/- 6.853
Evaluate (epoch 33) -- logp(x) = -50828.312 +/- 18.810
Evaluate (epoch 33) -- logp(x) = -464854144.000 +/- 41582924.000
Evaluate (epoch 33) -- logp(x) = -12199863.000 +/- 2405513.750
Evaluate (epoch 33) -- logp(x) = -801579456.000 +/- 118706112.000
Evaluate (epoch 33) -- logp(x) = -50182.816 +/- 190.846
Evaluate (epoch 34) -- logp(x) = -55631.855 +/- 52.479
Evaluate (epoch 34) -- logp(x) = -405965438976.000 +/- 16876007424.000
Evaluate (epoch 34) -- logp(x) = -14946072576.000 +/- 1758120064.000
Evaluate (epoch 34) -- logp(x) = -449819377664.000 +/- 19104444416.000
Evaluate (epoch 34) -- logp(x) = -1987728128.000 +/- 142183632.000
Evaluate (epoch 26) -- logp(x) = -62963.363 +/- 55.086
Evaluate (epoch 26) -- logp(x) = -115632985079808.000 +/- 7760981786624.000
Evaluate (epoch 26) -- logp(x) = -65139.777 +/- 444.582
Evaluate (epoch 26) -- logp(x) = -233898986438656.000 +/- 8320458948608.000
Evaluate (epoch 26) -- logp(x) = -62472.121 +/- 10.041
Evaluate (epoch 35) -- logp(x) = -63654.680 +/- 34.816
Evaluate (epoch 35) -- logp(x) = -22825748480.000 +/- 294730464.000
Evaluate (epoch 35) -- logp(x) = -3791781632.000 +/- 750871424.000
Evaluate (epoch 35) -- logp(x) = -22072866816.000 +/- 2937652224.000
Evaluate (epoch 35) -- logp(x) = -60833.891 +/- 313.477
Evaluate (epoch 27) -- logp(x) = -57736.961 +/- 60.365
Evaluate (epoch 27) -- logp(x) = -1923614383276032.000 +/- 209078823419904.000
Evaluate (epoch 27) -- logp(x) = -12582060.000 +/- 2480173.250
Evaluate (epoch 27) -- logp(x) = -2487911614251008.000 +/- 59437670727680.000
Evaluate (epoch 27) -- logp(x) = -57435.727 +/- 14.973
Evaluate (epoch 36) -- logp(x) = -53838.477 +/- 59.912
Evaluate (epoch 36) -- logp(x) = -30783379456.000 +/- 1486949760.000
Evaluate (epoch 36) -- logp(x) = -2216352256.000 +/- 347028224.000
Evaluate (epoch 36) -- logp(x) = -32695619584.000 +/- 3712946432.000
Evaluate (epoch 36) -- logp(x) = -1114590848.000 +/- 36561332.000
Evaluate (epoch 28) -- logp(x) = -73544.961 +/- 31.895
Evaluate (epoch 28) -- logp(x) = -728244454162432.000 +/- 30568582479872.000
Evaluate (epoch 28) -- logp(x) = -75005.688 +/- 245.077
Evaluate (epoch 28) -- logp(x) = -2889650775523328.000 +/- 109040579379200.000
Evaluate (epoch 28) -- logp(x) = -73306.859 +/- 5.516
Evaluate (epoch 37) -- logp(x) = -74949.633 +/- 75.084
Evaluate (epoch 37) -- logp(x) = -3244894208.000 +/- 130439512.000
Evaluate (epoch 37) -- logp(x) = -6589461504.000 +/- 792682176.000
Evaluate (epoch 37) -- logp(x) = -4099450368.000 +/- 153416944.000
Evaluate (epoch 37) -- logp(x) = -8492248.000 +/- 132944.812
Evaluate (epoch 29) -- logp(x) = -52222.109 +/- 37.693
Evaluate (epoch 29) -- logp(x) = -12521323242717184.000 +/- 1327420443459584.000
Evaluate (epoch 29) -- logp(x) = -3533138944.000 +/- 699655424.000
Evaluate (epoch 29) -- logp(x) = -36177746484789248.000 +/- 1884686947188736.000
Evaluate (epoch 29) -- logp(x) = -52472.824 +/- 4.785
Evaluate (epoch 30) -- logp(x) = -61486.863 +/- 21.691
Evaluate (epoch 30) -- logp(x) = -13786154604167168.000 +/- 2425085369516032.000
Evaluate (epoch 30) -- logp(x) = -62881.016 +/- 160.429
Evaluate (epoch 30) -- logp(x) = -19794235104428032.000 +/- 1515712346587136.000
Evaluate (epoch 30) -- logp(x) = -61921.988 +/- 17.905
Evaluate (epoch 31) -- logp(x) = -55644.531 +/- 24.997
Evaluate (epoch 31) -- logp(x) = -88910324152926208.000 +/- 7342362019758080.000
Evaluate (epoch 31) -- logp(x) = -56926.680 +/- 132.564
Evaluate (epoch 31) -- logp(x) = -825784428469944320.000 +/- 55744333290143744.000
Evaluate (epoch 31) -- logp(x) = -56247.301 +/- 15.771
{'activation_fn': 'relu',
 'batch_size': 4,
 'channel': -1,
 'cond_label_size': None,
 'conditional': False,
 'data_dir': './data/',
 'dataset': '',
 'device': device(type='cuda', index=0),
 'flip_toy_var_order': False,
 'generate': False,
 'hidden_size': 64,
 'input_dims': torch.Size([3, 64, 64]),
 'input_order': 'sequential',
 'input_size': 12288,
 'log_interval': 1000,
 'lr': 0.0001,
 'model': 'realnvp',
 'n_blocks': 32,
 'n_components': 1,
 'n_epochs': 50,
 'n_hidden': 1,
 'no_batch_norm': False,
 'no_cuda': False,
 'output_dir': './results/mafencoderFalse_channel-1_nblocks32/',
 'restore_file': None,
 'results_file': 'results.txt',
 'seed': 1,
 'start_epoch': 0,
 'train': True,
 'use_encoder': False}
RealNVP(
  (net): FlowSequential(
    (0): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (1): BatchNorm()
    (2): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (3): BatchNorm()
    (4): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (5): BatchNorm()
    (6): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (7): BatchNorm()
    (8): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (9): BatchNorm()
    (10): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (11): BatchNorm()
    (12): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (13): BatchNorm()
    (14): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (15): BatchNorm()
    (16): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (17): BatchNorm()
    (18): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (19): BatchNorm()
    (20): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (21): BatchNorm()
    (22): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (23): BatchNorm()
    (24): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (25): BatchNorm()
    (26): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (27): BatchNorm()
    (28): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (29): BatchNorm()
    (30): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (31): BatchNorm()
    (32): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (33): BatchNorm()
    (34): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (35): BatchNorm()
    (36): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (37): BatchNorm()
    (38): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (39): BatchNorm()
    (40): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (41): BatchNorm()
    (42): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (43): BatchNorm()
    (44): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (45): BatchNorm()
    (46): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (47): BatchNorm()
    (48): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (49): BatchNorm()
    (50): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (51): BatchNorm()
    (52): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (53): BatchNorm()
    (54): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (55): BatchNorm()
    (56): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (57): BatchNorm()
    (58): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (59): BatchNorm()
    (60): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (61): BatchNorm()
    (62): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (63): BatchNorm()
  )
)
{'activation_fn': 'relu',
 'batch_size': 4,
 'channel': -1,
 'cond_label_size': None,
 'conditional': False,
 'data_dir': './data/',
 'dataset': 'toy',
 'device': device(type='cuda', index=0),
 'flip_toy_var_order': False,
 'generate': False,
 'hidden_size': 64,
 'input_dims': torch.Size([3, 64, 64]),
 'input_order': 'sequential',
 'input_size': 12288,
 'log_interval': 1000,
 'lr': 0.0001,
 'model': 'realnvp',
 'n_blocks': 64,
 'n_components': 1,
 'n_epochs': 50,
 'n_hidden': 1,
 'no_batch_norm': False,
 'no_cuda': False,
 'output_dir': './results/mafencoderFalse_channel-1_nblocks64/',
 'restore_file': None,
 'results_file': 'results.txt',
 'seed': 1,
 'start_epoch': 0,
 'train': True,
 'use_encoder': False}
RealNVP(
  (net): FlowSequential(
    (0): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (1): BatchNorm()
    (2): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (3): BatchNorm()
    (4): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (5): BatchNorm()
    (6): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (7): BatchNorm()
    (8): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (9): BatchNorm()
    (10): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (11): BatchNorm()
    (12): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (13): BatchNorm()
    (14): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (15): BatchNorm()
    (16): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (17): BatchNorm()
    (18): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (19): BatchNorm()
    (20): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (21): BatchNorm()
    (22): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (23): BatchNorm()
    (24): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (25): BatchNorm()
    (26): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (27): BatchNorm()
    (28): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (29): BatchNorm()
    (30): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (31): BatchNorm()
    (32): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (33): BatchNorm()
    (34): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (35): BatchNorm()
    (36): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (37): BatchNorm()
    (38): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (39): BatchNorm()
    (40): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (41): BatchNorm()
    (42): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (43): BatchNorm()
    (44): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (45): BatchNorm()
    (46): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (47): BatchNorm()
    (48): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (49): BatchNorm()
    (50): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (51): BatchNorm()
    (52): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (53): BatchNorm()
    (54): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (55): BatchNorm()
    (56): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (57): BatchNorm()
    (58): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (59): BatchNorm()
    (60): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (61): BatchNorm()
    (62): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (63): BatchNorm()
    (64): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (65): BatchNorm()
    (66): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (67): BatchNorm()
    (68): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (69): BatchNorm()
    (70): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (71): BatchNorm()
    (72): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (73): BatchNorm()
    (74): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (75): BatchNorm()
    (76): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (77): BatchNorm()
    (78): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (79): BatchNorm()
    (80): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (81): BatchNorm()
    (82): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (83): BatchNorm()
    (84): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (85): BatchNorm()
    (86): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (87): BatchNorm()
    (88): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (89): BatchNorm()
    (90): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (91): BatchNorm()
    (92): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (93): BatchNorm()
    (94): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (95): BatchNorm()
    (96): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (97): BatchNorm()
    (98): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (99): BatchNorm()
    (100): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (101): BatchNorm()
    (102): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (103): BatchNorm()
    (104): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (105): BatchNorm()
    (106): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (107): BatchNorm()
    (108): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (109): BatchNorm()
    (110): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (111): BatchNorm()
    (112): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (113): BatchNorm()
    (114): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (115): BatchNorm()
    (116): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (117): BatchNorm()
    (118): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (119): BatchNorm()
    (120): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (121): BatchNorm()
    (122): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (123): BatchNorm()
    (124): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (125): BatchNorm()
    (126): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (127): BatchNorm()
  )
)
Evaluate (epoch 0) -- logp(x) = -14233.260 +/- 78.796
Evaluate (epoch 0) -- logp(x) = -95852.734 +/- 4034.554
Evaluate (epoch 0) -- logp(x) = -126737.078 +/- 6495.576
Evaluate (epoch 0) -- logp(x) = -2702351.500 +/- 215402.500
Evaluate (epoch 0) -- logp(x) = -347987.406 +/- 18191.471
Evaluate (epoch 0) -- logp(x) = -19446.477 +/- 72.984
Evaluate (epoch 0) -- logp(x) = -66651.953 +/- 822.298
Evaluate (epoch 0) -- logp(x) = -19471.102 +/- 256.410
Evaluate (epoch 0) -- logp(x) = -460482.875 +/- 8704.568
Evaluate (epoch 0) -- logp(x) = -21075.203 +/- 29.892
{'activation_fn': 'relu',
 'batch_size': 4,
 'channel': -1,
 'cond_label_size': None,
 'conditional': False,
 'data_dir': './data/',
 'dataset': '',
 'device': device(type='cuda', index=0),
 'flip_toy_var_order': False,
 'generate': False,
 'hidden_size': 64,
 'input_dims': torch.Size([3, 64, 64]),
 'input_order': 'sequential',
 'input_size': 12288,
 'log_interval': 1000,
 'lr': 0.0001,
 'model': 'realnvp',
 'n_blocks': 32,
 'n_components': 1,
 'n_epochs': 50,
 'n_hidden': 1,
 'no_batch_norm': False,
 'no_cuda': False,
 'output_dir': './results/mafencoderFalse_channel-1_nblocks32/',
 'restore_file': None,
 'results_file': 'results.txt',
 'seed': 1,
 'start_epoch': 0,
 'train': True,
 'use_encoder': False}
RealNVP(
  (net): FlowSequential(
    (0): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (1): BatchNorm()
    (2): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (3): BatchNorm()
    (4): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (5): BatchNorm()
    (6): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (7): BatchNorm()
    (8): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (9): BatchNorm()
    (10): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (11): BatchNorm()
    (12): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (13): BatchNorm()
    (14): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (15): BatchNorm()
    (16): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (17): BatchNorm()
    (18): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (19): BatchNorm()
    (20): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (21): BatchNorm()
    (22): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (23): BatchNorm()
    (24): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (25): BatchNorm()
    (26): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (27): BatchNorm()
    (28): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (29): BatchNorm()
    (30): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (31): BatchNorm()
    (32): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (33): BatchNorm()
    (34): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (35): BatchNorm()
    (36): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (37): BatchNorm()
    (38): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (39): BatchNorm()
    (40): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (41): BatchNorm()
    (42): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (43): BatchNorm()
    (44): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (45): BatchNorm()
    (46): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (47): BatchNorm()
    (48): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (49): BatchNorm()
    (50): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (51): BatchNorm()
    (52): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (53): BatchNorm()
    (54): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (55): BatchNorm()
    (56): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (57): BatchNorm()
    (58): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (59): BatchNorm()
    (60): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (61): BatchNorm()
    (62): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (63): BatchNorm()
  )
)
Evaluate (epoch 1) -- logp(x) = -20524.352 +/- 45.743
Evaluate (epoch 1) -- logp(x) = -135652288.000 +/- 3379906.250
Evaluate (epoch 1) -- logp(x) = -1697101.250 +/- 328654.969
Evaluate (epoch 1) -- logp(x) = -404992288.000 +/- 2091052.875
Evaluate (epoch 1) -- logp(x) = -135716.281 +/- 5711.109
Evaluate (epoch 0) -- logp(x) = -14233.260 +/- 78.796
Evaluate (epoch 0) -- logp(x) = -95852.734 +/- 4034.554
Evaluate (epoch 0) -- logp(x) = -126737.078 +/- 6495.576
Evaluate (epoch 0) -- logp(x) = -2529007.750 +/- 169388.875
Evaluate (epoch 0) -- logp(x) = -347987.406 +/- 18191.471
{'activation_fn': 'relu',
 'batch_size': 4,
 'channel': -1,
 'cond_label_size': None,
 'conditional': False,
 'data_dir': './data/',
 'dataset': '',
 'device': device(type='cuda', index=0),
 'flip_toy_var_order': False,
 'generate': False,
 'hidden_size': 64,
 'input_dims': torch.Size([3, 64, 64]),
 'input_order': 'sequential',
 'input_size': 12288,
 'log_interval': 1000,
 'lr': 0.0001,
 'model': 'realnvp',
 'n_blocks': 32,
 'n_components': 1,
 'n_epochs': 50,
 'n_hidden': 1,
 'no_batch_norm': False,
 'no_cuda': False,
 'output_dir': './results/mafencoderFalse_channel-1_nblocks32/',
 'restore_file': None,
 'results_file': 'results.txt',
 'seed': 1,
 'start_epoch': 0,
 'train': True,
 'use_encoder': False}
RealNVP(
  (net): FlowSequential(
    (0): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (1): BatchNorm()
    (2): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (3): BatchNorm()
    (4): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (5): BatchNorm()
    (6): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (7): BatchNorm()
    (8): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (9): BatchNorm()
    (10): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (11): BatchNorm()
    (12): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (13): BatchNorm()
    (14): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (15): BatchNorm()
    (16): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (17): BatchNorm()
    (18): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (19): BatchNorm()
    (20): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (21): BatchNorm()
    (22): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (23): BatchNorm()
    (24): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (25): BatchNorm()
    (26): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (27): BatchNorm()
    (28): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (29): BatchNorm()
    (30): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (31): BatchNorm()
    (32): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (33): BatchNorm()
    (34): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (35): BatchNorm()
    (36): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (37): BatchNorm()
    (38): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (39): BatchNorm()
    (40): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (41): BatchNorm()
    (42): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (43): BatchNorm()
    (44): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (45): BatchNorm()
    (46): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (47): BatchNorm()
    (48): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (49): BatchNorm()
    (50): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (51): BatchNorm()
    (52): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (53): BatchNorm()
    (54): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (55): BatchNorm()
    (56): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (57): BatchNorm()
    (58): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (59): BatchNorm()
    (60): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (61): BatchNorm()
    (62): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (63): BatchNorm()
  )
)
Evaluate (epoch 2) -- logp(x) = -48944.379 +/- 8.039
Evaluate (epoch 2) -- logp(x) = -4075944.250 +/- 809240.125
Evaluate (epoch 2) -- logp(x) = -49760.090 +/- 11.323
Evaluate (epoch 2) -- logp(x) = -10530455552.000 +/- 938973504.000
Evaluate (epoch 2) -- logp(x) = -49502.633 +/- 0.453
Evaluate (epoch 0) -- logp(x) = -14233.260 +/- 78.796
Evaluate (epoch 0) -- logp(x) = -95852.734 +/- 4034.554
Evaluate (epoch 0) -- logp(x) = -126737.078 +/- 6495.576
Evaluate (epoch 0) -- logp(x) = -3040359.000 +/- 134068.312
Evaluate (epoch 0) -- logp(x) = -347987.406 +/- 18191.471
Evaluate (epoch 3) -- logp(x) = -68223.344 +/- 8.907
Evaluate (epoch 3) -- logp(x) = -13772877987840.000 +/- 950997483520.000
Evaluate (epoch 3) -- logp(x) = -68935.250 +/- 14.031
Evaluate (epoch 3) -- logp(x) = -109480436039680.000 +/- 12323117858816.000
Evaluate (epoch 3) -- logp(x) = -68721.047 +/- 3.880
{'activation_fn': 'relu',
 'batch_size': 4,
 'channel': -1,
 'cond_label_size': None,
 'conditional': False,
 'data_dir': './data/',
 'dataset': '',
 'device': device(type='cuda', index=0),
 'flip_toy_var_order': False,
 'generate': False,
 'hidden_size': 64,
 'input_dims': torch.Size([3, 64, 64]),
 'input_order': 'sequential',
 'input_size': 12288,
 'log_interval': 1000,
 'lr': 0.0001,
 'model': 'realnvp',
 'n_blocks': 32,
 'n_components': 1,
 'n_epochs': 50,
 'n_hidden': 1,
 'no_batch_norm': False,
 'no_cuda': False,
 'output_dir': './results/mafencoderFalse_channel-1_nblocks32/',
 'restore_file': None,
 'results_file': 'results.txt',
 'seed': 1,
 'start_epoch': 0,
 'train': True,
 'use_encoder': False}
RealNVP(
  (net): FlowSequential(
    (0): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (1): BatchNorm()
    (2): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (3): BatchNorm()
    (4): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (5): BatchNorm()
    (6): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (7): BatchNorm()
    (8): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (9): BatchNorm()
    (10): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (11): BatchNorm()
    (12): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (13): BatchNorm()
    (14): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (15): BatchNorm()
    (16): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (17): BatchNorm()
    (18): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (19): BatchNorm()
    (20): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (21): BatchNorm()
    (22): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (23): BatchNorm()
    (24): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (25): BatchNorm()
    (26): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (27): BatchNorm()
    (28): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (29): BatchNorm()
    (30): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (31): BatchNorm()
    (32): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (33): BatchNorm()
    (34): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (35): BatchNorm()
    (36): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (37): BatchNorm()
    (38): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (39): BatchNorm()
    (40): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (41): BatchNorm()
    (42): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (43): BatchNorm()
    (44): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (45): BatchNorm()
    (46): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (47): BatchNorm()
    (48): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (49): BatchNorm()
    (50): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (51): BatchNorm()
    (52): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (53): BatchNorm()
    (54): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (55): BatchNorm()
    (56): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (57): BatchNorm()
    (58): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (59): BatchNorm()
    (60): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (61): BatchNorm()
    (62): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (63): BatchNorm()
  )
)
Evaluate (epoch 4) -- logp(x) = -72778.406 +/- 14.101
Evaluate (epoch 4) -- logp(x) = -138572925501440.000 +/- 8128032669696.000
Evaluate (epoch 4) -- logp(x) = -73672.047 +/- 13.716
Evaluate (epoch 4) -- logp(x) = -4395388463218688.000 +/- 339420796420096.000
Evaluate (epoch 4) -- logp(x) = -73492.297 +/- 2.363
Evaluate (epoch 0) -- logp(x) = -14233.260 +/- 78.796
Evaluate (epoch 0) -- logp(x) = -95852.734 +/- 4034.554
Evaluate (epoch 0) -- logp(x) = -126737.078 +/- 6495.576
Evaluate (epoch 0) -- logp(x) = -1607323.125 +/- 101887.922
Evaluate (epoch 0) -- logp(x) = -347987.406 +/- 18191.471
Evaluate (epoch 5) -- logp(x) = -61538.629 +/- 22.874
Evaluate (epoch 5) -- logp(x) = -232029536256.000 +/- 46580903936.000
Evaluate (epoch 5) -- logp(x) = -62089.703 +/- 23.644
Evaluate (epoch 5) -- logp(x) = -919490086830080.000 +/- 34404892147712.000
Evaluate (epoch 5) -- logp(x) = -61814.414 +/- 2.869
Evaluate (epoch 1) -- logp(x) = -19088.727 +/- 75.089
Evaluate (epoch 1) -- logp(x) = -4693816.500 +/- 179077.203
Evaluate (epoch 1) -- logp(x) = -292434.656 +/- 22280.365
Evaluate (epoch 1) -- logp(x) = -10751114.000 +/- 88101.266
Evaluate (epoch 1) -- logp(x) = -441433.875 +/- 22467.150
Evaluate (epoch 6) -- logp(x) = -92228.633 +/- 14.269
Evaluate (epoch 6) -- logp(x) = -4981259763712.000 +/- 886407495680.000
Evaluate (epoch 6) -- logp(x) = -92593.867 +/- 16.245
Evaluate (epoch 6) -- logp(x) = -255110185746432.000 +/- 31931706114048.000
Evaluate (epoch 6) -- logp(x) = -92242.203 +/- 2.582
Evaluate (epoch 2) -- logp(x) = -28832.061 +/- 41.053
Evaluate (epoch 2) -- logp(x) = -631298752.000 +/- 17419924.000
Evaluate (epoch 2) -- logp(x) = -9173300.000 +/- 1809547.625
Evaluate (epoch 2) -- logp(x) = -632953216.000 +/- 18223900.000
Evaluate (epoch 2) -- logp(x) = -29681.768 +/- 4.991
Evaluate (epoch 7) -- logp(x) = -100113.953 +/- 20.431
Evaluate (epoch 7) -- logp(x) = -99992.984 +/- 64.628
Evaluate (epoch 7) -- logp(x) = -100162.609 +/- 12.220
Evaluate (epoch 7) -- logp(x) = -6360894.000 +/- 1244829.000
Evaluate (epoch 7) -- logp(x) = -99879.078 +/- 1.347
Evaluate (epoch 3) -- logp(x) = -39935.898 +/- 42.204
Evaluate (epoch 3) -- logp(x) = -34986237952.000 +/- 551839104.000
Evaluate (epoch 3) -- logp(x) = -1813400448.000 +/- 158180336.000
Evaluate (epoch 3) -- logp(x) = -42735632384.000 +/- 705705536.000
Evaluate (epoch 3) -- logp(x) = -216293648.000 +/- 6792292.500
{'activation_fn': 'relu',
 'batch_size': 4,
 'channel': -1,
 'cond_label_size': None,
 'conditional': False,
 'data_dir': './data/',
 'dataset': '',
 'device': device(type='cuda', index=0),
 'flip_toy_var_order': False,
 'generate': False,
 'hidden_size': 64,
 'input_dims': torch.Size([3, 64, 64]),
 'input_order': 'sequential',
 'input_size': 12288,
 'log_interval': 1000,
 'lr': 0.0001,
 'model': 'realnvp',
 'n_blocks': 32,
 'n_components': 1,
 'n_epochs': 50,
 'n_hidden': 1,
 'no_batch_norm': False,
 'no_cuda': False,
 'output_dir': './results/mafencoderFalse_channel-1_nblocks32/',
 'restore_file': None,
 'results_file': 'results.txt',
 'seed': 1,
 'start_epoch': 0,
 'train': True,
 'use_encoder': False}
RealNVP(
  (net): FlowSequential(
    (0): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (1): BatchNorm()
    (2): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (3): BatchNorm()
    (4): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (5): BatchNorm()
    (6): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (7): BatchNorm()
    (8): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (9): BatchNorm()
    (10): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (11): BatchNorm()
    (12): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (13): BatchNorm()
    (14): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (15): BatchNorm()
    (16): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (17): BatchNorm()
    (18): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (19): BatchNorm()
    (20): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (21): BatchNorm()
    (22): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (23): BatchNorm()
    (24): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (25): BatchNorm()
    (26): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (27): BatchNorm()
    (28): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (29): BatchNorm()
    (30): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (31): BatchNorm()
    (32): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (33): BatchNorm()
    (34): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (35): BatchNorm()
    (36): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (37): BatchNorm()
    (38): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (39): BatchNorm()
    (40): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (41): BatchNorm()
    (42): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (43): BatchNorm()
    (44): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (45): BatchNorm()
    (46): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (47): BatchNorm()
    (48): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (49): BatchNorm()
    (50): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (51): BatchNorm()
    (52): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (53): BatchNorm()
    (54): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (55): BatchNorm()
    (56): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (57): BatchNorm()
    (58): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (59): BatchNorm()
    (60): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (61): BatchNorm()
    (62): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (63): BatchNorm()
  )
)
Evaluate (epoch 8) -- logp(x) = -64553.418 +/- 15.977
Evaluate (epoch 8) -- logp(x) = -64670.438 +/- 32.045
Evaluate (epoch 8) -- logp(x) = -64662.973 +/- 9.931
Evaluate (epoch 8) -- logp(x) = -63628.406 +/- 83.139
Evaluate (epoch 8) -- logp(x) = -64426.734 +/- 1.004
Evaluate (epoch 0) -- logp(x) = -14233.260 +/- 78.796
Evaluate (epoch 0) -- logp(x) = -95852.734 +/- 4034.554
Evaluate (epoch 0) -- logp(x) = -126737.078 +/- 6495.576
Evaluate (epoch 0) -- logp(x) = -1170912.625 +/- 22372.219
Evaluate (epoch 0) -- logp(x) = -347987.406 +/- 18191.471
Evaluate (epoch 9) -- logp(x) = -86880.266 +/- 37.329
Evaluate (epoch 9) -- logp(x) = -88028.281 +/- 43.485
Evaluate (epoch 9) -- logp(x) = -87176.820 +/- 23.012
Evaluate (epoch 9) -- logp(x) = -87792.625 +/- 46.633
Evaluate (epoch 9) -- logp(x) = -86678.438 +/- 6.418
Evaluate (epoch 1) -- logp(x) = -19088.727 +/- 75.089
Evaluate (epoch 1) -- logp(x) = -4693816.500 +/- 179077.203
Evaluate (epoch 1) -- logp(x) = -292434.656 +/- 22280.365
Evaluate (epoch 1) -- logp(x) = -10620582.000 +/- 184524.141
Evaluate (epoch 1) -- logp(x) = -441433.875 +/- 22467.150
{'activation_fn': 'relu',
 'batch_size': 4,
 'channel': -1,
 'cond_label_size': None,
 'conditional': False,
 'data_dir': './data/',
 'dataset': '',
 'device': device(type='cuda', index=0),
 'flip_toy_var_order': False,
 'generate': False,
 'hidden_size': 64,
 'input_dims': torch.Size([3, 64, 64]),
 'input_order': 'sequential',
 'input_size': 12288,
 'log_interval': 1000,
 'lr': 0.0001,
 'model': 'realnvp',
 'n_blocks': 32,
 'n_components': 1,
 'n_epochs': 50,
 'n_hidden': 1,
 'no_batch_norm': False,
 'no_cuda': False,
 'output_dir': './results/mafencoderFalse_channel-1_nblocks32/',
 'restore_file': None,
 'results_file': 'results.txt',
 'seed': 1,
 'start_epoch': 0,
 'train': True,
 'use_encoder': False}
RealNVP(
  (net): FlowSequential(
    (0): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (1): BatchNorm()
    (2): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (3): BatchNorm()
    (4): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (5): BatchNorm()
    (6): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (7): BatchNorm()
    (8): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (9): BatchNorm()
    (10): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (11): BatchNorm()
    (12): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (13): BatchNorm()
    (14): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (15): BatchNorm()
    (16): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (17): BatchNorm()
    (18): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (19): BatchNorm()
    (20): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (21): BatchNorm()
    (22): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (23): BatchNorm()
    (24): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (25): BatchNorm()
    (26): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (27): BatchNorm()
    (28): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (29): BatchNorm()
    (30): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (31): BatchNorm()
    (32): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (33): BatchNorm()
    (34): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (35): BatchNorm()
    (36): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (37): BatchNorm()
    (38): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (39): BatchNorm()
    (40): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (41): BatchNorm()
    (42): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (43): BatchNorm()
    (44): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (45): BatchNorm()
    (46): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (47): BatchNorm()
    (48): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (49): BatchNorm()
    (50): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (51): BatchNorm()
    (52): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (53): BatchNorm()
    (54): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (55): BatchNorm()
    (56): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (57): BatchNorm()
    (58): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (59): BatchNorm()
    (60): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (61): BatchNorm()
    (62): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (63): BatchNorm()
  )
)
{'activation_fn': 'relu',
 'batch_size': 4,
 'channel': -1,
 'cond_label_size': None,
 'conditional': False,
 'data_dir': './data/',
 'dataset': 'toy',
 'device': device(type='cuda', index=0),
 'flip_toy_var_order': False,
 'generate': False,
 'hidden_size': 64,
 'input_dims': torch.Size([3, 64, 64]),
 'input_order': 'sequential',
 'input_size': 12288,
 'log_interval': 1000,
 'lr': 0.0001,
 'model': 'realnvp',
 'n_blocks': 64,
 'n_components': 1,
 'n_epochs': 50,
 'n_hidden': 1,
 'no_batch_norm': False,
 'no_cuda': False,
 'output_dir': './results/mafencoderFalse_channel-1_nblocks64/',
 'restore_file': None,
 'results_file': 'results.txt',
 'seed': 1,
 'start_epoch': 0,
 'train': True,
 'use_encoder': False}
RealNVP(
  (net): FlowSequential(
    (0): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (1): BatchNorm()
    (2): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (3): BatchNorm()
    (4): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (5): BatchNorm()
    (6): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (7): BatchNorm()
    (8): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (9): BatchNorm()
    (10): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (11): BatchNorm()
    (12): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (13): BatchNorm()
    (14): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (15): BatchNorm()
    (16): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (17): BatchNorm()
    (18): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (19): BatchNorm()
    (20): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (21): BatchNorm()
    (22): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (23): BatchNorm()
    (24): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (25): BatchNorm()
    (26): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (27): BatchNorm()
    (28): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (29): BatchNorm()
    (30): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (31): BatchNorm()
    (32): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (33): BatchNorm()
    (34): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (35): BatchNorm()
    (36): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (37): BatchNorm()
    (38): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (39): BatchNorm()
    (40): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (41): BatchNorm()
    (42): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (43): BatchNorm()
    (44): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (45): BatchNorm()
    (46): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (47): BatchNorm()
    (48): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (49): BatchNorm()
    (50): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (51): BatchNorm()
    (52): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (53): BatchNorm()
    (54): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (55): BatchNorm()
    (56): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (57): BatchNorm()
    (58): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (59): BatchNorm()
    (60): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (61): BatchNorm()
    (62): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (63): BatchNorm()
    (64): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (65): BatchNorm()
    (66): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (67): BatchNorm()
    (68): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (69): BatchNorm()
    (70): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (71): BatchNorm()
    (72): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (73): BatchNorm()
    (74): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (75): BatchNorm()
    (76): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (77): BatchNorm()
    (78): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (79): BatchNorm()
    (80): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (81): BatchNorm()
    (82): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (83): BatchNorm()
    (84): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (85): BatchNorm()
    (86): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (87): BatchNorm()
    (88): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (89): BatchNorm()
    (90): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (91): BatchNorm()
    (92): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (93): BatchNorm()
    (94): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (95): BatchNorm()
    (96): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (97): BatchNorm()
    (98): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (99): BatchNorm()
    (100): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (101): BatchNorm()
    (102): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (103): BatchNorm()
    (104): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (105): BatchNorm()
    (106): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (107): BatchNorm()
    (108): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (109): BatchNorm()
    (110): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (111): BatchNorm()
    (112): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (113): BatchNorm()
    (114): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (115): BatchNorm()
    (116): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (117): BatchNorm()
    (118): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (119): BatchNorm()
    (120): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (121): BatchNorm()
    (122): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (123): BatchNorm()
    (124): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (125): BatchNorm()
    (126): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (127): BatchNorm()
  )
)
Evaluate (epoch 0) -- logp(x) = -14233.260 +/- 78.796
Evaluate (epoch 0) -- logp(x) = -95852.734 +/- 4034.554
Evaluate (epoch 0) -- logp(x) = -126737.078 +/- 6495.576
Evaluate (epoch 0) -- logp(x) = -4137442.500 +/- 88550.219
Evaluate (epoch 0) -- logp(x) = -347987.406 +/- 18191.471
Evaluate (epoch 0) -- logp(x) = -19446.477 +/- 72.984
Evaluate (epoch 0) -- logp(x) = -66651.953 +/- 822.298
Evaluate (epoch 0) -- logp(x) = -19471.102 +/- 256.410
Evaluate (epoch 0) -- logp(x) = -427846.125 +/- 11659.573
Evaluate (epoch 0) -- logp(x) = -21075.203 +/- 29.892
Evaluate (epoch 1) -- logp(x) = -19088.727 +/- 75.089
Evaluate (epoch 1) -- logp(x) = -4693816.500 +/- 179077.203
Evaluate (epoch 1) -- logp(x) = -292434.656 +/- 22280.365
Evaluate (epoch 1) -- logp(x) = -14063788.000 +/- 277170.125
Evaluate (epoch 1) -- logp(x) = -441433.875 +/- 22467.150
Evaluate (epoch 1) -- logp(x) = -20524.352 +/- 45.743
Evaluate (epoch 1) -- logp(x) = -135652288.000 +/- 3379906.250
Evaluate (epoch 1) -- logp(x) = -1697101.250 +/- 328654.969
Evaluate (epoch 1) -- logp(x) = -361761184.000 +/- 3863054.250
Evaluate (epoch 1) -- logp(x) = -135716.281 +/- 5711.109
Evaluate (epoch 2) -- logp(x) = -28832.061 +/- 41.053
Evaluate (epoch 2) -- logp(x) = -631298752.000 +/- 17419924.000
Evaluate (epoch 2) -- logp(x) = -9173300.000 +/- 1809547.625
Evaluate (epoch 2) -- logp(x) = -574020416.000 +/- 14273172.000
Evaluate (epoch 2) -- logp(x) = -29681.768 +/- 4.991
Evaluate (epoch 3) -- logp(x) = -39935.898 +/- 42.204
Evaluate (epoch 3) -- logp(x) = -34986237952.000 +/- 551839104.000
Evaluate (epoch 3) -- logp(x) = -1813400448.000 +/- 158180336.000
Evaluate (epoch 3) -- logp(x) = -42293141504.000 +/- 627202368.000
Evaluate (epoch 3) -- logp(x) = -216293648.000 +/- 6792292.500
Evaluate (epoch 2) -- logp(x) = -48944.379 +/- 8.039
Evaluate (epoch 2) -- logp(x) = -4075944.250 +/- 809240.125
Evaluate (epoch 2) -- logp(x) = -49760.090 +/- 11.323
Evaluate (epoch 2) -- logp(x) = -9466340352.000 +/- 1124575232.000
Evaluate (epoch 2) -- logp(x) = -49502.633 +/- 0.453
Evaluate (epoch 4) -- logp(x) = -38086.547 +/- 34.144
Evaluate (epoch 4) -- logp(x) = -4750835712000.000 +/- 190485348352.000
Evaluate (epoch 4) -- logp(x) = -38336.242 +/- 15.088
Evaluate (epoch 4) -- logp(x) = -5258259464192.000 +/- 114248138752.000
Evaluate (epoch 4) -- logp(x) = -38070.504 +/- 2.416
Evaluate (epoch 3) -- logp(x) = -68223.344 +/- 8.907
Evaluate (epoch 3) -- logp(x) = -13772877987840.000 +/- 950997483520.000
Evaluate (epoch 3) -- logp(x) = -68935.250 +/- 14.031
Evaluate (epoch 3) -- logp(x) = -49818994999296.000 +/- 3506445484032.000
Evaluate (epoch 3) -- logp(x) = -68721.047 +/- 3.880
Evaluate (epoch 5) -- logp(x) = -51178.898 +/- 18.826
Evaluate (epoch 5) -- logp(x) = -89074827264.000 +/- 1824197888.000
Evaluate (epoch 5) -- logp(x) = -544693056.000 +/- 107855200.000
Evaluate (epoch 5) -- logp(x) = -110239924224.000 +/- 1472970496.000
Evaluate (epoch 5) -- logp(x) = -50699.836 +/- 19.140
Evaluate (epoch 4) -- logp(x) = -72778.406 +/- 14.101
Evaluate (epoch 4) -- logp(x) = -138572925501440.000 +/- 8128032669696.000
Evaluate (epoch 4) -- logp(x) = -73672.047 +/- 13.716
Evaluate (epoch 4) -- logp(x) = -5141346973122560.000 +/- 430238953635840.000
Evaluate (epoch 4) -- logp(x) = -73492.297 +/- 2.363
Evaluate (epoch 6) -- logp(x) = -38830.312 +/- 39.034
Evaluate (epoch 6) -- logp(x) = -648934785024.000 +/- 20212023296.000
Evaluate (epoch 6) -- logp(x) = -1262976256.000 +/- 250099024.000
Evaluate (epoch 6) -- logp(x) = -595690782720.000 +/- 17116679168.000
Evaluate (epoch 6) -- logp(x) = -37770.984 +/- 41.184
Evaluate (epoch 5) -- logp(x) = -61538.629 +/- 22.874
Evaluate (epoch 5) -- logp(x) = -232029536256.000 +/- 46580903936.000
Evaluate (epoch 5) -- logp(x) = -62089.703 +/- 23.644
Evaluate (epoch 7) -- logp(x) = -45645.016 +/- 28.231
Evaluate (epoch 7) -- logp(x) = -81659927986176.000 +/- 3119351595008.000
Evaluate (epoch 5) -- logp(x) = -1545145220595712.000 +/- 90010661421056.000
Evaluate (epoch 7) -- logp(x) = -836884889600.000 +/- 165727911936.000
Evaluate (epoch 5) -- logp(x) = -61814.414 +/- 2.869
Evaluate (epoch 7) -- logp(x) = -130422100983808.000 +/- 4390198181888.000
Evaluate (epoch 7) -- logp(x) = -45269.715 +/- 24.750
Evaluate (epoch 8) -- logp(x) = -64254.980 +/- 49.084
Evaluate (epoch 8) -- logp(x) = -386161115136.000 +/- 18632673280.000
Evaluate (epoch 8) -- logp(x) = -63440.055 +/- 247.518
Evaluate (epoch 8) -- logp(x) = -613029904384.000 +/- 14865240064.000
Evaluate (epoch 8) -- logp(x) = -64328.004 +/- 3.322
Evaluate (epoch 6) -- logp(x) = -92228.633 +/- 14.269
Evaluate (epoch 6) -- logp(x) = -4981259763712.000 +/- 886407495680.000
Evaluate (epoch 6) -- logp(x) = -92593.867 +/- 16.245
Evaluate (epoch 6) -- logp(x) = -28167553155072.000 +/- 2858022600704.000
Evaluate (epoch 6) -- logp(x) = -92242.203 +/- 2.582
Evaluate (epoch 9) -- logp(x) = -48794.664 +/- 39.890
Evaluate (epoch 9) -- logp(x) = -1866220699648.000 +/- 71771004928.000
Evaluate (epoch 9) -- logp(x) = -813116800.000 +/- 161011488.000
Evaluate (epoch 9) -- logp(x) = -2168406540288.000 +/- 53257781248.000
Evaluate (epoch 9) -- logp(x) = -48367.410 +/- 0.824
Evaluate (epoch 7) -- logp(x) = -100113.953 +/- 20.431
Evaluate (epoch 7) -- logp(x) = -99992.984 +/- 64.628
Evaluate (epoch 7) -- logp(x) = -100162.609 +/- 12.220
Evaluate (epoch 7) -- logp(x) = -1396587.125 +/- 258805.312
Evaluate (epoch 7) -- logp(x) = -99879.078 +/- 1.347
Evaluate (epoch 10) -- logp(x) = -42883.035 +/- 25.242
Evaluate (epoch 10) -- logp(x) = -2362530332672.000 +/- 90991484928.000
Evaluate (epoch 10) -- logp(x) = -43452.000 +/- 121.874
Evaluate (epoch 10) -- logp(x) = -2503726596096.000 +/- 55748706304.000
Evaluate (epoch 10) -- logp(x) = -42885.336 +/- 9.000
Evaluate (epoch 8) -- logp(x) = -64553.418 +/- 15.977
Evaluate (epoch 8) -- logp(x) = -64670.438 +/- 32.045
Evaluate (epoch 8) -- logp(x) = -64662.973 +/- 9.931
Evaluate (epoch 8) -- logp(x) = -63422.578 +/- 84.662
Evaluate (epoch 8) -- logp(x) = -64426.734 +/- 1.004
Evaluate (epoch 11) -- logp(x) = -44064.871 +/- 34.989
Evaluate (epoch 11) -- logp(x) = -12103479984128.000 +/- 459873386496.000
Evaluate (epoch 11) -- logp(x) = -14522398720.000 +/- 2875854848.000
Evaluate (epoch 11) -- logp(x) = -11193129369600.000 +/- 510472257536.000
Evaluate (epoch 11) -- logp(x) = -43622.285 +/- 13.170
Evaluate (epoch 12) -- logp(x) = -128263380992.000 +/- 25493823488.000
Evaluate (epoch 12) -- logp(x) = -60069.289 +/- 21.744
Evaluate (epoch 12) -- logp(x) = -58813.164 +/- 88.700
Evaluate (epoch 12) -- logp(x) = -60732.930 +/- 36.203
Evaluate (epoch 12) -- logp(x) = -58027.977 +/- 9.351
Evaluate (epoch 9) -- logp(x) = -86880.266 +/- 37.329
Evaluate (epoch 9) -- logp(x) = -88028.281 +/- 43.485
Evaluate (epoch 9) -- logp(x) = -87176.820 +/- 23.012
Evaluate (epoch 9) -- logp(x) = -87872.000 +/- 38.848
Evaluate (epoch 9) -- logp(x) = -86678.438 +/- 6.418
Evaluate (epoch 13) -- logp(x) = -50488.945 +/- 23.780
Evaluate (epoch 13) -- logp(x) = -11873682456576.000 +/- 778756685824.000
Evaluate (epoch 13) -- logp(x) = -663537188864.000 +/- 131377250304.000
Evaluate (epoch 13) -- logp(x) = -7530263085056.000 +/- 894087331840.000
Evaluate (epoch 13) -- logp(x) = -44857.734 +/- 84.091
Evaluate (epoch 10) -- logp(x) = -64930.816 +/- 21.634
Evaluate (epoch 10) -- logp(x) = -65933.836 +/- 23.701
Evaluate (epoch 10) -- logp(x) = -65196.047 +/- 17.197
Evaluate (epoch 10) -- logp(x) = -66251.562 +/- 26.963
Evaluate (epoch 10) -- logp(x) = -64930.578 +/- 3.012
Evaluate (epoch 14) -- logp(x) = -45558.719 +/- 30.503
Evaluate (epoch 14) -- logp(x) = -15623438467072.000 +/- 1580172050432.000
Evaluate (epoch 14) -- logp(x) = -15855360000.000 +/- 3132137472.000
Evaluate (epoch 14) -- logp(x) = -6490234028032.000 +/- 563799261184.000
Evaluate (epoch 14) -- logp(x) = -45729.680 +/- 6.334
Evaluate (epoch 11) -- logp(x) = -70340.641 +/- 17.360
Evaluate (epoch 11) -- logp(x) = -71106.344 +/- 22.562
Evaluate (epoch 11) -- logp(x) = -70579.773 +/- 12.468
Evaluate (epoch 11) -- logp(x) = -71155.281 +/- 19.169
Evaluate (epoch 11) -- logp(x) = -70374.125 +/- 2.125
Evaluate (epoch 15) -- logp(x) = -49084.633 +/- 20.719
Evaluate (epoch 15) -- logp(x) = -58149.000 +/- 850.106
Evaluate (epoch 15) -- logp(x) = -4302258831360.000 +/- 851974225920.000
Evaluate (epoch 15) -- logp(x) = -1499573604319232.000 +/- 183104136282112.000
Evaluate (epoch 15) -- logp(x) = -287758178320384.000 +/- 8539622342656.000
Evaluate (epoch 16) -- logp(x) = -609097351168.000 +/- 39557824512.000
Evaluate (epoch 16) -- logp(x) = -143444.281 +/- 5169.705
Evaluate (epoch 12) -- logp(x) = -142666.547 +/- 25.482
Evaluate (epoch 16) -- logp(x) = -6354930176.000 +/- 1258450560.000
Evaluate (epoch 12) -- logp(x) = -179185.969 +/- 7572.319
Evaluate (epoch 12) -- logp(x) = -142666.016 +/- 13.042
Evaluate (epoch 16) -- logp(x) = -152912.219 +/- 6938.012
Evaluate (epoch 16) -- logp(x) = -58275.219 +/- 325.550
Evaluate (epoch 12) -- logp(x) = -7009792000.000 +/- 873720384.000
Evaluate (epoch 12) -- logp(x) = -142440.172 +/- 7.665
Evaluate (epoch 17) -- logp(x) = -52031.234 +/- 267.784
Evaluate (epoch 17) -- logp(x) = -23335972.000 +/- 1742631.250
Evaluate (epoch 17) -- logp(x) = -79681.359 +/- 3627.579
Evaluate (epoch 17) -- logp(x) = -20130548.000 +/- 1706615.250
Evaluate (epoch 17) -- logp(x) = -59808.309 +/- 5.935
Evaluate (epoch 13) -- logp(x) = -52491.164 +/- 53.638
Evaluate (epoch 13) -- logp(x) = -60049.258 +/- 778.303
Evaluate (epoch 13) -- logp(x) = -53048.789 +/- 53.476
Evaluate (epoch 13) -- logp(x) = -4896336117760.000 +/- 747508334592.000
Evaluate (epoch 13) -- logp(x) = -52523.156 +/- 10.200
Evaluate (epoch 18) -- logp(x) = -62208.285 +/- 16.145
Evaluate (epoch 18) -- logp(x) = -789394496.000 +/- 146016944.000
Evaluate (epoch 18) -- logp(x) = -62447.547 +/- 37.381
Evaluate (epoch 18) -- logp(x) = -2379596288.000 +/- 338676704.000
Evaluate (epoch 18) -- logp(x) = -60606.062 +/- 7.239
Evaluate (epoch 14) -- logp(x) = -51563.836 +/- 42.228
Evaluate (epoch 14) -- logp(x) = -10201756336128.000 +/- 2048046006272.000
Evaluate (epoch 14) -- logp(x) = -51831.055 +/- 25.201
Evaluate (epoch 14) -- logp(x) = -278630114525184.000 +/- 44453112840192.000
Evaluate (epoch 14) -- logp(x) = -51483.637 +/- 4.785
Evaluate (epoch 19) -- logp(x) = -57894.332 +/- 29.206
Evaluate (epoch 19) -- logp(x) = -10749530112.000 +/- 1971471360.000
Evaluate (epoch 19) -- logp(x) = -59185.234 +/- 203.923
Evaluate (epoch 19) -- logp(x) = -16981009408.000 +/- 2533041920.000
Evaluate (epoch 19) -- logp(x) = -56781.336 +/- 28.918
Evaluate (epoch 15) -- logp(x) = -99711.906 +/- 34.772
Evaluate (epoch 15) -- logp(x) = -98396.078 +/- 458.096
Evaluate (epoch 15) -- logp(x) = -99922.984 +/- 13.674
Evaluate (epoch 15) -- logp(x) = -5603378688.000 +/- 1077581568.000
Evaluate (epoch 15) -- logp(x) = -99675.562 +/- 5.092
Evaluate (epoch 20) -- logp(x) = -55283.891 +/- 88.966
Evaluate (epoch 20) -- logp(x) = -751268.500 +/- 79124.141
Evaluate (epoch 20) -- logp(x) = -58095.625 +/- 359.109
Evaluate (epoch 20) -- logp(x) = -823091.875 +/- 53118.930
Evaluate (epoch 20) -- logp(x) = -56185.656 +/- 24.063
Evaluate (epoch 21) -- logp(x) = -49614.457 +/- 34.862
Evaluate (epoch 21) -- logp(x) = -5621677.500 +/- 990621.500
Evaluate (epoch 21) -- logp(x) = -50764.965 +/- 255.997
Evaluate (epoch 21) -- logp(x) = -6706307.000 +/- 919103.000
Evaluate (epoch 21) -- logp(x) = -48089.109 +/- 68.797
Evaluate (epoch 16) -- logp(x) = -73180.461 +/- 393.978
Evaluate (epoch 16) -- logp(x) = -3624472.000 +/- 345275.250
Evaluate (epoch 16) -- logp(x) = -301804.750 +/- 27214.373
Evaluate (epoch 16) -- logp(x) = -73042.492 +/- 111.130
Evaluate (epoch 16) -- logp(x) = -69948.391 +/- 105.891
Evaluate (epoch 22) -- logp(x) = -47866.660 +/- 113.671
Evaluate (epoch 22) -- logp(x) = -175923.312 +/- 14015.790
Evaluate (epoch 22) -- logp(x) = -49147.059 +/- 149.766
Evaluate (epoch 22) -- logp(x) = -303500.125 +/- 26795.479
Evaluate (epoch 22) -- logp(x) = -46854.945 +/- 1.466
Evaluate (epoch 17) -- logp(x) = -80917.781 +/- 218.202
Evaluate (epoch 17) -- logp(x) = -85081.773 +/- 63.864
Evaluate (epoch 17) -- logp(x) = -82448.828 +/- 89.303
Evaluate (epoch 17) -- logp(x) = -87092.969 +/- 157.258
Evaluate (epoch 17) -- logp(x) = -81390.562 +/- 9.800
Evaluate (epoch 23) -- logp(x) = -52442.645 +/- 73.399
Evaluate (epoch 23) -- logp(x) = -104946.797 +/- 5294.562
Evaluate (epoch 23) -- logp(x) = -53955.516 +/- 122.850
Evaluate (epoch 23) -- logp(x) = -267678.375 +/- 32084.031
Evaluate (epoch 23) -- logp(x) = -52372.480 +/- 21.086
Evaluate (epoch 18) -- logp(x) = -68182.188 +/- 49.645
Evaluate (epoch 18) -- logp(x) = -73433.844 +/- 1132.371
Evaluate (epoch 18) -- logp(x) = -68066.000 +/- 23.380
Evaluate (epoch 18) -- logp(x) = -152556.453 +/- 10704.604
Evaluate (epoch 18) -- logp(x) = -68044.156 +/- 10.337
Evaluate (epoch 24) -- logp(x) = -49768.938 +/- 30.434
Evaluate (epoch 24) -- logp(x) = -73997184.000 +/- 14359664.000
Evaluate (epoch 24) -- logp(x) = -51004.996 +/- 193.051
Evaluate (epoch 24) -- logp(x) = -293090016.000 +/- 44198748.000
Evaluate (epoch 24) -- logp(x) = -139569774592.000 +/- 7550397440.000
Evaluate (epoch 25) -- logp(x) = -54657.383 +/- 73.009
Evaluate (epoch 19) -- logp(x) = -65568.398 +/- 52.605
Evaluate (epoch 25) -- logp(x) = -3049761792.000 +/- 302628384.000
Evaluate (epoch 19) -- logp(x) = -2304597.500 +/- 169184.422
Evaluate (epoch 25) -- logp(x) = -94958056.000 +/- 18793486.000
Evaluate (epoch 19) -- logp(x) = -65688.625 +/- 22.103
Evaluate (epoch 25) -- logp(x) = -7769512960.000 +/- 473457568.000
Evaluate (epoch 19) -- logp(x) = -3992416768.000 +/- 353914944.000
Evaluate (epoch 25) -- logp(x) = -6856203304960.000 +/- 73044893696.000
Evaluate (epoch 19) -- logp(x) = -65291.441 +/- 10.363
Evaluate (epoch 26) -- logp(x) = -52566.617 +/- 96.554
Evaluate (epoch 26) -- logp(x) = -2018697088.000 +/- 254741696.000
Evaluate (epoch 26) -- logp(x) = -46582451929088.000 +/- 7469675315200.000
Evaluate (epoch 26) -- logp(x) = -3116485632.000 +/- 438109632.000
Evaluate (epoch 26) -- logp(x) = -1989549752320.000 +/- 54129692672.000
Evaluate (epoch 20) -- logp(x) = -80075.922 +/- 36.296
Evaluate (epoch 20) -- logp(x) = -494222.406 +/- 82922.797
Evaluate (epoch 20) -- logp(x) = -80725.859 +/- 16.318
Evaluate (epoch 20) -- logp(x) = -260972736.000 +/- 9037164.000
Evaluate (epoch 20) -- logp(x) = -80194.359 +/- 5.232
Evaluate (epoch 27) -- logp(x) = -53669.090 +/- 122.030
Evaluate (epoch 27) -- logp(x) = -298337.375 +/- 34568.172
Evaluate (epoch 27) -- logp(x) = -54316.004 +/- 187.154
Evaluate (epoch 27) -- logp(x) = -414899.938 +/- 55067.086
Evaluate (epoch 27) -- logp(x) = -53543.043 +/- 43.473
Evaluate (epoch 21) -- logp(x) = -120653.078 +/- 103.875
Evaluate (epoch 21) -- logp(x) = -947879424.000 +/- 12756137.000
Evaluate (epoch 21) -- logp(x) = -1875473.375 +/- 347870.156
Evaluate (epoch 21) -- logp(x) = -1887123584.000 +/- 38616980.000
Evaluate (epoch 21) -- logp(x) = -118941.156 +/- 11.545
Evaluate (epoch 28) -- logp(x) = -54503.730 +/- 68.269
Evaluate (epoch 28) -- logp(x) = -210435407872.000 +/- 22599227392.000
Evaluate (epoch 28) -- logp(x) = -13969073.000 +/- 2755324.500
Evaluate (epoch 28) -- logp(x) = -254538727424.000 +/- 47799586816.000
Evaluate (epoch 28) -- logp(x) = -54570.695 +/- 16.592
Evaluate (epoch 22) -- logp(x) = -71867.477 +/- 33.359
Evaluate (epoch 22) -- logp(x) = -3138099085312.000 +/- 74061471744.000
Evaluate (epoch 22) -- logp(x) = -74601.727 +/- 357.041
Evaluate (epoch 22) -- logp(x) = -7628195364864.000 +/- 386538536960.000
Evaluate (epoch 22) -- logp(x) = -71686.703 +/- 6.381
Evaluate (epoch 29) -- logp(x) = -77352.406 +/- 43.233
Evaluate (epoch 29) -- logp(x) = -106864885760.000 +/- 8491059200.000
Evaluate (epoch 29) -- logp(x) = -77758.375 +/- 92.375
Evaluate (epoch 29) -- logp(x) = -81475239936.000 +/- 9721465856.000
Evaluate (epoch 29) -- logp(x) = -78218.680 +/- 8.893
Evaluate (epoch 23) -- logp(x) = -70675.695 +/- 36.518
Evaluate (epoch 23) -- logp(x) = -5296108339200.000 +/- 205162184704.000
Evaluate (epoch 23) -- logp(x) = -33194895360.000 +/- 6573554176.000
Evaluate (epoch 23) -- logp(x) = -10317742473216.000 +/- 458687807488.000
Evaluate (epoch 23) -- logp(x) = -71093.078 +/- 26.410
Evaluate (epoch 30) -- logp(x) = -53772.098 +/- 55.093
Evaluate (epoch 30) -- logp(x) = -10407103168512.000 +/- 492631097344.000
Evaluate (epoch 30) -- logp(x) = -414446223360.000 +/- 82072576000.000
Evaluate (epoch 30) -- logp(x) = -57968871604224.000 +/- 4004491034624.000
Evaluate (epoch 30) -- logp(x) = -55807.008 +/- 5.027
Evaluate (epoch 31) -- logp(x) = -100084.172 +/- 156.252
Evaluate (epoch 31) -- logp(x) = -5215449.000 +/- 306192.562
Evaluate (epoch 31) -- logp(x) = -473392.562 +/- 73066.000
Evaluate (epoch 31) -- logp(x) = -9048688.000 +/- 576453.500
Evaluate (epoch 31) -- logp(x) = -104841.703 +/- 16.583
Evaluate (epoch 24) -- logp(x) = -61331.320 +/- 23.930
Evaluate (epoch 24) -- logp(x) = -43958298214400.000 +/- 1712414916608.000
Evaluate (epoch 24) -- logp(x) = -516261.906 +/- 90047.242
Evaluate (epoch 24) -- logp(x) = -173279381815296.000 +/- 2780578447360.000
Evaluate (epoch 24) -- logp(x) = -61697.316 +/- 11.472
Evaluate (epoch 32) -- logp(x) = -51587.836 +/- 48.896
Evaluate (epoch 32) -- logp(x) = -516019328.000 +/- 42322444.000
Evaluate (epoch 32) -- logp(x) = -131144.281 +/- 15824.662
Evaluate (epoch 32) -- logp(x) = -642152128.000 +/- 49200576.000
Evaluate (epoch 32) -- logp(x) = -52295.531 +/- 23.429
Evaluate (epoch 25) -- logp(x) = -71932.984 +/- 11.290
Evaluate (epoch 25) -- logp(x) = -23924159021056.000 +/- 1000363065344.000
Evaluate (epoch 25) -- logp(x) = -40114827264.000 +/- 7943906816.000
Evaluate (epoch 25) -- logp(x) = -114352749281280.000 +/- 2660134813696.000
Evaluate (epoch 25) -- logp(x) = -71795.844 +/- 6.853
Evaluate (epoch 33) -- logp(x) = -50828.312 +/- 18.810
Evaluate (epoch 33) -- logp(x) = -464854144.000 +/- 41582924.000
Evaluate (epoch 33) -- logp(x) = -12199863.000 +/- 2405513.750
Evaluate (epoch 33) -- logp(x) = -655848384.000 +/- 103117656.000
Evaluate (epoch 33) -- logp(x) = -50182.816 +/- 190.846
Evaluate (epoch 26) -- logp(x) = -62963.363 +/- 55.086
Evaluate (epoch 26) -- logp(x) = -115632985079808.000 +/- 7760981786624.000
Evaluate (epoch 26) -- logp(x) = -65139.777 +/- 444.582
Evaluate (epoch 26) -- logp(x) = -231235603398656.000 +/- 13015893147648.000
Evaluate (epoch 26) -- logp(x) = -62472.121 +/- 10.041
Evaluate (epoch 34) -- logp(x) = -55631.855 +/- 52.479
Evaluate (epoch 34) -- logp(x) = -405965438976.000 +/- 16876007424.000
Evaluate (epoch 34) -- logp(x) = -14946072576.000 +/- 1758120064.000
Evaluate (epoch 34) -- logp(x) = -487130529792.000 +/- 24529737728.000
Evaluate (epoch 34) -- logp(x) = -1987728128.000 +/- 142183632.000
Evaluate (epoch 35) -- logp(x) = -63654.680 +/- 34.816
Evaluate (epoch 35) -- logp(x) = -22825748480.000 +/- 294730464.000
Evaluate (epoch 35) -- logp(x) = -3791781632.000 +/- 750871424.000
Evaluate (epoch 35) -- logp(x) = -18639159296.000 +/- 2371767296.000
Evaluate (epoch 35) -- logp(x) = -60833.891 +/- 313.477
Evaluate (epoch 27) -- logp(x) = -57736.961 +/- 60.365
Evaluate (epoch 27) -- logp(x) = -1923614383276032.000 +/- 209078823419904.000
Evaluate (epoch 27) -- logp(x) = -12582060.000 +/- 2480173.250
Evaluate (epoch 27) -- logp(x) = -2681970316279808.000 +/- 129746021122048.000
Evaluate (epoch 27) -- logp(x) = -57435.727 +/- 14.973
Evaluate (epoch 36) -- logp(x) = -53838.477 +/- 59.912
Evaluate (epoch 36) -- logp(x) = -30783379456.000 +/- 1486949760.000
Evaluate (epoch 36) -- logp(x) = -2216352256.000 +/- 347028224.000
Evaluate (epoch 36) -- logp(x) = -19129759744.000 +/- 1638663552.000
Evaluate (epoch 36) -- logp(x) = -1114590848.000 +/- 36561332.000
Evaluate (epoch 28) -- logp(x) = -73544.961 +/- 31.895
Evaluate (epoch 28) -- logp(x) = -728244454162432.000 +/- 30568582479872.000
Evaluate (epoch 28) -- logp(x) = -75005.688 +/- 245.077
Evaluate (epoch 28) -- logp(x) = -3211586491645952.000 +/- 179900208119808.000
Evaluate (epoch 28) -- logp(x) = -73306.859 +/- 5.516
Evaluate (epoch 37) -- logp(x) = -74949.633 +/- 75.084
Evaluate (epoch 37) -- logp(x) = -3244894208.000 +/- 130439512.000
Evaluate (epoch 37) -- logp(x) = -6589461504.000 +/- 792682176.000
Evaluate (epoch 37) -- logp(x) = -4028356608.000 +/- 102085512.000
Evaluate (epoch 37) -- logp(x) = -8492248.000 +/- 132944.812
Evaluate (epoch 29) -- logp(x) = -52222.109 +/- 37.693
Evaluate (epoch 29) -- logp(x) = -12521323242717184.000 +/- 1327420443459584.000
Evaluate (epoch 29) -- logp(x) = -3533138944.000 +/- 699655424.000
Evaluate (epoch 29) -- logp(x) = -29153976012242944.000 +/- 618919886848000.000
Evaluate (epoch 29) -- logp(x) = -52472.824 +/- 4.785
Evaluate (epoch 38) -- logp(x) = -39996.422 +/- 119.429
Evaluate (epoch 38) -- logp(x) = -340781888.000 +/- 13820339.000
Evaluate (epoch 38) -- logp(x) = -18876914.000 +/- 2786537.250
Evaluate (epoch 38) -- logp(x) = -322652448.000 +/- 17260268.000
Evaluate (epoch 38) -- logp(x) = -15128271.000 +/- 293351.312
Evaluate (epoch 30) -- logp(x) = -61486.863 +/- 21.691
Evaluate (epoch 30) -- logp(x) = -13786154604167168.000 +/- 2425085369516032.000
Evaluate (epoch 30) -- logp(x) = -62881.016 +/- 160.429
Evaluate (epoch 30) -- logp(x) = -19482633079619584.000 +/- 473864211529728.000
Evaluate (epoch 30) -- logp(x) = -61921.988 +/- 17.905
Evaluate (epoch 39) -- logp(x) = -44228.746 +/- 64.770
Evaluate (epoch 39) -- logp(x) = -216682987520.000 +/- 7642531328.000
Evaluate (epoch 39) -- logp(x) = -4386127872.000 +/- 825247040.000
Evaluate (epoch 39) -- logp(x) = -281531187200.000 +/- 4830894592.000
Evaluate (epoch 39) -- logp(x) = -205486944.000 +/- 7961779.000
Evaluate (epoch 40) -- logp(x) = -46869.051 +/- 51.989
Evaluate (epoch 40) -- logp(x) = -58519588864.000 +/- 1948071552.000
Evaluate (epoch 40) -- logp(x) = -836083072.000 +/- 158905888.000
Evaluate (epoch 40) -- logp(x) = -93639606272.000 +/- 5816431104.000
Evaluate (epoch 40) -- logp(x) = -50348.359 +/- 22.824
Evaluate (epoch 31) -- logp(x) = -55644.531 +/- 24.997
Evaluate (epoch 31) -- logp(x) = -88910324152926208.000 +/- 7342362019758080.000
Evaluate (epoch 31) -- logp(x) = -56926.680 +/- 132.564
Evaluate (epoch 31) -- logp(x) = -307865729678442496.000 +/- 10542203386462208.000
Evaluate (epoch 31) -- logp(x) = -56247.301 +/- 15.771
Evaluate (epoch 41) -- logp(x) = -72652.000 +/- 37.380
Evaluate (epoch 41) -- logp(x) = -151367488.000 +/- 5830656.000
Evaluate (epoch 41) -- logp(x) = -90912.500 +/- 3398.725
Evaluate (epoch 41) -- logp(x) = -221989664.000 +/- 3641792.500
Evaluate (epoch 41) -- logp(x) = -73860.711 +/- 12.172
Evaluate (epoch 32) -- logp(x) = -70423.352 +/- 21.536
Evaluate (epoch 32) -- logp(x) = -166340200723644416.000 +/- 12291106553200640.000
Evaluate (epoch 32) -- logp(x) = -71265.648 +/- 101.301
Evaluate (epoch 32) -- logp(x) = -820686474088284160.000 +/- 39547195573141504.000
Evaluate (epoch 32) -- logp(x) = -71024.164 +/- 6.086
Evaluate (epoch 42) -- logp(x) = -49670.969 +/- 36.517
Evaluate (epoch 42) -- logp(x) = -44589408256.000 +/- 2111306368.000
Evaluate (epoch 42) -- logp(x) = -1602508800.000 +/- 230534704.000
Evaluate (epoch 42) -- logp(x) = -53582635008.000 +/- 4532615680.000
Evaluate (epoch 42) -- logp(x) = -702058432.000 +/- 107480144.000
Evaluate (epoch 33) -- logp(x) = -50221.484 +/- 40.166
Evaluate (epoch 33) -- logp(x) = -33826574012776448.000 +/- 1277216168083456.000
Evaluate (epoch 33) -- logp(x) = -68992.828 +/- 3638.841
Evaluate (epoch 33) -- logp(x) = -48265425133240320.000 +/- 2165077578874880.000
Evaluate (epoch 33) -- logp(x) = -51087.141 +/- 9.425
Evaluate (epoch 43) -- logp(x) = -54213.125 +/- 29.799
Evaluate (epoch 43) -- logp(x) = -18671902720.000 +/- 808195072.000
Evaluate (epoch 43) -- logp(x) = -144088704.000 +/- 28522740.000
Evaluate (epoch 43) -- logp(x) = -35221192704.000 +/- 722688384.000
Evaluate (epoch 43) -- logp(x) = -56293.820 +/- 25.987
Evaluate (epoch 34) -- logp(x) = -75683.055 +/- 19.474
Evaluate (epoch 34) -- logp(x) = -3711478808969216.000 +/- 267450968965120.000
Evaluate (epoch 34) -- logp(x) = -75824.812 +/- 65.568
Evaluate (epoch 34) -- logp(x) = -4801040071262208.000 +/- 159754160701440.000
Evaluate (epoch 34) -- logp(x) = -76086.039 +/- 6.259
Evaluate (epoch 44) -- logp(x) = -47961.852 +/- 22.685
Evaluate (epoch 44) -- logp(x) = -135573200896.000 +/- 4970395648.000
Evaluate (epoch 44) -- logp(x) = -828767808.000 +/- 164101840.000
Evaluate (epoch 44) -- logp(x) = -203328946176.000 +/- 3740820992.000
Evaluate (epoch 44) -- logp(x) = -51736.555 +/- 5.469
Evaluate (epoch 45) -- logp(x) = -50168.961 +/- 22.179
Evaluate (epoch 45) -- logp(x) = -1766628524032.000 +/- 85352652800.000
Evaluate (epoch 45) -- logp(x) = -16965986304.000 +/- 2018073088.000
Evaluate (epoch 45) -- logp(x) = -2209290125312.000 +/- 51909025792.000
Evaluate (epoch 45) -- logp(x) = -52004.164 +/- 38.942
Evaluate (epoch 35) -- logp(x) = -87967.805 +/- 19.166
Evaluate (epoch 35) -- logp(x) = -19885717907832832.000 +/- 660928961970176.000
Evaluate (epoch 35) -- logp(x) = -88630.891 +/- 22.643
Evaluate (epoch 35) -- logp(x) = -17406009949552640.000 +/- 1418153909288960.000
Evaluate (epoch 35) -- logp(x) = -88661.523 +/- 8.263
Evaluate (epoch 46) -- logp(x) = -49827.676 +/- 32.825
Evaluate (epoch 46) -- logp(x) = -138780737536.000 +/- 5109817856.000
Evaluate (epoch 46) -- logp(x) = -223069306880.000 +/- 43336216576.000
Evaluate (epoch 46) -- logp(x) = -264731803648.000 +/- 1281342848.000
Evaluate (epoch 46) -- logp(x) = -52999.332 +/- 501.904
Evaluate (epoch 36) -- logp(x) = -51066.211 +/- 62.720
Evaluate (epoch 36) -- logp(x) = -45017792512.000 +/- 4028755968.000
Evaluate (epoch 36) -- logp(x) = -53824.309 +/- 394.793
Evaluate (epoch 36) -- logp(x) = -39346606080.000 +/- 7449111552.000
Evaluate (epoch 36) -- logp(x) = -51449.699 +/- 7.461
Evaluate (epoch 47) -- logp(x) = -90323.906 +/- 26.292
Evaluate (epoch 47) -- logp(x) = -423109248.000 +/- 24915940.000
Evaluate (epoch 47) -- logp(x) = -113127.125 +/- 4310.743
Evaluate (epoch 47) -- logp(x) = -826643200.000 +/- 13374570.000
Evaluate (epoch 47) -- logp(x) = -91220.625 +/- 11.993
Evaluate (epoch 37) -- logp(x) = -61873.344 +/- 20.637
Evaluate (epoch 37) -- logp(x) = -1349526749184.000 +/- 117886967808.000
Evaluate (epoch 37) -- logp(x) = -64030.453 +/- 75.363
Evaluate (epoch 37) -- logp(x) = -10690504704.000 +/- 2039856640.000
Evaluate (epoch 37) -- logp(x) = -63183.516 +/- 6.533
Evaluate (epoch 48) -- logp(x) = -51020.262 +/- 15.773
Evaluate (epoch 48) -- logp(x) = -35879276544.000 +/- 2848555264.000
Evaluate (epoch 48) -- logp(x) = -348155776.000 +/- 66310392.000
Evaluate (epoch 48) -- logp(x) = -87525670912.000 +/- 4906475008.000
Evaluate (epoch 48) -- logp(x) = -53922.582 +/- 43.121
Evaluate (epoch 38) -- logp(x) = -1991450034176.000 +/- 158930894848.000
Evaluate (epoch 38) -- logp(x) = -84064.984 +/- 13.507
Evaluate (epoch 38) -- logp(x) = -79303.336 +/- 865.882
Evaluate (epoch 38) -- logp(x) = -84378.461 +/- 30.243
Evaluate (epoch 38) -- logp(x) = -396392202240.000 +/- 7514509824.000
Evaluate (epoch 49) -- logp(x) = -50673.410 +/- 23.494
Evaluate (epoch 49) -- logp(x) = -1193507553280.000 +/- 70682042368.000
Evaluate (epoch 49) -- logp(x) = -1203192448.000 +/- 238257200.000
Evaluate (epoch 49) -- logp(x) = -2438582763520.000 +/- 151781261312.000
Evaluate (epoch 49) -- logp(x) = -52126.098 +/- 17.824
Evaluate (epoch 39) -- logp(x) = -16303582609408.000 +/- 1130582245376.000
Evaluate (epoch 39) -- logp(x) = -66157.938 +/- 27.870
Evaluate (epoch 39) -- logp(x) = -62795.547 +/- 148.014
Evaluate (epoch 39) -- logp(x) = -68625.125 +/- 20.450
Evaluate (epoch 39) -- logp(x) = -64611.996 +/- 33.497
Evaluate (epoch 40) -- logp(x) = -5307127824384.000 +/- 1054853300224.000
Evaluate (epoch 40) -- logp(x) = -72195.594 +/- 11.524
Evaluate (epoch 40) -- logp(x) = -71158.141 +/- 44.904
Evaluate (epoch 40) -- logp(x) = -73897.242 +/- 11.704
Evaluate (epoch 40) -- logp(x) = -70854.164 +/- 9.517
Evaluate (epoch 41) -- logp(x) = -260410343424.000 +/- 51752890368.000
Evaluate (epoch 41) -- logp(x) = -99693.984 +/- 15.814
Evaluate (epoch 41) -- logp(x) = -98492.227 +/- 26.522
Evaluate (epoch 41) -- logp(x) = -99515.617 +/- 47.987
Evaluate (epoch 41) -- logp(x) = -99181.641 +/- 17.316
Evaluate (epoch 42) -- logp(x) = -307422528.000 +/- 34969700.000
Evaluate (epoch 42) -- logp(x) = -60748.344 +/- 122.513
Evaluate (epoch 42) -- logp(x) = -53686.898 +/- 160.084
Evaluate (epoch 42) -- logp(x) = -189018.391 +/- 8411.413
Evaluate (epoch 42) -- logp(x) = -53560.973 +/- 42.324
Evaluate (epoch 43) -- logp(x) = -207646982144.000 +/- 36750532608.000
Evaluate (epoch 43) -- logp(x) = -68168.945 +/- 17.770
Evaluate (epoch 43) -- logp(x) = -64856.617 +/- 102.553
Evaluate (epoch 43) -- logp(x) = -72420.109 +/- 193.882
Evaluate (epoch 43) -- logp(x) = -64056.508 +/- 16.274
Evaluate (epoch 44) -- logp(x) = -3835202304.000 +/- 762279360.000
Evaluate (epoch 44) -- logp(x) = -61973.109 +/- 56.437
Evaluate (epoch 44) -- logp(x) = -57297.566 +/- 85.351
Evaluate (epoch 44) -- logp(x) = -178169.219 +/- 16190.021
Evaluate (epoch 44) -- logp(x) = -56700.215 +/- 13.910
Evaluate (epoch 45) -- logp(x) = -13110436954112.000 +/- 2089657696256.000
Evaluate (epoch 45) -- logp(x) = -64280.516 +/- 60.257
Evaluate (epoch 45) -- logp(x) = -61186.562 +/- 47.833
Evaluate (epoch 45) -- logp(x) = -68761.453 +/- 321.260
Evaluate (epoch 45) -- logp(x) = -61422.254 +/- 16.794
Evaluate (epoch 46) -- logp(x) = -1966344765440.000 +/- 273834835968.000
Evaluate (epoch 46) -- logp(x) = -65897.383 +/- 33.513
Evaluate (epoch 46) -- logp(x) = -62196.582 +/- 97.337
Evaluate (epoch 46) -- logp(x) = -67436.547 +/- 69.507
Evaluate (epoch 46) -- logp(x) = -63071.133 +/- 14.986
Evaluate (epoch 47) -- logp(x) = -577713702174720.000 +/- 99722371006464.000
Evaluate (epoch 47) -- logp(x) = -65139.816 +/- 35.660
Evaluate (epoch 47) -- logp(x) = -62063.078 +/- 105.623
Evaluate (epoch 47) -- logp(x) = -69498.883 +/- 351.135
Evaluate (epoch 47) -- logp(x) = -62278.836 +/- 14.852
Evaluate (epoch 48) -- logp(x) = -249834192.000 +/- 40726568.000
Evaluate (epoch 48) -- logp(x) = -95699.219 +/- 25.237
Evaluate (epoch 48) -- logp(x) = -91742.672 +/- 31.226
Evaluate (epoch 48) -- logp(x) = -93761.039 +/- 98.703
Evaluate (epoch 48) -- logp(x) = -90963.539 +/- 47.460
Evaluate (epoch 49) -- logp(x) = -13412289478656.000 +/- 1569252311040.000
Evaluate (epoch 49) -- logp(x) = -66929.500 +/- 53.868
Evaluate (epoch 49) -- logp(x) = -63127.809 +/- 51.758
Evaluate (epoch 49) -- logp(x) = -5744079405056.000 +/- 1062907871232.000
Evaluate (epoch 49) -- logp(x) = -64956.523 +/- 10.469
{'activation_fn': 'relu',
 'batch_size': 4,
 'channel': -1,
 'cond_label_size': None,
 'conditional': False,
 'data_dir': './data/',
 'dataset': '',
 'device': device(type='cuda', index=0),
 'flip_toy_var_order': False,
 'generate': False,
 'hidden_size': 64,
 'input_dims': torch.Size([3, 64, 64]),
 'input_order': 'sequential',
 'input_size': 12288,
 'log_interval': 1000,
 'lr': 0.0001,
 'model': 'realnvp',
 'n_blocks': 32,
 'n_components': 1,
 'n_epochs': 50,
 'n_hidden': 1,
 'no_batch_norm': False,
 'no_cuda': False,
 'output_dir': './results/mafencoderFalse_channel-1_nblocks32/',
 'restore_file': None,
 'results_file': 'results.txt',
 'seed': 1,
 'start_epoch': 0,
 'train': False,
 'use_encoder': False}
RealNVP(
  (net): FlowSequential(
    (0): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (1): BatchNorm()
    (2): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (3): BatchNorm()
    (4): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (5): BatchNorm()
    (6): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (7): BatchNorm()
    (8): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (9): BatchNorm()
    (10): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (11): BatchNorm()
    (12): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (13): BatchNorm()
    (14): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (15): BatchNorm()
    (16): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (17): BatchNorm()
    (18): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (19): BatchNorm()
    (20): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (21): BatchNorm()
    (22): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (23): BatchNorm()
    (24): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (25): BatchNorm()
    (26): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (27): BatchNorm()
    (28): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (29): BatchNorm()
    (30): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (31): BatchNorm()
    (32): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (33): BatchNorm()
    (34): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (35): BatchNorm()
    (36): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (37): BatchNorm()
    (38): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (39): BatchNorm()
    (40): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (41): BatchNorm()
    (42): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (43): BatchNorm()
    (44): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (45): BatchNorm()
    (46): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (47): BatchNorm()
    (48): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (49): BatchNorm()
    (50): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (51): BatchNorm()
    (52): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (53): BatchNorm()
    (54): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (55): BatchNorm()
    (56): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (57): BatchNorm()
    (58): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (59): BatchNorm()
    (60): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (61): BatchNorm()
    (62): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (63): BatchNorm()
  )
)
{'activation_fn': 'relu',
 'batch_size': 4,
 'channel': -1,
 'cond_label_size': None,
 'conditional': False,
 'data_dir': './data/',
 'dataset': 'toy',
 'device': device(type='cuda', index=0),
 'flip_toy_var_order': False,
 'generate': False,
 'hidden_size': 64,
 'input_dims': torch.Size([3, 64, 64]),
 'input_order': 'sequential',
 'input_size': 12288,
 'log_interval': 1000,
 'lr': 0.0001,
 'model': 'realnvp',
 'n_blocks': 64,
 'n_components': 1,
 'n_epochs': 50,
 'n_hidden': 1,
 'no_batch_norm': False,
 'no_cuda': False,
 'output_dir': './results/mafencoderFalse_channel-1_nblocks64/',
 'restore_file': None,
 'results_file': 'results.txt',
 'seed': 1,
 'start_epoch': 0,
 'train': False,
 'use_encoder': False}
RealNVP(
  (net): FlowSequential(
    (0): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (1): BatchNorm()
    (2): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (3): BatchNorm()
    (4): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (5): BatchNorm()
    (6): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (7): BatchNorm()
    (8): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (9): BatchNorm()
    (10): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (11): BatchNorm()
    (12): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (13): BatchNorm()
    (14): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (15): BatchNorm()
    (16): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (17): BatchNorm()
    (18): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (19): BatchNorm()
    (20): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (21): BatchNorm()
    (22): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (23): BatchNorm()
    (24): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (25): BatchNorm()
    (26): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (27): BatchNorm()
    (28): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (29): BatchNorm()
    (30): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (31): BatchNorm()
    (32): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (33): BatchNorm()
    (34): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (35): BatchNorm()
    (36): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (37): BatchNorm()
    (38): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (39): BatchNorm()
    (40): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (41): BatchNorm()
    (42): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (43): BatchNorm()
    (44): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (45): BatchNorm()
    (46): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (47): BatchNorm()
    (48): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (49): BatchNorm()
    (50): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (51): BatchNorm()
    (52): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (53): BatchNorm()
    (54): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (55): BatchNorm()
    (56): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (57): BatchNorm()
    (58): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (59): BatchNorm()
    (60): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (61): BatchNorm()
    (62): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (63): BatchNorm()
    (64): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (65): BatchNorm()
    (66): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (67): BatchNorm()
    (68): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (69): BatchNorm()
    (70): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (71): BatchNorm()
    (72): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (73): BatchNorm()
    (74): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (75): BatchNorm()
    (76): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (77): BatchNorm()
    (78): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (79): BatchNorm()
    (80): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (81): BatchNorm()
    (82): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (83): BatchNorm()
    (84): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (85): BatchNorm()
    (86): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (87): BatchNorm()
    (88): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (89): BatchNorm()
    (90): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (91): BatchNorm()
    (92): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (93): BatchNorm()
    (94): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (95): BatchNorm()
    (96): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (97): BatchNorm()
    (98): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (99): BatchNorm()
    (100): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (101): BatchNorm()
    (102): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (103): BatchNorm()
    (104): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (105): BatchNorm()
    (106): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (107): BatchNorm()
    (108): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (109): BatchNorm()
    (110): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (111): BatchNorm()
    (112): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (113): BatchNorm()
    (114): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (115): BatchNorm()
    (116): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (117): BatchNorm()
    (118): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (119): BatchNorm()
    (120): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (121): BatchNorm()
    (122): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (123): BatchNorm()
    (124): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (125): BatchNorm()
    (126): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (127): BatchNorm()
  )
)
Evaluate logp(x) = -18857.855 +/- 79.836
Evaluate logp(x) = -25219.863 +/- 472.002
Evaluate logp(x) = -25341.551 +/- 326.634
Evaluate logp(x) = -14047.594 +/- 39.657
Evaluate logp(x) = -81306.797 +/- 3773.726
Evaluate logp(x) = -15651.905 +/- 37.565
Evaluate logp(x) = -72212.977 +/- 1762.305
Evaluate logp(x) = -13811.287 +/- 3.786
Evaluate logp(x) = -626001.750 +/- 20090.354
Evaluate logp(x) = -14346.033 +/- 6.806
{'activation_fn': 'relu',
 'batch_size': 4,
 'channel': -1,
 'cond_label_size': None,
 'conditional': False,
 'data_dir': './data/',
 'dataset': '',
 'device': device(type='cuda', index=0),
 'flip_toy_var_order': False,
 'generate': False,
 'hidden_size': 64,
 'input_dims': torch.Size([3, 64, 64]),
 'input_order': 'sequential',
 'input_size': 12288,
 'log_interval': 1000,
 'lr': 0.0001,
 'model': 'realnvp',
 'n_blocks': 32,
 'n_components': 1,
 'n_epochs': 50,
 'n_hidden': 1,
 'no_batch_norm': False,
 'no_cuda': False,
 'output_dir': './results/mafencoderFalse_channel-1_nblocks32/',
 'restore_file': None,
 'results_file': 'results.txt',
 'seed': 1,
 'start_epoch': 0,
 'train': True,
 'use_encoder': False}
RealNVP(
  (net): FlowSequential(
    (0): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (1): BatchNorm()
    (2): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (3): BatchNorm()
    (4): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (5): BatchNorm()
    (6): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (7): BatchNorm()
    (8): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (9): BatchNorm()
    (10): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (11): BatchNorm()
    (12): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (13): BatchNorm()
    (14): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (15): BatchNorm()
    (16): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (17): BatchNorm()
    (18): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (19): BatchNorm()
    (20): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (21): BatchNorm()
    (22): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (23): BatchNorm()
    (24): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (25): BatchNorm()
    (26): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (27): BatchNorm()
    (28): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (29): BatchNorm()
    (30): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (31): BatchNorm()
    (32): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (33): BatchNorm()
    (34): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (35): BatchNorm()
    (36): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (37): BatchNorm()
    (38): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (39): BatchNorm()
    (40): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (41): BatchNorm()
    (42): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (43): BatchNorm()
    (44): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (45): BatchNorm()
    (46): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (47): BatchNorm()
    (48): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (49): BatchNorm()
    (50): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (51): BatchNorm()
    (52): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (53): BatchNorm()
    (54): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (55): BatchNorm()
    (56): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (57): BatchNorm()
    (58): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (59): BatchNorm()
    (60): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (61): BatchNorm()
    (62): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (63): BatchNorm()
  )
)
{'activation_fn': 'relu',
 'batch_size': 4,
 'channel': -1,
 'cond_label_size': None,
 'conditional': False,
 'data_dir': './data/',
 'dataset': 'toy',
 'device': device(type='cuda', index=0),
 'flip_toy_var_order': False,
 'generate': False,
 'hidden_size': 64,
 'input_dims': torch.Size([3, 64, 64]),
 'input_order': 'sequential',
 'input_size': 12288,
 'log_interval': 1000,
 'lr': 0.0001,
 'model': 'realnvp',
 'n_blocks': 64,
 'n_components': 1,
 'n_epochs': 50,
 'n_hidden': 1,
 'no_batch_norm': False,
 'no_cuda': False,
 'output_dir': './results/mafencoderFalse_channel-1_nblocks64/',
 'restore_file': None,
 'results_file': 'results.txt',
 'seed': 1,
 'start_epoch': 0,
 'train': True,
 'use_encoder': False}
RealNVP(
  (net): FlowSequential(
    (0): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (1): BatchNorm()
    (2): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (3): BatchNorm()
    (4): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (5): BatchNorm()
    (6): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (7): BatchNorm()
    (8): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (9): BatchNorm()
    (10): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (11): BatchNorm()
    (12): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (13): BatchNorm()
    (14): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (15): BatchNorm()
    (16): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (17): BatchNorm()
    (18): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (19): BatchNorm()
    (20): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (21): BatchNorm()
    (22): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (23): BatchNorm()
    (24): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (25): BatchNorm()
    (26): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (27): BatchNorm()
    (28): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (29): BatchNorm()
    (30): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (31): BatchNorm()
    (32): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (33): BatchNorm()
    (34): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (35): BatchNorm()
    (36): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (37): BatchNorm()
    (38): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (39): BatchNorm()
    (40): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (41): BatchNorm()
    (42): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (43): BatchNorm()
    (44): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (45): BatchNorm()
    (46): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (47): BatchNorm()
    (48): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (49): BatchNorm()
    (50): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (51): BatchNorm()
    (52): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (53): BatchNorm()
    (54): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (55): BatchNorm()
    (56): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (57): BatchNorm()
    (58): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (59): BatchNorm()
    (60): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (61): BatchNorm()
    (62): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (63): BatchNorm()
    (64): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (65): BatchNorm()
    (66): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (67): BatchNorm()
    (68): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (69): BatchNorm()
    (70): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (71): BatchNorm()
    (72): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (73): BatchNorm()
    (74): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (75): BatchNorm()
    (76): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (77): BatchNorm()
    (78): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (79): BatchNorm()
    (80): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (81): BatchNorm()
    (82): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (83): BatchNorm()
    (84): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (85): BatchNorm()
    (86): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (87): BatchNorm()
    (88): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (89): BatchNorm()
    (90): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (91): BatchNorm()
    (92): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (93): BatchNorm()
    (94): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (95): BatchNorm()
    (96): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (97): BatchNorm()
    (98): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (99): BatchNorm()
    (100): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (101): BatchNorm()
    (102): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (103): BatchNorm()
    (104): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (105): BatchNorm()
    (106): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (107): BatchNorm()
    (108): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (109): BatchNorm()
    (110): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (111): BatchNorm()
    (112): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (113): BatchNorm()
    (114): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (115): BatchNorm()
    (116): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (117): BatchNorm()
    (118): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (119): BatchNorm()
    (120): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (121): BatchNorm()
    (122): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (123): BatchNorm()
    (124): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (125): BatchNorm()
    (126): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (127): BatchNorm()
  )
)
Evaluate (epoch 0) -- logp(x) = -14233.260 +/- 78.796
Evaluate (epoch 0) -- logp(x) = -95852.734 +/- 4034.554
Evaluate (epoch 0) -- logp(x) = -126737.078 +/- 6495.576
Evaluate (epoch 0) -- logp(x) = -2274209.250 +/- 131972.859
Evaluate (epoch 0) -- logp(x) = -347987.406 +/- 18191.471
Evaluate (epoch 0) -- logp(x) = -19446.477 +/- 72.984
Evaluate (epoch 0) -- logp(x) = -66651.953 +/- 822.298
Evaluate (epoch 0) -- logp(x) = -19471.102 +/- 256.410
Evaluate (epoch 0) -- logp(x) = -389670.750 +/- 14670.816
Evaluate (epoch 0) -- logp(x) = -21075.203 +/- 29.892
{'activation_fn': 'relu',
 'batch_size': 4,
 'channel': -1,
 'cond_label_size': None,
 'conditional': False,
 'data_dir': './data/',
 'dataset': '',
 'device': device(type='cuda', index=0),
 'flip_toy_var_order': False,
 'generate': False,
 'hidden_size': 64,
 'input_dims': torch.Size([3, 64, 64]),
 'input_order': 'sequential',
 'input_size': 12288,
 'log_interval': 1000,
 'lr': 0.0001,
 'model': 'realnvp',
 'n_blocks': 32,
 'n_components': 1,
 'n_epochs': 50,
 'n_hidden': 1,
 'no_batch_norm': False,
 'no_cuda': False,
 'output_dir': './results/mafencoderFalse_channel-1_nblocks32/',
 'restore_file': None,
 'results_file': 'results.txt',
 'seed': 1,
 'start_epoch': 0,
 'train': True,
 'use_encoder': False}
RealNVP(
  (net): FlowSequential(
    (0): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (1): BatchNorm()
    (2): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (3): BatchNorm()
    (4): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (5): BatchNorm()
    (6): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (7): BatchNorm()
    (8): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (9): BatchNorm()
    (10): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (11): BatchNorm()
    (12): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (13): BatchNorm()
    (14): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (15): BatchNorm()
    (16): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (17): BatchNorm()
    (18): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (19): BatchNorm()
    (20): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (21): BatchNorm()
    (22): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (23): BatchNorm()
    (24): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (25): BatchNorm()
    (26): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (27): BatchNorm()
    (28): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (29): BatchNorm()
    (30): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (31): BatchNorm()
    (32): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (33): BatchNorm()
    (34): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (35): BatchNorm()
    (36): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (37): BatchNorm()
    (38): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (39): BatchNorm()
    (40): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (41): BatchNorm()
    (42): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (43): BatchNorm()
    (44): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (45): BatchNorm()
    (46): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (47): BatchNorm()
    (48): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (49): BatchNorm()
    (50): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (51): BatchNorm()
    (52): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (53): BatchNorm()
    (54): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (55): BatchNorm()
    (56): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (57): BatchNorm()
    (58): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (59): BatchNorm()
    (60): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (61): BatchNorm()
    (62): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (63): BatchNorm()
  )
)
{'activation_fn': 'relu',
 'batch_size': 4,
 'channel': -1,
 'cond_label_size': None,
 'conditional': False,
 'data_dir': './data/',
 'dataset': 'toy',
 'device': device(type='cuda', index=0),
 'flip_toy_var_order': False,
 'generate': False,
 'hidden_size': 64,
 'input_dims': torch.Size([3, 64, 64]),
 'input_order': 'sequential',
 'input_size': 12288,
 'log_interval': 1000,
 'lr': 0.0001,
 'model': 'realnvp',
 'n_blocks': 64,
 'n_components': 1,
 'n_epochs': 50,
 'n_hidden': 1,
 'no_batch_norm': False,
 'no_cuda': False,
 'output_dir': './results/mafencoderFalse_channel-1_nblocks64/',
 'restore_file': None,
 'results_file': 'results.txt',
 'seed': 1,
 'start_epoch': 0,
 'train': True,
 'use_encoder': False}
RealNVP(
  (net): FlowSequential(
    (0): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (1): BatchNorm()
    (2): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (3): BatchNorm()
    (4): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (5): BatchNorm()
    (6): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (7): BatchNorm()
    (8): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (9): BatchNorm()
    (10): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (11): BatchNorm()
    (12): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (13): BatchNorm()
    (14): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (15): BatchNorm()
    (16): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (17): BatchNorm()
    (18): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (19): BatchNorm()
    (20): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (21): BatchNorm()
    (22): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (23): BatchNorm()
    (24): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (25): BatchNorm()
    (26): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (27): BatchNorm()
    (28): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (29): BatchNorm()
    (30): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (31): BatchNorm()
    (32): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (33): BatchNorm()
    (34): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (35): BatchNorm()
    (36): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (37): BatchNorm()
    (38): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (39): BatchNorm()
    (40): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (41): BatchNorm()
    (42): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (43): BatchNorm()
    (44): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (45): BatchNorm()
    (46): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (47): BatchNorm()
    (48): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (49): BatchNorm()
    (50): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (51): BatchNorm()
    (52): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (53): BatchNorm()
    (54): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (55): BatchNorm()
    (56): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (57): BatchNorm()
    (58): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (59): BatchNorm()
    (60): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (61): BatchNorm()
    (62): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (63): BatchNorm()
    (64): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (65): BatchNorm()
    (66): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (67): BatchNorm()
    (68): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (69): BatchNorm()
    (70): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (71): BatchNorm()
    (72): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (73): BatchNorm()
    (74): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (75): BatchNorm()
    (76): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (77): BatchNorm()
    (78): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (79): BatchNorm()
    (80): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (81): BatchNorm()
    (82): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (83): BatchNorm()
    (84): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (85): BatchNorm()
    (86): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (87): BatchNorm()
    (88): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (89): BatchNorm()
    (90): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (91): BatchNorm()
    (92): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (93): BatchNorm()
    (94): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (95): BatchNorm()
    (96): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (97): BatchNorm()
    (98): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (99): BatchNorm()
    (100): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (101): BatchNorm()
    (102): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (103): BatchNorm()
    (104): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (105): BatchNorm()
    (106): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (107): BatchNorm()
    (108): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (109): BatchNorm()
    (110): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (111): BatchNorm()
    (112): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (113): BatchNorm()
    (114): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (115): BatchNorm()
    (116): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (117): BatchNorm()
    (118): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (119): BatchNorm()
    (120): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (121): BatchNorm()
    (122): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (123): BatchNorm()
    (124): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (125): BatchNorm()
    (126): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=12288, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=12288, bias=True)
      )
    )
    (127): BatchNorm()
  )
)
Evaluate (epoch 0) -- logp(x) = -14233.260 +/- 78.796
Evaluate (epoch 0) -- logp(x) = -95852.734 +/- 4034.554
Evaluate (epoch 0) -- logp(x) = -126737.078 +/- 6495.576
Evaluate (epoch 0) -- logp(x) = -2407823.250 +/- 73649.461
Evaluate (epoch 0) -- logp(x) = -347987.406 +/- 18191.471
Evaluate (epoch 0) -- logp(x) = -19446.477 +/- 72.984
Evaluate (epoch 0) -- logp(x) = -66651.953 +/- 822.298
Evaluate (epoch 0) -- logp(x) = -19471.102 +/- 256.410
Evaluate (epoch 0) -- logp(x) = -429156.125 +/- 13422.520
Evaluate (epoch 0) -- logp(x) = -21075.203 +/- 29.892
Evaluate (epoch 1) -- logp(x) = -19088.727 +/- 75.089
Evaluate (epoch 1) -- logp(x) = -4693816.500 +/- 179077.203
Evaluate (epoch 1) -- logp(x) = -292434.656 +/- 22280.365
Evaluate (epoch 1) -- logp(x) = -11891570.000 +/- 156755.906
Evaluate (epoch 1) -- logp(x) = -441433.875 +/- 22467.150
Evaluate (epoch 1) -- logp(x) = -20524.352 +/- 45.743
Evaluate (epoch 1) -- logp(x) = -135652288.000 +/- 3379906.250
Evaluate (epoch 1) -- logp(x) = -1697101.250 +/- 328654.969
Evaluate (epoch 1) -- logp(x) = -399446560.000 +/- 2131957.500
Evaluate (epoch 1) -- logp(x) = -135716.281 +/- 5711.109
Evaluate (epoch 2) -- logp(x) = -28832.061 +/- 41.053
Evaluate (epoch 2) -- logp(x) = -631298752.000 +/- 17419924.000
Evaluate (epoch 2) -- logp(x) = -9173300.000 +/- 1809547.625
Evaluate (epoch 2) -- logp(x) = -668015232.000 +/- 6216848.500
Evaluate (epoch 2) -- logp(x) = -29681.768 +/- 4.991
Evaluate (epoch 2) -- logp(x) = -48944.379 +/- 8.039
Evaluate (epoch 2) -- logp(x) = -4075944.250 +/- 809240.125
Evaluate (epoch 2) -- logp(x) = -49760.090 +/- 11.323
Evaluate (epoch 2) -- logp(x) = -20283019264.000 +/- 1867720320.000
Evaluate (epoch 2) -- logp(x) = -49502.633 +/- 0.453
Evaluate (epoch 3) -- logp(x) = -39935.898 +/- 42.204
Evaluate (epoch 3) -- logp(x) = -34986237952.000 +/- 551839104.000
Evaluate (epoch 3) -- logp(x) = -1813400448.000 +/- 158180336.000
Evaluate (epoch 3) -- logp(x) = -42761555968.000 +/- 763391680.000
Evaluate (epoch 3) -- logp(x) = -216293648.000 +/- 6792292.500
Evaluate (epoch 4) -- logp(x) = -38086.547 +/- 34.144
Evaluate (epoch 4) -- logp(x) = -4750835712000.000 +/- 190485348352.000
Evaluate (epoch 4) -- logp(x) = -38336.242 +/- 15.088
Evaluate (epoch 4) -- logp(x) = -7043156017152.000 +/- 249326501888.000
Evaluate (epoch 4) -- logp(x) = -38070.504 +/- 2.416
Evaluate (epoch 3) -- logp(x) = -68223.344 +/- 8.907
Evaluate (epoch 3) -- logp(x) = -13772877987840.000 +/- 950997483520.000
Evaluate (epoch 3) -- logp(x) = -68935.250 +/- 14.031
Evaluate (epoch 3) -- logp(x) = -109458147508224.000 +/- 5530389577728.000
Evaluate (epoch 3) -- logp(x) = -68721.047 +/- 3.880
Evaluate (epoch 5) -- logp(x) = -51178.898 +/- 18.826
Evaluate (epoch 5) -- logp(x) = -89074827264.000 +/- 1824197888.000
Evaluate (epoch 5) -- logp(x) = -544693056.000 +/- 107855200.000
Evaluate (epoch 5) -- logp(x) = -112299638784.000 +/- 1444338176.000
Evaluate (epoch 5) -- logp(x) = -50699.836 +/- 19.140
Evaluate (epoch 4) -- logp(x) = -72778.406 +/- 14.101
Evaluate (epoch 4) -- logp(x) = -138572925501440.000 +/- 8128032669696.000
Evaluate (epoch 4) -- logp(x) = -73672.047 +/- 13.716
Evaluate (epoch 4) -- logp(x) = -4570091089821696.000 +/- 316692467023872.000
Evaluate (epoch 4) -- logp(x) = -73492.297 +/- 2.363
Evaluate (epoch 6) -- logp(x) = -38830.312 +/- 39.034
Evaluate (epoch 6) -- logp(x) = -648934785024.000 +/- 20212023296.000
Evaluate (epoch 6) -- logp(x) = -1262976256.000 +/- 250099024.000
Evaluate (epoch 6) -- logp(x) = -509534273536.000 +/- 12559041536.000
Evaluate (epoch 6) -- logp(x) = -37770.984 +/- 41.184
Evaluate (epoch 5) -- logp(x) = -61538.629 +/- 22.874
Evaluate (epoch 5) -- logp(x) = -232029536256.000 +/- 46580903936.000
Evaluate (epoch 5) -- logp(x) = -62089.703 +/- 23.644
Evaluate (epoch 5) -- logp(x) = -813582702870528.000 +/- 30137158467584.000
Evaluate (epoch 5) -- logp(x) = -61814.414 +/- 2.869
Evaluate (epoch 7) -- logp(x) = -45645.016 +/- 28.231
Evaluate (epoch 7) -- logp(x) = -81659927986176.000 +/- 3119351595008.000
Evaluate (epoch 7) -- logp(x) = -836884889600.000 +/- 165727911936.000
Evaluate (epoch 7) -- logp(x) = -97895676116992.000 +/- 2784335757312.000
Evaluate (epoch 7) -- logp(x) = -45269.715 +/- 24.750
Evaluate (epoch 6) -- logp(x) = -92228.633 +/- 14.269
Evaluate (epoch 6) -- logp(x) = -4981259763712.000 +/- 886407495680.000
Evaluate (epoch 6) -- logp(x) = -92593.867 +/- 16.245
Evaluate (epoch 6) -- logp(x) = -187969881243648.000 +/- 8702627676160.000
Evaluate (epoch 6) -- logp(x) = -92242.203 +/- 2.582
Evaluate (epoch 8) -- logp(x) = -64254.980 +/- 49.084
Evaluate (epoch 8) -- logp(x) = -386161115136.000 +/- 18632673280.000
Evaluate (epoch 8) -- logp(x) = -63440.055 +/- 247.518
Evaluate (epoch 8) -- logp(x) = -791991943168.000 +/- 18948894720.000
Evaluate (epoch 8) -- logp(x) = -64328.004 +/- 3.322
Evaluate (epoch 9) -- logp(x) = -48794.664 +/- 39.890
Evaluate (epoch 9) -- logp(x) = -1866220699648.000 +/- 71771004928.000
Evaluate (epoch 9) -- logp(x) = -813116800.000 +/- 161011488.000
Evaluate (epoch 9) -- logp(x) = -2193054236672.000 +/- 31589615616.000
Evaluate (epoch 9) -- logp(x) = -48367.410 +/- 0.824
Evaluate (epoch 7) -- logp(x) = -100113.953 +/- 20.431
Evaluate (epoch 7) -- logp(x) = -99992.984 +/- 64.628
Evaluate (epoch 7) -- logp(x) = -100162.609 +/- 12.220
Evaluate (epoch 7) -- logp(x) = -20745620.000 +/- 4104696.500
Evaluate (epoch 7) -- logp(x) = -99879.078 +/- 1.347
Evaluate (epoch 10) -- logp(x) = -42883.035 +/- 25.242
Evaluate (epoch 10) -- logp(x) = -2362530332672.000 +/- 90991484928.000
Evaluate (epoch 10) -- logp(x) = -43452.000 +/- 121.874
Evaluate (epoch 10) -- logp(x) = -2456917114880.000 +/- 78104780800.000
Evaluate (epoch 10) -- logp(x) = -42885.336 +/- 9.000
Evaluate (epoch 8) -- logp(x) = -64553.418 +/- 15.977
Evaluate (epoch 8) -- logp(x) = -64670.438 +/- 32.045
Evaluate (epoch 8) -- logp(x) = -64662.973 +/- 9.931
Evaluate (epoch 8) -- logp(x) = -63509.469 +/- 103.139
Evaluate (epoch 8) -- logp(x) = -64426.734 +/- 1.004
Evaluate (epoch 11) -- logp(x) = -44064.871 +/- 34.989
Evaluate (epoch 11) -- logp(x) = -12103479984128.000 +/- 459873386496.000
Evaluate (epoch 11) -- logp(x) = -14522398720.000 +/- 2875854848.000
Evaluate (epoch 11) -- logp(x) = -11946693754880.000 +/- 401213063168.000
Evaluate (epoch 11) -- logp(x) = -43622.285 +/- 13.170
Evaluate (epoch 9) -- logp(x) = -86880.266 +/- 37.329
Evaluate (epoch 9) -- logp(x) = -88028.281 +/- 43.485
Evaluate (epoch 9) -- logp(x) = -87176.820 +/- 23.012
Evaluate (epoch 9) -- logp(x) = -87495.750 +/- 51.850
Evaluate (epoch 9) -- logp(x) = -86678.438 +/- 6.418
Evaluate (epoch 12) -- logp(x) = -128263380992.000 +/- 25493823488.000
Evaluate (epoch 12) -- logp(x) = -60069.289 +/- 21.744
Evaluate (epoch 12) -- logp(x) = -58813.164 +/- 88.700
Evaluate (epoch 12) -- logp(x) = -60569.574 +/- 25.443
Evaluate (epoch 12) -- logp(x) = -58027.977 +/- 9.351
Evaluate (epoch 13) -- logp(x) = -50488.945 +/- 23.780
Evaluate (epoch 13) -- logp(x) = -11873682456576.000 +/- 778756685824.000
Evaluate (epoch 13) -- logp(x) = -663537188864.000 +/- 131377250304.000
Evaluate (epoch 10) -- logp(x) = -64930.816 +/- 21.634
Evaluate (epoch 10) -- logp(x) = -65933.836 +/- 23.701
Evaluate (epoch 13) -- logp(x) = -9360882270208.000 +/- 898482831360.000
Evaluate (epoch 10) -- logp(x) = -65196.047 +/- 17.197
Evaluate (epoch 13) -- logp(x) = -44857.734 +/- 84.091
Evaluate (epoch 10) -- logp(x) = -66259.188 +/- 23.493
Evaluate (epoch 10) -- logp(x) = -64930.578 +/- 3.012
Evaluate (epoch 14) -- logp(x) = -45558.719 +/- 30.503
Evaluate (epoch 14) -- logp(x) = -15623438467072.000 +/- 1580172050432.000
Evaluate (epoch 14) -- logp(x) = -15855360000.000 +/- 3132137472.000
Evaluate (epoch 14) -- logp(x) = -8106038788096.000 +/- 491613323264.000
Evaluate (epoch 14) -- logp(x) = -45729.680 +/- 6.334
Evaluate (epoch 11) -- logp(x) = -70340.641 +/- 17.360
Evaluate (epoch 11) -- logp(x) = -71106.344 +/- 22.562
Evaluate (epoch 11) -- logp(x) = -70579.773 +/- 12.468
Evaluate (epoch 11) -- logp(x) = -71227.750 +/- 12.680
Evaluate (epoch 11) -- logp(x) = -70374.125 +/- 2.125
Evaluate (epoch 15) -- logp(x) = -49084.633 +/- 20.719
Evaluate (epoch 15) -- logp(x) = -58149.000 +/- 850.106
Evaluate (epoch 15) -- logp(x) = -4302258831360.000 +/- 851974225920.000
Evaluate (epoch 15) -- logp(x) = -5339200010321920.000 +/- 1004989866573824.000
Evaluate (epoch 15) -- logp(x) = -287758178320384.000 +/- 8539622342656.000
Evaluate (epoch 12) -- logp(x) = -142666.547 +/- 25.482
Evaluate (epoch 12) -- logp(x) = -179185.969 +/- 7572.319
Evaluate (epoch 12) -- logp(x) = -142666.016 +/- 13.042
Evaluate (epoch 12) -- logp(x) = -8421757952.000 +/- 792603584.000
Evaluate (epoch 12) -- logp(x) = -142440.172 +/- 7.665
Evaluate (epoch 16) -- logp(x) = -609097351168.000 +/- 39557824512.000
Evaluate (epoch 16) -- logp(x) = -143444.281 +/- 5169.705
Evaluate (epoch 16) -- logp(x) = -6354930176.000 +/- 1258450560.000
Evaluate (epoch 16) -- logp(x) = -150424.844 +/- 5257.064
Evaluate (epoch 16) -- logp(x) = -58275.219 +/- 325.550
Evaluate (epoch 13) -- logp(x) = -52491.164 +/- 53.638
Evaluate (epoch 13) -- logp(x) = -60049.258 +/- 778.303
Evaluate (epoch 13) -- logp(x) = -53048.789 +/- 53.476
Evaluate (epoch 13) -- logp(x) = -928876068864.000 +/- 79197134848.000
Evaluate (epoch 13) -- logp(x) = -52523.156 +/- 10.200
Evaluate (epoch 17) -- logp(x) = -52031.234 +/- 267.784
Evaluate (epoch 17) -- logp(x) = -23335972.000 +/- 1742631.250
Evaluate (epoch 17) -- logp(x) = -79681.359 +/- 3627.579
Evaluate (epoch 17) -- logp(x) = -14561792.000 +/- 1528290.875
Evaluate (epoch 17) -- logp(x) = -59808.309 +/- 5.935
Evaluate (epoch 18) -- logp(x) = -62208.285 +/- 16.145
Evaluate (epoch 18) -- logp(x) = -789394496.000 +/- 146016944.000
Evaluate (epoch 18) -- logp(x) = -62447.547 +/- 37.381
Evaluate (epoch 18) -- logp(x) = -2524146432.000 +/- 257287136.000
Evaluate (epoch 18) -- logp(x) = -60606.062 +/- 7.239
Evaluate (epoch 14) -- logp(x) = -51563.836 +/- 42.228
Evaluate (epoch 14) -- logp(x) = -10201756336128.000 +/- 2048046006272.000
Evaluate (epoch 14) -- logp(x) = -51831.055 +/- 25.201
Evaluate (epoch 14) -- logp(x) = -10341008864706560.000 +/- 1971718419644416.000
Evaluate (epoch 14) -- logp(x) = -51483.637 +/- 4.785
Evaluate (epoch 19) -- logp(x) = -57894.332 +/- 29.206
Evaluate (epoch 19) -- logp(x) = -10749530112.000 +/- 1971471360.000
Evaluate (epoch 19) -- logp(x) = -59185.234 +/- 203.923
Evaluate (epoch 19) -- logp(x) = -33036163072.000 +/- 5504017408.000
Evaluate (epoch 19) -- logp(x) = -56781.336 +/- 28.918
Evaluate (epoch 15) -- logp(x) = -99711.906 +/- 34.772
Evaluate (epoch 15) -- logp(x) = -98396.078 +/- 458.096
Evaluate (epoch 15) -- logp(x) = -99922.984 +/- 13.674
Evaluate (epoch 15) -- logp(x) = -115914600.000 +/- 16655306.000
Evaluate (epoch 15) -- logp(x) = -99675.562 +/- 5.092
Evaluate (epoch 20) -- logp(x) = -55283.891 +/- 88.966
Evaluate (epoch 20) -- logp(x) = -751268.500 +/- 79124.141
Evaluate (epoch 20) -- logp(x) = -58095.625 +/- 359.109
Evaluate (epoch 20) -- logp(x) = -838738.438 +/- 85557.578
Evaluate (epoch 20) -- logp(x) = -56185.656 +/- 24.063
Evaluate (epoch 16) -- logp(x) = -73180.461 +/- 393.978
Evaluate (epoch 16) -- logp(x) = -3624472.000 +/- 345275.250
Evaluate (epoch 16) -- logp(x) = -301804.750 +/- 27214.373
Evaluate (epoch 16) -- logp(x) = -73520.844 +/- 259.463
Evaluate (epoch 16) -- logp(x) = -69948.391 +/- 105.891
Evaluate (epoch 21) -- logp(x) = -49614.457 +/- 34.862
Evaluate (epoch 21) -- logp(x) = -5621677.500 +/- 990621.500
Evaluate (epoch 21) -- logp(x) = -50764.965 +/- 255.997
Evaluate (epoch 21) -- logp(x) = -4710128.000 +/- 361992.156
Evaluate (epoch 21) -- logp(x) = -48089.109 +/- 68.797
Evaluate (epoch 22) -- logp(x) = -47866.660 +/- 113.671
Evaluate (epoch 22) -- logp(x) = -175923.312 +/- 14015.790
Evaluate (epoch 22) -- logp(x) = -49147.059 +/- 149.766
Evaluate (epoch 22) -- logp(x) = -321732.094 +/- 33825.730
Evaluate (epoch 22) -- logp(x) = -46854.945 +/- 1.466
Evaluate (epoch 17) -- logp(x) = -80917.781 +/- 218.202
Evaluate (epoch 17) -- logp(x) = -85081.773 +/- 63.864
Evaluate (epoch 17) -- logp(x) = -82448.828 +/- 89.303
Evaluate (epoch 17) -- logp(x) = -87931.352 +/- 88.165
Evaluate (epoch 17) -- logp(x) = -81390.562 +/- 9.800
Evaluate (epoch 23) -- logp(x) = -52442.645 +/- 73.399
Evaluate (epoch 23) -- logp(x) = -104946.797 +/- 5294.562
Evaluate (epoch 23) -- logp(x) = -53955.516 +/- 122.850
Evaluate (epoch 23) -- logp(x) = -148002.531 +/- 8031.371
Evaluate (epoch 23) -- logp(x) = -52372.480 +/- 21.086
Evaluate (epoch 18) -- logp(x) = -68182.188 +/- 49.645
Evaluate (epoch 18) -- logp(x) = -73433.844 +/- 1132.371
Evaluate (epoch 18) -- logp(x) = -68066.000 +/- 23.380
Evaluate (epoch 18) -- logp(x) = -92134.094 +/- 1236.062
Evaluate (epoch 18) -- logp(x) = -68044.156 +/- 10.337
Evaluate (epoch 24) -- logp(x) = -49768.938 +/- 30.434
Evaluate (epoch 24) -- logp(x) = -73997184.000 +/- 14359664.000
Evaluate (epoch 24) -- logp(x) = -51004.996 +/- 193.051
Evaluate (epoch 24) -- logp(x) = -402521024.000 +/- 79152584.000
Evaluate (epoch 24) -- logp(x) = -139569774592.000 +/- 7550397440.000
Evaluate (epoch 19) -- logp(x) = -65568.398 +/- 52.605
Evaluate (epoch 19) -- logp(x) = -2304597.500 +/- 169184.422
Evaluate (epoch 19) -- logp(x) = -65688.625 +/- 22.103
Evaluate (epoch 19) -- logp(x) = -2017937664.000 +/- 183984192.000
Evaluate (epoch 19) -- logp(x) = -65291.441 +/- 10.363
Evaluate (epoch 25) -- logp(x) = -54657.383 +/- 73.009
Evaluate (epoch 25) -- logp(x) = -3049761792.000 +/- 302628384.000
Evaluate (epoch 25) -- logp(x) = -94958056.000 +/- 18793486.000
Evaluate (epoch 25) -- logp(x) = -7122916864.000 +/- 729694656.000
Evaluate (epoch 25) -- logp(x) = -6856203304960.000 +/- 73044893696.000
Evaluate (epoch 20) -- logp(x) = -80075.922 +/- 36.296
Evaluate (epoch 20) -- logp(x) = -494222.406 +/- 82922.797
Evaluate (epoch 20) -- logp(x) = -80725.859 +/- 16.318
Evaluate (epoch 20) -- logp(x) = -334173472.000 +/- 16049366.000
Evaluate (epoch 20) -- logp(x) = -80194.359 +/- 5.232
Evaluate (epoch 26) -- logp(x) = -52566.617 +/- 96.554
Evaluate (epoch 26) -- logp(x) = -2018697088.000 +/- 254741696.000
Evaluate (epoch 26) -- logp(x) = -46582451929088.000 +/- 7469675315200.000
Evaluate (epoch 26) -- logp(x) = -3756196864.000 +/- 634587520.000
Evaluate (epoch 26) -- logp(x) = -1989549752320.000 +/- 54129692672.000
Evaluate (epoch 27) -- logp(x) = -53669.090 +/- 122.030
Evaluate (epoch 27) -- logp(x) = -298337.375 +/- 34568.172
Evaluate (epoch 27) -- logp(x) = -54316.004 +/- 187.154
Evaluate (epoch 27) -- logp(x) = -459052.500 +/- 59534.035
Evaluate (epoch 27) -- logp(x) = -53543.043 +/- 43.473
Evaluate (epoch 21) -- logp(x) = -120653.078 +/- 103.875
Evaluate (epoch 21) -- logp(x) = -947879424.000 +/- 12756137.000
Evaluate (epoch 21) -- logp(x) = -1875473.375 +/- 347870.156
Evaluate (epoch 21) -- logp(x) = -1765307648.000 +/- 6009955.000
Evaluate (epoch 21) -- logp(x) = -118941.156 +/- 11.545
Evaluate (epoch 28) -- logp(x) = -54503.730 +/- 68.269
Evaluate (epoch 28) -- logp(x) = -210435407872.000 +/- 22599227392.000
Evaluate (epoch 28) -- logp(x) = -13969073.000 +/- 2755324.500
Evaluate (epoch 28) -- logp(x) = -96247406592.000 +/- 18865668096.000
Evaluate (epoch 28) -- logp(x) = -54570.695 +/- 16.592
Evaluate (epoch 22) -- logp(x) = -71867.477 +/- 33.359
Evaluate (epoch 22) -- logp(x) = -3138099085312.000 +/- 74061471744.000
Evaluate (epoch 22) -- logp(x) = -74601.727 +/- 357.041
Evaluate (epoch 22) -- logp(x) = -7859975749632.000 +/- 617045098496.000
Evaluate (epoch 22) -- logp(x) = -71686.703 +/- 6.381
Evaluate (epoch 29) -- logp(x) = -77352.406 +/- 43.233
Evaluate (epoch 29) -- logp(x) = -106864885760.000 +/- 8491059200.000
Evaluate (epoch 29) -- logp(x) = -77758.375 +/- 92.375
Evaluate (epoch 29) -- logp(x) = -64121720832.000 +/- 7942158848.000
Evaluate (epoch 29) -- logp(x) = -78218.680 +/- 8.893
Evaluate (epoch 23) -- logp(x) = -70675.695 +/- 36.518
Evaluate (epoch 23) -- logp(x) = -5296108339200.000 +/- 205162184704.000
Evaluate (epoch 23) -- logp(x) = -33194895360.000 +/- 6573554176.000
Evaluate (epoch 23) -- logp(x) = -9031918813184.000 +/- 562500075520.000
Evaluate (epoch 23) -- logp(x) = -71093.078 +/- 26.410
Evaluate (epoch 30) -- logp(x) = -53772.098 +/- 55.093
Evaluate (epoch 30) -- logp(x) = -10407103168512.000 +/- 492631097344.000
Evaluate (epoch 30) -- logp(x) = -414446223360.000 +/- 82072576000.000
Evaluate (epoch 30) -- logp(x) = -53465963298816.000 +/- 3213777436672.000
Evaluate (epoch 30) -- logp(x) = -55807.008 +/- 5.027
Evaluate (epoch 24) -- logp(x) = -61331.320 +/- 23.930
Evaluate (epoch 24) -- logp(x) = -43958298214400.000 +/- 1712414916608.000
Evaluate (epoch 24) -- logp(x) = -516261.906 +/- 90047.242
Evaluate (epoch 24) -- logp(x) = -139707266629632.000 +/- 6205558226944.000
Evaluate (epoch 24) -- logp(x) = -61697.316 +/- 11.472
Evaluate (epoch 31) -- logp(x) = -100084.172 +/- 156.252
Evaluate (epoch 31) -- logp(x) = -5215449.000 +/- 306192.562
Evaluate (epoch 31) -- logp(x) = -473392.562 +/- 73066.000
Evaluate (epoch 31) -- logp(x) = -8719339.000 +/- 628254.750
Evaluate (epoch 31) -- logp(x) = -104841.703 +/- 16.583
Evaluate (epoch 32) -- logp(x) = -51587.836 +/- 48.896
Evaluate (epoch 32) -- logp(x) = -516019328.000 +/- 42322444.000
Evaluate (epoch 32) -- logp(x) = -131144.281 +/- 15824.662
Evaluate (epoch 32) -- logp(x) = -931178368.000 +/- 66934256.000
Evaluate (epoch 32) -- logp(x) = -52295.531 +/- 23.429
Evaluate (epoch 25) -- logp(x) = -71932.984 +/- 11.290
Evaluate (epoch 25) -- logp(x) = -23924159021056.000 +/- 1000363065344.000
Evaluate (epoch 25) -- logp(x) = -40114827264.000 +/- 7943906816.000
Evaluate (epoch 25) -- logp(x) = -105922022080512.000 +/- 4336429039616.000
Evaluate (epoch 25) -- logp(x) = -71795.844 +/- 6.853
Evaluate (epoch 33) -- logp(x) = -50828.312 +/- 18.810
Evaluate (epoch 33) -- logp(x) = -464854144.000 +/- 41582924.000
Evaluate (epoch 33) -- logp(x) = -12199863.000 +/- 2405513.750
Evaluate (epoch 33) -- logp(x) = -567631296.000 +/- 86017488.000
Evaluate (epoch 33) -- logp(x) = -50182.816 +/- 190.846
Evaluate (epoch 26) -- logp(x) = -62963.363 +/- 55.086
Evaluate (epoch 26) -- logp(x) = -115632985079808.000 +/- 7760981786624.000
Evaluate (epoch 26) -- logp(x) = -65139.777 +/- 444.582
Evaluate (epoch 26) -- logp(x) = -302450993004544.000 +/- 7591293353984.000
Evaluate (epoch 26) -- logp(x) = -62472.121 +/- 10.041
Evaluate (epoch 34) -- logp(x) = -55631.855 +/- 52.479
Evaluate (epoch 34) -- logp(x) = -405965438976.000 +/- 16876007424.000
Evaluate (epoch 34) -- logp(x) = -14946072576.000 +/- 1758120064.000
Evaluate (epoch 34) -- logp(x) = -493179633664.000 +/- 30291085312.000
Evaluate (epoch 34) -- logp(x) = -1987728128.000 +/- 142183632.000
Evaluate (epoch 27) -- logp(x) = -57736.961 +/- 60.365
Evaluate (epoch 27) -- logp(x) = -1923614383276032.000 +/- 209078823419904.000
Evaluate (epoch 27) -- logp(x) = -12582060.000 +/- 2480173.250
Evaluate (epoch 27) -- logp(x) = -2804166967689216.000 +/- 139036639363072.000
Evaluate (epoch 27) -- logp(x) = -57435.727 +/- 14.973
Evaluate (epoch 35) -- logp(x) = -63654.680 +/- 34.816
Evaluate (epoch 35) -- logp(x) = -22825748480.000 +/- 294730464.000
Evaluate (epoch 35) -- logp(x) = -3791781632.000 +/- 750871424.000
Evaluate (epoch 35) -- logp(x) = -20161927168.000 +/- 1598336896.000
Evaluate (epoch 35) -- logp(x) = -60833.891 +/- 313.477
Evaluate (epoch 28) -- logp(x) = -73544.961 +/- 31.895
Evaluate (epoch 28) -- logp(x) = -728244454162432.000 +/- 30568582479872.000
Evaluate (epoch 28) -- logp(x) = -75005.688 +/- 245.077
Evaluate (epoch 28) -- logp(x) = -1566816417611776.000 +/- 38887565033472.000
Evaluate (epoch 28) -- logp(x) = -73306.859 +/- 5.516
Evaluate (epoch 36) -- logp(x) = -53838.477 +/- 59.912
Evaluate (epoch 36) -- logp(x) = -30783379456.000 +/- 1486949760.000
Evaluate (epoch 36) -- logp(x) = -2216352256.000 +/- 347028224.000
Evaluate (epoch 36) -- logp(x) = -24264292352.000 +/- 1675827072.000
Evaluate (epoch 36) -- logp(x) = -1114590848.000 +/- 36561332.000
Evaluate (epoch 37) -- logp(x) = -74949.633 +/- 75.084
Evaluate (epoch 37) -- logp(x) = -3244894208.000 +/- 130439512.000
Evaluate (epoch 37) -- logp(x) = -6589461504.000 +/- 792682176.000
Evaluate (epoch 37) -- logp(x) = -3524494336.000 +/- 154338944.000
Evaluate (epoch 37) -- logp(x) = -8492248.000 +/- 132944.812
Evaluate (epoch 29) -- logp(x) = -52222.109 +/- 37.693
Evaluate (epoch 29) -- logp(x) = -12521323242717184.000 +/- 1327420443459584.000
Evaluate (epoch 29) -- logp(x) = -3533138944.000 +/- 699655424.000
Evaluate (epoch 29) -- logp(x) = -22229550280736768.000 +/- 1155965349003264.000
Evaluate (epoch 29) -- logp(x) = -52472.824 +/- 4.785
Evaluate (epoch 38) -- logp(x) = -39996.422 +/- 119.429
Evaluate (epoch 38) -- logp(x) = -340781888.000 +/- 13820339.000
Evaluate (epoch 38) -- logp(x) = -18876914.000 +/- 2786537.250
Evaluate (epoch 38) -- logp(x) = -274507328.000 +/- 10472335.000
Evaluate (epoch 38) -- logp(x) = -15128271.000 +/- 293351.312
Evaluate (epoch 30) -- logp(x) = -61486.863 +/- 21.691
Evaluate (epoch 30) -- logp(x) = -13786154604167168.000 +/- 2425085369516032.000
Evaluate (epoch 30) -- logp(x) = -62881.016 +/- 160.429
Evaluate (epoch 30) -- logp(x) = -31646998369140736.000 +/- 1783415510663168.000
Evaluate (epoch 30) -- logp(x) = -61921.988 +/- 17.905
Evaluate (epoch 39) -- logp(x) = -44228.746 +/- 64.770
Evaluate (epoch 39) -- logp(x) = -216682987520.000 +/- 7642531328.000
Evaluate (epoch 39) -- logp(x) = -4386127872.000 +/- 825247040.000
Evaluate (epoch 39) -- logp(x) = -284802973696.000 +/- 8221955584.000
Evaluate (epoch 39) -- logp(x) = -205486944.000 +/- 7961779.000
Evaluate (epoch 31) -- logp(x) = -55644.531 +/- 24.997
Evaluate (epoch 31) -- logp(x) = -88910324152926208.000 +/- 7342362019758080.000
Evaluate (epoch 31) -- logp(x) = -56926.680 +/- 132.564
Evaluate (epoch 31) -- logp(x) = -528513592836751360.000 +/- 9340681990438912.000
Evaluate (epoch 31) -- logp(x) = -56247.301 +/- 15.771
Evaluate (epoch 40) -- logp(x) = -46869.051 +/- 51.989
Evaluate (epoch 40) -- logp(x) = -58519588864.000 +/- 1948071552.000
Evaluate (epoch 40) -- logp(x) = -836083072.000 +/- 158905888.000
Evaluate (epoch 40) -- logp(x) = -92203294720.000 +/- 3669197312.000
Evaluate (epoch 40) -- logp(x) = -50348.359 +/- 22.824
Evaluate (epoch 32) -- logp(x) = -70423.352 +/- 21.536
Evaluate (epoch 32) -- logp(x) = -166340200723644416.000 +/- 12291106553200640.000
Evaluate (epoch 32) -- logp(x) = -71265.648 +/- 101.301
Evaluate (epoch 32) -- logp(x) = -506862009862586368.000 +/- 27703561261416448.000
Evaluate (epoch 32) -- logp(x) = -71024.164 +/- 6.086
Evaluate (epoch 41) -- logp(x) = -72652.000 +/- 37.380
Evaluate (epoch 41) -- logp(x) = -151367488.000 +/- 5830656.000
Evaluate (epoch 41) -- logp(x) = -90912.500 +/- 3398.725
Evaluate (epoch 41) -- logp(x) = -230957664.000 +/- 9712299.000
Evaluate (epoch 41) -- logp(x) = -73860.711 +/- 12.172
Evaluate (epoch 42) -- logp(x) = -49670.969 +/- 36.517
Evaluate (epoch 42) -- logp(x) = -44589408256.000 +/- 2111306368.000
Evaluate (epoch 42) -- logp(x) = -1602508800.000 +/- 230534704.000
Evaluate (epoch 42) -- logp(x) = -60205293568.000 +/- 171285296.000
Evaluate (epoch 42) -- logp(x) = -702058432.000 +/- 107480144.000
Evaluate (epoch 33) -- logp(x) = -50221.484 +/- 40.166
Evaluate (epoch 33) -- logp(x) = -33826574012776448.000 +/- 1277216168083456.000
Evaluate (epoch 33) -- logp(x) = -68992.828 +/- 3638.841
Evaluate (epoch 33) -- logp(x) = -48656550625017856.000 +/- 2227713637089280.000
Evaluate (epoch 33) -- logp(x) = -51087.141 +/- 9.425
Evaluate (epoch 43) -- logp(x) = -54213.125 +/- 29.799
Evaluate (epoch 43) -- logp(x) = -18671902720.000 +/- 808195072.000
Evaluate (epoch 43) -- logp(x) = -144088704.000 +/- 28522740.000
Evaluate (epoch 43) -- logp(x) = -26138927104.000 +/- 841585024.000
Evaluate (epoch 43) -- logp(x) = -56293.820 +/- 25.987
Evaluate (epoch 34) -- logp(x) = -75683.055 +/- 19.474
Evaluate (epoch 34) -- logp(x) = -3711478808969216.000 +/- 267450968965120.000
Evaluate (epoch 34) -- logp(x) = -75824.812 +/- 65.568
Evaluate (epoch 34) -- logp(x) = -3511595594416128.000 +/- 207626973478912.000
Evaluate (epoch 34) -- logp(x) = -76086.039 +/- 6.259
Evaluate (epoch 44) -- logp(x) = -47961.852 +/- 22.685
Evaluate (epoch 44) -- logp(x) = -135573200896.000 +/- 4970395648.000
Evaluate (epoch 44) -- logp(x) = -828767808.000 +/- 164101840.000
Evaluate (epoch 44) -- logp(x) = -201835544576.000 +/- 2718836224.000
Evaluate (epoch 44) -- logp(x) = -51736.555 +/- 5.469
Evaluate (epoch 35) -- logp(x) = -87967.805 +/- 19.166
Evaluate (epoch 35) -- logp(x) = -19885717907832832.000 +/- 660928961970176.000
Evaluate (epoch 35) -- logp(x) = -88630.891 +/- 22.643
Evaluate (epoch 35) -- logp(x) = -23253562825900032.000 +/- 1059871294226432.000
Evaluate (epoch 35) -- logp(x) = -88661.523 +/- 8.263
Evaluate (epoch 45) -- logp(x) = -50168.961 +/- 22.179
Evaluate (epoch 45) -- logp(x) = -1766628524032.000 +/- 85352652800.000
Evaluate (epoch 45) -- logp(x) = -16965986304.000 +/- 2018073088.000
Evaluate (epoch 45) -- logp(x) = -2911818481664.000 +/- 75621572608.000
Evaluate (epoch 45) -- logp(x) = -52004.164 +/- 38.942
Evaluate (epoch 36) -- logp(x) = -51066.211 +/- 62.720
Evaluate (epoch 36) -- logp(x) = -45017792512.000 +/- 4028755968.000
Evaluate (epoch 36) -- logp(x) = -53824.309 +/- 394.793
Evaluate (epoch 36) -- logp(x) = -1406402688.000 +/- 110691496.000
Evaluate (epoch 36) -- logp(x) = -51449.699 +/- 7.461
Evaluate (epoch 46) -- logp(x) = -49827.676 +/- 32.825
Evaluate (epoch 46) -- logp(x) = -138780737536.000 +/- 5109817856.000
Evaluate (epoch 46) -- logp(x) = -223069306880.000 +/- 43336216576.000
Evaluate (epoch 46) -- logp(x) = -237996294144.000 +/- 2747475200.000
Evaluate (epoch 46) -- logp(x) = -52999.332 +/- 501.904
Evaluate (epoch 47) -- logp(x) = -90323.906 +/- 26.292
Evaluate (epoch 47) -- logp(x) = -423109248.000 +/- 24915940.000
Evaluate (epoch 47) -- logp(x) = -113127.125 +/- 4310.743
Evaluate (epoch 47) -- logp(x) = -724477952.000 +/- 16337112.000
Evaluate (epoch 47) -- logp(x) = -91220.625 +/- 11.993
Evaluate (epoch 37) -- logp(x) = -61873.344 +/- 20.637
Evaluate (epoch 37) -- logp(x) = -1349526749184.000 +/- 117886967808.000
Evaluate (epoch 37) -- logp(x) = -64030.453 +/- 75.363
Evaluate (epoch 37) -- logp(x) = -26952040448.000 +/- 4022227712.000
Evaluate (epoch 37) -- logp(x) = -63183.516 +/- 6.533
Evaluate (epoch 48) -- logp(x) = -51020.262 +/- 15.773
Evaluate (epoch 48) -- logp(x) = -35879276544.000 +/- 2848555264.000
Evaluate (epoch 48) -- logp(x) = -348155776.000 +/- 66310392.000
Evaluate (epoch 48) -- logp(x) = -88022949888.000 +/- 4972619264.000
Evaluate (epoch 48) -- logp(x) = -53922.582 +/- 43.121
Evaluate (epoch 38) -- logp(x) = -1991450034176.000 +/- 158930894848.000
Evaluate (epoch 38) -- logp(x) = -84064.984 +/- 13.507
Evaluate (epoch 38) -- logp(x) = -79303.336 +/- 865.882
Evaluate (epoch 38) -- logp(x) = -84555.398 +/- 32.916
Evaluate (epoch 38) -- logp(x) = -396392202240.000 +/- 7514509824.000
Evaluate (epoch 49) -- logp(x) = -50673.410 +/- 23.494
Evaluate (epoch 49) -- logp(x) = -1193507553280.000 +/- 70682042368.000
Evaluate (epoch 49) -- logp(x) = -1203192448.000 +/- 238257200.000
Evaluate (epoch 49) -- logp(x) = -2090653319168.000 +/- 22738423808.000
Evaluate (epoch 49) -- logp(x) = -52126.098 +/- 17.824
Evaluate (epoch 39) -- logp(x) = -16303582609408.000 +/- 1130582245376.000
Evaluate (epoch 39) -- logp(x) = -66157.938 +/- 27.870
Evaluate (epoch 39) -- logp(x) = -62795.547 +/- 148.014
Evaluate (epoch 39) -- logp(x) = -69262.484 +/- 83.582
Evaluate (epoch 39) -- logp(x) = -64611.996 +/- 33.497
Evaluate (epoch 40) -- logp(x) = -5307127824384.000 +/- 1054853300224.000
Evaluate (epoch 40) -- logp(x) = -72195.594 +/- 11.524
Evaluate (epoch 40) -- logp(x) = -71158.141 +/- 44.904
Evaluate (epoch 40) -- logp(x) = -73745.875 +/- 19.947
Evaluate (epoch 40) -- logp(x) = -70854.164 +/- 9.517
Evaluate (epoch 41) -- logp(x) = -260410343424.000 +/- 51752890368.000
Evaluate (epoch 41) -- logp(x) = -99693.984 +/- 15.814
Evaluate (epoch 41) -- logp(x) = -98492.227 +/- 26.522
Evaluate (epoch 41) -- logp(x) = -99608.383 +/- 37.599
Evaluate (epoch 41) -- logp(x) = -99181.641 +/- 17.316
Evaluate (epoch 42) -- logp(x) = -307422528.000 +/- 34969700.000
Evaluate (epoch 42) -- logp(x) = -60748.344 +/- 122.513
Evaluate (epoch 42) -- logp(x) = -53686.898 +/- 160.084
Evaluate (epoch 42) -- logp(x) = -150140.594 +/- 3316.686
Evaluate (epoch 42) -- logp(x) = -53560.973 +/- 42.324
Evaluate (epoch 43) -- logp(x) = -207646982144.000 +/- 36750532608.000
Evaluate (epoch 43) -- logp(x) = -68168.945 +/- 17.770
Evaluate (epoch 43) -- logp(x) = -64856.617 +/- 102.553
Evaluate (epoch 43) -- logp(x) = -73703.547 +/- 265.231
Evaluate (epoch 43) -- logp(x) = -64056.508 +/- 16.274
Evaluate (epoch 44) -- logp(x) = -3835202304.000 +/- 762279360.000
Evaluate (epoch 44) -- logp(x) = -61973.109 +/- 56.437
Evaluate (epoch 44) -- logp(x) = -57297.566 +/- 85.351
Evaluate (epoch 44) -- logp(x) = -155288.953 +/- 11599.923
Evaluate (epoch 44) -- logp(x) = -56700.215 +/- 13.910
Evaluate (epoch 45) -- logp(x) = -13110436954112.000 +/- 2089657696256.000
Evaluate (epoch 45) -- logp(x) = -64280.516 +/- 60.257
Evaluate (epoch 45) -- logp(x) = -61186.562 +/- 47.833
Evaluate (epoch 45) -- logp(x) = -69036.562 +/- 318.217
Evaluate (epoch 45) -- logp(x) = -61422.254 +/- 16.794
Evaluate (epoch 46) -- logp(x) = -1966344765440.000 +/- 273834835968.000
Evaluate (epoch 46) -- logp(x) = -65897.383 +/- 33.513
Evaluate (epoch 46) -- logp(x) = -62196.582 +/- 97.337
Evaluate (epoch 46) -- logp(x) = -67979.773 +/- 161.998
Evaluate (epoch 46) -- logp(x) = -63071.133 +/- 14.986
Evaluate (epoch 47) -- logp(x) = -577713702174720.000 +/- 99722371006464.000
Evaluate (epoch 47) -- logp(x) = -65139.816 +/- 35.660
Evaluate (epoch 47) -- logp(x) = -62063.078 +/- 105.623
Evaluate (epoch 47) -- logp(x) = -69593.422 +/- 316.036
Evaluate (epoch 47) -- logp(x) = -62278.836 +/- 14.852
Evaluate (epoch 48) -- logp(x) = -249834192.000 +/- 40726568.000
Evaluate (epoch 48) -- logp(x) = -95699.219 +/- 25.237
Evaluate (epoch 48) -- logp(x) = -91742.672 +/- 31.226
Evaluate (epoch 48) -- logp(x) = -93908.031 +/- 120.896
Evaluate (epoch 48) -- logp(x) = -90963.539 +/- 47.460
Evaluate (epoch 49) -- logp(x) = -13412289478656.000 +/- 1569252311040.000
Evaluate (epoch 49) -- logp(x) = -66929.500 +/- 53.868
Evaluate (epoch 49) -- logp(x) = -63127.809 +/- 51.758
Evaluate (epoch 49) -- logp(x) = -68027.500 +/- 49.037
Evaluate (epoch 49) -- logp(x) = -64956.523 +/- 10.469
{'activation_fn': 'relu',
 'batch_size': 4,
 'channel': -1,
 'cond_label_size': None,
 'conditional': False,
 'data_dir': './data/',
 'dataset': 'cityscapes',
 'device': device(type='cuda', index=0),
 'flip_toy_var_order': False,
 'generate': False,
 'hidden_size': 64,
 'input_dims': (1024, 2048, 3),
 'input_order': 'sequential',
 'input_size': 8192,
 'log_interval': 1000,
 'lr': 0.0001,
 'model': 'realnvp',
 'n_blocks': 32,
 'n_components': 1,
 'n_epochs': 50,
 'n_hidden': 1,
 'no_batch_norm': False,
 'no_cuda': False,
 'output_dir': './results/mafencoderFalse_channel-1_nblocks32/',
 'restore_file': None,
 'results_file': 'results.txt',
 'seed': 1,
 'start_epoch': 0,
 'train': True,
 'use_encoder': False}
RealNVP(
  (net): FlowSequential(
    (0): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=8192, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=8192, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=8192, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=8192, bias=True)
      )
    )
    (1): BatchNorm()
    (2): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=8192, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=8192, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=8192, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=8192, bias=True)
      )
    )
    (3): BatchNorm()
    (4): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=8192, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=8192, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=8192, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=8192, bias=True)
      )
    )
    (5): BatchNorm()
    (6): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=8192, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=8192, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=8192, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=8192, bias=True)
      )
    )
    (7): BatchNorm()
    (8): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=8192, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=8192, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=8192, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=8192, bias=True)
      )
    )
    (9): BatchNorm()
    (10): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=8192, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=8192, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=8192, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=8192, bias=True)
      )
    )
    (11): BatchNorm()
    (12): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=8192, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=8192, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=8192, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=8192, bias=True)
      )
    )
    (13): BatchNorm()
    (14): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=8192, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=8192, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=8192, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=8192, bias=True)
      )
    )
    (15): BatchNorm()
    (16): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=8192, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=8192, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=8192, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=8192, bias=True)
      )
    )
    (17): BatchNorm()
    (18): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=8192, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=8192, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=8192, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=8192, bias=True)
      )
    )
    (19): BatchNorm()
    (20): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=8192, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=8192, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=8192, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=8192, bias=True)
      )
    )
    (21): BatchNorm()
    (22): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=8192, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=8192, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=8192, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=8192, bias=True)
      )
    )
    (23): BatchNorm()
    (24): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=8192, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=8192, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=8192, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=8192, bias=True)
      )
    )
    (25): BatchNorm()
    (26): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=8192, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=8192, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=8192, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=8192, bias=True)
      )
    )
    (27): BatchNorm()
    (28): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=8192, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=8192, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=8192, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=8192, bias=True)
      )
    )
    (29): BatchNorm()
    (30): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=8192, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=8192, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=8192, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=8192, bias=True)
      )
    )
    (31): BatchNorm()
    (32): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=8192, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=8192, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=8192, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=8192, bias=True)
      )
    )
    (33): BatchNorm()
    (34): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=8192, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=8192, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=8192, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=8192, bias=True)
      )
    )
    (35): BatchNorm()
    (36): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=8192, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=8192, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=8192, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=8192, bias=True)
      )
    )
    (37): BatchNorm()
    (38): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=8192, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=8192, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=8192, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=8192, bias=True)
      )
    )
    (39): BatchNorm()
    (40): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=8192, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=8192, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=8192, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=8192, bias=True)
      )
    )
    (41): BatchNorm()
    (42): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=8192, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=8192, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=8192, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=8192, bias=True)
      )
    )
    (43): BatchNorm()
    (44): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=8192, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=8192, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=8192, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=8192, bias=True)
      )
    )
    (45): BatchNorm()
    (46): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=8192, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=8192, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=8192, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=8192, bias=True)
      )
    )
    (47): BatchNorm()
    (48): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=8192, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=8192, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=8192, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=8192, bias=True)
      )
    )
    (49): BatchNorm()
    (50): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=8192, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=8192, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=8192, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=8192, bias=True)
      )
    )
    (51): BatchNorm()
    (52): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=8192, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=8192, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=8192, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=8192, bias=True)
      )
    )
    (53): BatchNorm()
    (54): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=8192, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=8192, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=8192, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=8192, bias=True)
      )
    )
    (55): BatchNorm()
    (56): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=8192, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=8192, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=8192, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=8192, bias=True)
      )
    )
    (57): BatchNorm()
    (58): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=8192, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=8192, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=8192, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=8192, bias=True)
      )
    )
    (59): BatchNorm()
    (60): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=8192, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=8192, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=8192, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=8192, bias=True)
      )
    )
    (61): BatchNorm()
    (62): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=8192, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=8192, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=8192, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=8192, bias=True)
      )
    )
    (63): BatchNorm()
  )
)
{'activation_fn': 'relu',
 'batch_size': 4,
 'channel': -1,
 'cond_label_size': None,
 'conditional': False,
 'data_dir': './data/',
 'dataset': 'cityscapes',
 'device': device(type='cuda', index=0),
 'flip_toy_var_order': False,
 'generate': False,
 'hidden_size': 64,
 'input_dims': torch.Size([3, 128, 64]),
 'input_order': 'sequential',
 'input_size': 8192,
 'log_interval': 1000,
 'lr': 0.0001,
 'model': 'realnvp',
 'n_blocks': 32,
 'n_components': 1,
 'n_epochs': 50,
 'n_hidden': 1,
 'no_batch_norm': False,
 'no_cuda': False,
 'output_dir': './results/mafencoderFalse_channel-1_nblocks32/',
 'restore_file': None,
 'results_file': 'results.txt',
 'seed': 1,
 'start_epoch': 0,
 'train': True,
 'use_encoder': False}
RealNVP(
  (net): FlowSequential(
    (0): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=8192, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=8192, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=8192, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=8192, bias=True)
      )
    )
    (1): BatchNorm()
    (2): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=8192, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=8192, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=8192, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=8192, bias=True)
      )
    )
    (3): BatchNorm()
    (4): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=8192, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=8192, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=8192, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=8192, bias=True)
      )
    )
    (5): BatchNorm()
    (6): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=8192, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=8192, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=8192, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=8192, bias=True)
      )
    )
    (7): BatchNorm()
    (8): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=8192, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=8192, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=8192, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=8192, bias=True)
      )
    )
    (9): BatchNorm()
    (10): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=8192, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=8192, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=8192, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=8192, bias=True)
      )
    )
    (11): BatchNorm()
    (12): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=8192, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=8192, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=8192, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=8192, bias=True)
      )
    )
    (13): BatchNorm()
    (14): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=8192, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=8192, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=8192, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=8192, bias=True)
      )
    )
    (15): BatchNorm()
    (16): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=8192, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=8192, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=8192, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=8192, bias=True)
      )
    )
    (17): BatchNorm()
    (18): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=8192, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=8192, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=8192, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=8192, bias=True)
      )
    )
    (19): BatchNorm()
    (20): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=8192, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=8192, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=8192, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=8192, bias=True)
      )
    )
    (21): BatchNorm()
    (22): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=8192, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=8192, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=8192, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=8192, bias=True)
      )
    )
    (23): BatchNorm()
    (24): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=8192, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=8192, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=8192, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=8192, bias=True)
      )
    )
    (25): BatchNorm()
    (26): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=8192, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=8192, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=8192, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=8192, bias=True)
      )
    )
    (27): BatchNorm()
    (28): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=8192, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=8192, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=8192, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=8192, bias=True)
      )
    )
    (29): BatchNorm()
    (30): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=8192, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=8192, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=8192, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=8192, bias=True)
      )
    )
    (31): BatchNorm()
    (32): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=8192, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=8192, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=8192, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=8192, bias=True)
      )
    )
    (33): BatchNorm()
    (34): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=8192, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=8192, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=8192, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=8192, bias=True)
      )
    )
    (35): BatchNorm()
    (36): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=8192, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=8192, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=8192, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=8192, bias=True)
      )
    )
    (37): BatchNorm()
    (38): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=8192, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=8192, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=8192, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=8192, bias=True)
      )
    )
    (39): BatchNorm()
    (40): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=8192, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=8192, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=8192, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=8192, bias=True)
      )
    )
    (41): BatchNorm()
    (42): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=8192, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=8192, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=8192, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=8192, bias=True)
      )
    )
    (43): BatchNorm()
    (44): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=8192, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=8192, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=8192, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=8192, bias=True)
      )
    )
    (45): BatchNorm()
    (46): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=8192, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=8192, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=8192, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=8192, bias=True)
      )
    )
    (47): BatchNorm()
    (48): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=8192, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=8192, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=8192, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=8192, bias=True)
      )
    )
    (49): BatchNorm()
    (50): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=8192, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=8192, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=8192, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=8192, bias=True)
      )
    )
    (51): BatchNorm()
    (52): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=8192, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=8192, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=8192, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=8192, bias=True)
      )
    )
    (53): BatchNorm()
    (54): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=8192, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=8192, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=8192, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=8192, bias=True)
      )
    )
    (55): BatchNorm()
    (56): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=8192, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=8192, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=8192, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=8192, bias=True)
      )
    )
    (57): BatchNorm()
    (58): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=8192, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=8192, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=8192, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=8192, bias=True)
      )
    )
    (59): BatchNorm()
    (60): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=8192, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=8192, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=8192, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=8192, bias=True)
      )
    )
    (61): BatchNorm()
    (62): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=8192, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=8192, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=8192, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=8192, bias=True)
      )
    )
    (63): BatchNorm()
  )
)
{'activation_fn': 'relu',
 'batch_size': 4,
 'channel': -1,
 'cond_label_size': None,
 'conditional': False,
 'data_dir': './data/',
 'dataset': 'cityscapes',
 'device': device(type='cuda', index=0),
 'flip_toy_var_order': False,
 'generate': False,
 'hidden_size': 64,
 'input_dims': torch.Size([3, 128, 64]),
 'input_order': 'sequential',
 'input_size': 24576,
 'log_interval': 1000,
 'lr': 0.0001,
 'model': 'realnvp',
 'n_blocks': 32,
 'n_components': 1,
 'n_epochs': 50,
 'n_hidden': 1,
 'no_batch_norm': False,
 'no_cuda': False,
 'output_dir': './results/mafencoderFalse_channel-1_nblocks32/',
 'restore_file': None,
 'results_file': 'results.txt',
 'seed': 1,
 'start_epoch': 0,
 'train': True,
 'use_encoder': False}
RealNVP(
  (net): FlowSequential(
    (0): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=24576, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=24576, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=24576, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=24576, bias=True)
      )
    )
    (1): BatchNorm()
    (2): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=24576, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=24576, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=24576, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=24576, bias=True)
      )
    )
    (3): BatchNorm()
    (4): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=24576, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=24576, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=24576, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=24576, bias=True)
      )
    )
    (5): BatchNorm()
    (6): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=24576, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=24576, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=24576, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=24576, bias=True)
      )
    )
    (7): BatchNorm()
    (8): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=24576, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=24576, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=24576, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=24576, bias=True)
      )
    )
    (9): BatchNorm()
    (10): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=24576, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=24576, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=24576, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=24576, bias=True)
      )
    )
    (11): BatchNorm()
    (12): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=24576, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=24576, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=24576, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=24576, bias=True)
      )
    )
    (13): BatchNorm()
    (14): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=24576, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=24576, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=24576, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=24576, bias=True)
      )
    )
    (15): BatchNorm()
    (16): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=24576, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=24576, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=24576, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=24576, bias=True)
      )
    )
    (17): BatchNorm()
    (18): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=24576, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=24576, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=24576, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=24576, bias=True)
      )
    )
    (19): BatchNorm()
    (20): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=24576, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=24576, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=24576, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=24576, bias=True)
      )
    )
    (21): BatchNorm()
    (22): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=24576, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=24576, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=24576, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=24576, bias=True)
      )
    )
    (23): BatchNorm()
    (24): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=24576, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=24576, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=24576, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=24576, bias=True)
      )
    )
    (25): BatchNorm()
    (26): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=24576, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=24576, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=24576, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=24576, bias=True)
      )
    )
    (27): BatchNorm()
    (28): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=24576, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=24576, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=24576, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=24576, bias=True)
      )
    )
    (29): BatchNorm()
    (30): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=24576, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=24576, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=24576, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=24576, bias=True)
      )
    )
    (31): BatchNorm()
    (32): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=24576, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=24576, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=24576, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=24576, bias=True)
      )
    )
    (33): BatchNorm()
    (34): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=24576, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=24576, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=24576, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=24576, bias=True)
      )
    )
    (35): BatchNorm()
    (36): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=24576, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=24576, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=24576, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=24576, bias=True)
      )
    )
    (37): BatchNorm()
    (38): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=24576, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=24576, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=24576, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=24576, bias=True)
      )
    )
    (39): BatchNorm()
    (40): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=24576, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=24576, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=24576, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=24576, bias=True)
      )
    )
    (41): BatchNorm()
    (42): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=24576, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=24576, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=24576, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=24576, bias=True)
      )
    )
    (43): BatchNorm()
    (44): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=24576, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=24576, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=24576, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=24576, bias=True)
      )
    )
    (45): BatchNorm()
    (46): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=24576, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=24576, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=24576, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=24576, bias=True)
      )
    )
    (47): BatchNorm()
    (48): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=24576, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=24576, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=24576, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=24576, bias=True)
      )
    )
    (49): BatchNorm()
    (50): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=24576, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=24576, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=24576, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=24576, bias=True)
      )
    )
    (51): BatchNorm()
    (52): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=24576, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=24576, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=24576, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=24576, bias=True)
      )
    )
    (53): BatchNorm()
    (54): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=24576, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=24576, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=24576, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=24576, bias=True)
      )
    )
    (55): BatchNorm()
    (56): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=24576, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=24576, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=24576, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=24576, bias=True)
      )
    )
    (57): BatchNorm()
    (58): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=24576, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=24576, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=24576, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=24576, bias=True)
      )
    )
    (59): BatchNorm()
    (60): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=24576, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=24576, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=24576, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=24576, bias=True)
      )
    )
    (61): BatchNorm()
    (62): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=24576, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=24576, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=24576, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=24576, bias=True)
      )
    )
    (63): BatchNorm()
  )
)
{'activation_fn': 'relu',
 'batch_size': 4,
 'channel': -1,
 'cond_label_size': None,
 'conditional': False,
 'data_dir': './data/',
 'dataset': 'cityscapes',
 'device': device(type='cuda', index=0),
 'flip_toy_var_order': False,
 'generate': False,
 'hidden_size': 64,
 'input_dims': torch.Size([3, 128, 64]),
 'input_order': 'sequential',
 'input_size': 24576,
 'log_interval': 1000,
 'lr': 0.0001,
 'model': 'realnvp',
 'n_blocks': 32,
 'n_components': 1,
 'n_epochs': 50,
 'n_hidden': 1,
 'no_batch_norm': False,
 'no_cuda': False,
 'output_dir': './results/mafencoderFalse_channel-1_nblocks32/',
 'restore_file': None,
 'results_file': 'results.txt',
 'seed': 1,
 'start_epoch': 0,
 'train': True,
 'use_encoder': False}
RealNVP(
  (net): FlowSequential(
    (0): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=24576, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=24576, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=24576, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=24576, bias=True)
      )
    )
    (1): BatchNorm()
    (2): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=24576, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=24576, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=24576, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=24576, bias=True)
      )
    )
    (3): BatchNorm()
    (4): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=24576, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=24576, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=24576, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=24576, bias=True)
      )
    )
    (5): BatchNorm()
    (6): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=24576, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=24576, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=24576, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=24576, bias=True)
      )
    )
    (7): BatchNorm()
    (8): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=24576, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=24576, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=24576, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=24576, bias=True)
      )
    )
    (9): BatchNorm()
    (10): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=24576, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=24576, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=24576, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=24576, bias=True)
      )
    )
    (11): BatchNorm()
    (12): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=24576, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=24576, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=24576, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=24576, bias=True)
      )
    )
    (13): BatchNorm()
    (14): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=24576, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=24576, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=24576, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=24576, bias=True)
      )
    )
    (15): BatchNorm()
    (16): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=24576, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=24576, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=24576, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=24576, bias=True)
      )
    )
    (17): BatchNorm()
    (18): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=24576, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=24576, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=24576, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=24576, bias=True)
      )
    )
    (19): BatchNorm()
    (20): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=24576, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=24576, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=24576, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=24576, bias=True)
      )
    )
    (21): BatchNorm()
    (22): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=24576, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=24576, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=24576, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=24576, bias=True)
      )
    )
    (23): BatchNorm()
    (24): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=24576, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=24576, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=24576, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=24576, bias=True)
      )
    )
    (25): BatchNorm()
    (26): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=24576, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=24576, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=24576, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=24576, bias=True)
      )
    )
    (27): BatchNorm()
    (28): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=24576, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=24576, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=24576, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=24576, bias=True)
      )
    )
    (29): BatchNorm()
    (30): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=24576, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=24576, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=24576, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=24576, bias=True)
      )
    )
    (31): BatchNorm()
    (32): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=24576, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=24576, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=24576, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=24576, bias=True)
      )
    )
    (33): BatchNorm()
    (34): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=24576, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=24576, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=24576, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=24576, bias=True)
      )
    )
    (35): BatchNorm()
    (36): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=24576, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=24576, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=24576, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=24576, bias=True)
      )
    )
    (37): BatchNorm()
    (38): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=24576, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=24576, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=24576, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=24576, bias=True)
      )
    )
    (39): BatchNorm()
    (40): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=24576, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=24576, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=24576, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=24576, bias=True)
      )
    )
    (41): BatchNorm()
    (42): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=24576, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=24576, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=24576, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=24576, bias=True)
      )
    )
    (43): BatchNorm()
    (44): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=24576, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=24576, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=24576, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=24576, bias=True)
      )
    )
    (45): BatchNorm()
    (46): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=24576, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=24576, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=24576, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=24576, bias=True)
      )
    )
    (47): BatchNorm()
    (48): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=24576, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=24576, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=24576, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=24576, bias=True)
      )
    )
    (49): BatchNorm()
    (50): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=24576, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=24576, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=24576, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=24576, bias=True)
      )
    )
    (51): BatchNorm()
    (52): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=24576, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=24576, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=24576, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=24576, bias=True)
      )
    )
    (53): BatchNorm()
    (54): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=24576, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=24576, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=24576, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=24576, bias=True)
      )
    )
    (55): BatchNorm()
    (56): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=24576, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=24576, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=24576, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=24576, bias=True)
      )
    )
    (57): BatchNorm()
    (58): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=24576, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=24576, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=24576, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=24576, bias=True)
      )
    )
    (59): BatchNorm()
    (60): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=24576, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=24576, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=24576, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=24576, bias=True)
      )
    )
    (61): BatchNorm()
    (62): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=24576, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=24576, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=24576, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=24576, bias=True)
      )
    )
    (63): BatchNorm()
  )
)
{'activation_fn': 'relu',
 'batch_size': 4,
 'channel': -1,
 'cond_label_size': None,
 'conditional': False,
 'data_dir': './data/',
 'dataset': 'cityscapes',
 'device': device(type='cuda', index=0),
 'flip_toy_var_order': False,
 'generate': False,
 'hidden_size': 64,
 'input_dims': torch.Size([3, 128, 64]),
 'input_order': 'sequential',
 'input_size': 24576,
 'log_interval': 1000,
 'lr': 0.0001,
 'model': 'realnvp',
 'n_blocks': 16,
 'n_components': 1,
 'n_epochs': 50,
 'n_hidden': 1,
 'no_batch_norm': False,
 'no_cuda': False,
 'output_dir': './results/mafencoderFalse_channel-1_nblocks16/',
 'restore_file': None,
 'results_file': 'results.txt',
 'seed': 1,
 'start_epoch': 0,
 'train': True,
 'use_encoder': False}
RealNVP(
  (net): FlowSequential(
    (0): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=24576, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=24576, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=24576, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=24576, bias=True)
      )
    )
    (1): BatchNorm()
    (2): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=24576, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=24576, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=24576, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=24576, bias=True)
      )
    )
    (3): BatchNorm()
    (4): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=24576, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=24576, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=24576, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=24576, bias=True)
      )
    )
    (5): BatchNorm()
    (6): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=24576, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=24576, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=24576, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=24576, bias=True)
      )
    )
    (7): BatchNorm()
    (8): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=24576, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=24576, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=24576, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=24576, bias=True)
      )
    )
    (9): BatchNorm()
    (10): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=24576, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=24576, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=24576, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=24576, bias=True)
      )
    )
    (11): BatchNorm()
    (12): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=24576, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=24576, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=24576, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=24576, bias=True)
      )
    )
    (13): BatchNorm()
    (14): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=24576, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=24576, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=24576, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=24576, bias=True)
      )
    )
    (15): BatchNorm()
    (16): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=24576, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=24576, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=24576, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=24576, bias=True)
      )
    )
    (17): BatchNorm()
    (18): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=24576, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=24576, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=24576, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=24576, bias=True)
      )
    )
    (19): BatchNorm()
    (20): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=24576, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=24576, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=24576, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=24576, bias=True)
      )
    )
    (21): BatchNorm()
    (22): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=24576, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=24576, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=24576, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=24576, bias=True)
      )
    )
    (23): BatchNorm()
    (24): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=24576, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=24576, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=24576, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=24576, bias=True)
      )
    )
    (25): BatchNorm()
    (26): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=24576, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=24576, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=24576, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=24576, bias=True)
      )
    )
    (27): BatchNorm()
    (28): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=24576, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=24576, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=24576, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=24576, bias=True)
      )
    )
    (29): BatchNorm()
    (30): LinearMaskedCoupling(
      (s_net): Sequential(
        (0): Linear(in_features=24576, out_features=64, bias=True)
        (1): Tanh()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): Tanh()
        (4): Linear(in_features=64, out_features=24576, bias=True)
      )
      (t_net): Sequential(
        (0): Linear(in_features=24576, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=24576, bias=True)
      )
    )
    (31): BatchNorm()
  )
)
Evaluate (epoch 0) -- logp(x) = -439434.844 +/- 48236.852
